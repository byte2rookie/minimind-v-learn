{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b32453b6",
   "metadata": {},
   "source": [
    "# Pretrain阶段的训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5e686c",
   "metadata": {},
   "source": [
    "### 设置训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5a1d77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class pretrain_args:\n",
    "    out_dir = \"../out\"\n",
    "    epochs = 1\n",
    "    batch_size = 32\n",
    "    learning_rate = 5e-4\n",
    "    device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dtype = \"bfloat16\"\n",
    "    use_wandb = False\n",
    "    wandb_project = \"MiniMind-Pretrain\"\n",
    "    num_workers = 1\n",
    "    ddp = False\n",
    "    accumulation_steps = 8\n",
    "    grad_clip = 1.0\n",
    "    warmup_iters = 0\n",
    "    log_interval = 100\n",
    "    save_interval = 100\n",
    "    local_rank = -1\n",
    "    embed_dim = 512\n",
    "    block_num = 8\n",
    "    max_seqlen = 1024\n",
    "    use_moe = False\n",
    "    # data_path = \"../data/pretrain_data.jsonl\" #toy_dataset\n",
    "    data_path = \"../data/pretrain_hq.jsonl\" #full_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fb2865",
   "metadata": {},
   "source": [
    "### 加载model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb7ce093",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/zyp/miniconda3/envs/minimind/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查看工作设备 cuda:3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 获取当前 notebook 所在目录（trainer/）\n",
    "current_dir = os.path.dirname(os.path.abspath(\"__file__\"))  # 注意 Jupyter 中可能需要调整\n",
    "# 或者直接写死路径\n",
    "current_dir = \"/data/zyp/jinbu/ZZY/minimind-v-learn/trainer\"\n",
    "\n",
    "# 上一级目录就是项目根目录，拼接 model 路径\n",
    "model_dir = os.path.join(os.path.dirname(current_dir), \"model\")\n",
    "sys.path.append(model_dir)\n",
    "\n",
    "# 现在可以用绝对导入\n",
    "from model import MinimindForCausalLM, MinimindConfig\n",
    "train_args = pretrain_args()\n",
    "train_args.save_dir = os.path.join(train_args.out_dir)\n",
    "# 确保输出目录存在\n",
    "os.makedirs(train_args.save_dir, exist_ok=True)\n",
    "# 初始化模型配置\n",
    "config = MinimindConfig(\n",
    "    embed_dim=train_args.embed_dim,\n",
    "    block_num=train_args.block_num,\n",
    "    max_seqlen=train_args.max_seqlen,\n",
    ")\n",
    "print(f'查看工作设备 {train_args.device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2d05da",
   "metadata": {},
   "source": [
    "# 单卡加载和训练（不采用DDP，wandb）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c53e8c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM可训练总参数量：38.075 百万\n",
      "MinimindForCausalLM(\n",
      "  (embed): Embed(\n",
      "    (embedding): Embedding(6400, 512)\n",
      "  )\n",
      "  (rmsnorm): RMSNorm()\n",
      "  (minimind_dense): Minimind_Dense(\n",
      "    (blocks): ModuleList(\n",
      "      (0-7): 8 x Minimind_Block(\n",
      "        (attention): GroupQueryAttention(\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (k_proj): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (o_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (res_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (rmsnorm1): RMSNorm()\n",
      "        (ffn): FeedForward(\n",
      "          (gate): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (up_proj): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (down_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (rmsnorm2): RMSNorm()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=6400, bias=True)\n",
      ")\n",
      "PreTrainedTokenizerFast(name_or_path='../model/', vocab_size=6400, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|im_start|>', 'eos_token': '<|im_end|>', 'unk_token': '<|endoftext|>', 'pad_token': '<pad>', 'additional_special_tokens': ['<pad>', '<mask>', '<s>', '</s>', '<unk>', '<UNK>', '<EOS>', '<zzy>', '<|s1|>', '<|s2|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<endoftext>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t5: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t6: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t7: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t8: AddedToken(\"<UNK>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t9: AddedToken(\"<EOS>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t10: AddedToken(\"<zzy>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t11: AddedToken(\"<|s1|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t12: AddedToken(\"<|s2|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t6400: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/zyp/jinbu/ZZY/minimind-v-learn/dataset/lm_dataset.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X=torch.tensor(input_ids[:-1],dtype=torch.long)  # 去掉最后一个token\n",
      "/data/zyp/jinbu/ZZY/minimind-v-learn/dataset/lm_dataset.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y=torch.tensor(input_ids[1:],dtype=torch.long)  # 去掉第一个\n",
      "/data/zyp/jinbu/ZZY/minimind-v-learn/dataset/lm_dataset.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss_mask = torch.tensor(loss_mask[1:], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "打印一个 iter 的数据:\n",
      "[tensor([[   2,  310, 2439,  ...,    3,    3,    3],\n",
      "        [   2,  542, 1231,  ...,    3,    3,    3],\n",
      "        [   2,  348, 1406,  ...,    3,    3,    3],\n",
      "        ...,\n",
      "        [   2, 2621, 3016,  ...,    3,    3,    3],\n",
      "        [   2, 1283, 2398,  ...,    3,    3,    3],\n",
      "        [   2,  211, 6000,  ...,    3,    3,    3]]), tensor([[ 310, 2439, 5822,  ...,    3,    3,    3],\n",
      "        [ 542, 1231, 1349,  ...,    3,    3,    3],\n",
      "        [ 348, 1406, 1337,  ...,    3,    3,    3],\n",
      "        ...,\n",
      "        [2621, 3016,  607,  ...,    3,    3,    3],\n",
      "        [1283, 2398, 2103,  ...,    3,    3,    3],\n",
      "        [ 211, 6000,  320,  ...,    3,    3,    3]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])]\n",
      "\n",
      "数据集大小：1413103, DataLoader 大小：44160\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# 项目根目录：/data/zyp/jinbu/ZZY/minimind-v-learn\n",
    "root_dir = Path(\"/data/zyp/jinbu/ZZY/minimind-v-learn\")\n",
    "\n",
    "# 将根目录添加到 Python 可搜索路径\n",
    "sys.path.append(str(root_dir))\n",
    "from dataset.lm_dataset import PretrainDataset\n",
    "\n",
    "def Logger(content):\n",
    "    print(content)\n",
    "\n",
    "def init_model(lm_config):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('../model/')\n",
    "    model = MinimindForCausalLM(lm_config).to(train_args.device)\n",
    "    Logger(f'LLM可训练总参数量：{sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.3f} 百万')\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "model, tokenizer = init_model(config)\n",
    "print(model)\n",
    "print(tokenizer)\n",
    "train_ds = PretrainDataset(\n",
    "    data_path=train_args.data_path,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seqlen=train_args.max_seqlen,\n",
    ")   \n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=train_args.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=train_args.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "loader = iter(train_loader)\n",
    "print(f'打印一个 iter 的数据:\\n{next(loader)}\\n')\n",
    "print(f'数据集大小：{len(train_ds)}, DataLoader 大小：{len(loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1a83bd",
   "metadata": {},
   "source": [
    "### 选定优化器和scaler，自动进行混合精度训练加速\n",
    "[常见的optimizer](https://zhuanlan.zhihu.com/p/416979875)<br>\n",
    "[最新的Muon optimizer](https://blog.csdn.net/weixin_44778145/article/details/148722786)<br>\n",
    "[混合精度原理](https://www.cnblogs.com/jimchen1218/p/14315008.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95edb179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化器方面 选择 AdamW 优化器 并在混精度场景下创建 scaler 进行梯度缩放避免数值下溢\n",
    "from torch import optim\n",
    "from contextlib import nullcontext\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(train_args.dtype in ['float16', 'bfloat16']))\n",
    "optimizer = optim.AdamW(model.parameters(), lr=train_args.learning_rate)\n",
    "\n",
    "device_type = \"cuda\" if \"cuda\" in train_args.device else \"cpu\"\n",
    "ctx = nullcontext() if device_type == \"cpu\" else torch.cuda.amp.autocast() # 在 cuda 上启动混精度训练，否则空白上下文"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10952d9d",
   "metadata": {},
   "source": [
    "### 正式进行训练\n",
    "[余弦退火学习率](https://blog.csdn.net/weixin_42392454/article/details/127766771)<br>\n",
    "[CrossEntropy介绍]()<br>\n",
    "[梯度裁剪-clip_grad_norm全解](https://www.hubtools.cn/2025/clip_grad_norm.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65a6722c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import time\n",
    "iter_per_epoch = len(train_loader) # 计算每个 epoch 的迭代次数\n",
    "def get_lr(current_step, total_steps, lr):\n",
    "    # 余弦退火学习率调度\n",
    "    return lr / 10 + 0.5 * lr * (1 + math.cos(math.pi * current_step / total_steps))\n",
    "\n",
    "def train_epoch(epoch):\n",
    "    loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
    "    start_time = time.time()\n",
    "    for step, (X, Y, loss_mask) in enumerate(train_loader):\n",
    "        X = X.to(train_args.device)\n",
    "        Y = Y.to(train_args.device)\n",
    "        loss_mask = loss_mask.to(train_args.device)\n",
    "\n",
    "        lr = get_lr(epoch * iter_per_epoch + step, train_args.epochs * iter_per_epoch, train_args.learning_rate)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        with ctx:\n",
    "            # print(f\"X = {X}\")\n",
    "            # print(f\"res={res}\")\n",
    "            # if torch.isnan(res.logits).any() or torch.isinf(res.logits).any():\n",
    "            #     Logger(f\"Warning: logits contains NaN/Inf at step {step}\")\n",
    "            #     # 打印logits的范围，辅助排查\n",
    "            #     Logger(f\"logits range: {res.logits.min().item()} ~ {res.logits.max().item()}\")\n",
    "            res = model(X)\n",
    "            loss = loss_fct(\n",
    "                res.logits.view(-1, res.logits.size(-1)),\n",
    "                Y.view(-1)\n",
    "            ).view(Y.size())\n",
    "            # print(f\"loss_mask.sum(): {loss_mask.sum()}\")\n",
    "            loss = (loss * loss_mask).sum() / loss_mask.sum() # 这里的loss 是有效非pad的token的平均loss\n",
    "            # loss += res.aux_loss\n",
    "            loss = loss / train_args.accumulation_steps\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % train_args.accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), train_args.grad_clip)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)  # 清空梯度，为下一个iter做准备\n",
    "\n",
    "        if step % train_args.log_interval == 0:\n",
    "            spend_time = time.time() - start_time\n",
    "            Logger(\n",
    "                'Epoch:[{}/{}]({}/{}) loss:{:.3f} lr:{:.12f} epoch_Time:{}min:'.format(\n",
    "                    epoch + 1,\n",
    "                    train_args.epochs,\n",
    "                    step,\n",
    "                    iter_per_epoch,\n",
    "                    loss.item() * train_args.accumulation_steps,\n",
    "                    optimizer.param_groups[-1]['lr'],\n",
    "                    spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60))\n",
    "\n",
    "\n",
    "        if (step + 1) % train_args.save_interval == 0:\n",
    "            model.eval()\n",
    "            moe_path = '_moe' if train_args.use_moe else ''\n",
    "            ckp = f'{train_args.save_dir}/pretrain_{config.embed_dim}{moe_path}.pth'\n",
    "            Logger(f'保存模型到 {ckp}')\n",
    "            state_dict = model.state_dict()\n",
    "\n",
    "            state_dict = {k: v.half() for k, v in state_dict.items()}  # 半精度保存\n",
    "            torch.save(state_dict, ckp)\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e3a627",
   "metadata": {},
   "source": [
    "## 开始训练！ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25424934",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/zyp/jinbu/ZZY/minimind-v-learn/dataset/lm_dataset.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X=torch.tensor(input_ids[:-1],dtype=torch.long)  # 去掉最后一个token\n",
      "/data/zyp/jinbu/ZZY/minimind-v-learn/dataset/lm_dataset.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y=torch.tensor(input_ids[1:],dtype=torch.long)  # 去掉第一个\n",
      "/data/zyp/jinbu/ZZY/minimind-v-learn/dataset/lm_dataset.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss_mask = torch.tensor(loss_mask[1:], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[1/1](0/44160) loss:8.945 lr:0.000550000000 epoch_Time:1249.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](100/44160) loss:7.218 lr:0.000549993674 epoch_Time:366.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](200/44160) loss:6.730 lr:0.000549974695 epoch_Time:366.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](300/44160) loss:6.586 lr:0.000549943065 epoch_Time:390.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](400/44160) loss:6.179 lr:0.000549898786 epoch_Time:382.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](500/44160) loss:5.908 lr:0.000549841859 epoch_Time:382.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](600/44160) loss:5.830 lr:0.000549772287 epoch_Time:390.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](700/44160) loss:5.571 lr:0.000549690074 epoch_Time:383.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](800/44160) loss:5.368 lr:0.000549595224 epoch_Time:386.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](900/44160) loss:5.358 lr:0.000549487743 epoch_Time:385.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](1000/44160) loss:5.302 lr:0.000549367634 epoch_Time:381.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](1100/44160) loss:5.136 lr:0.000549234905 epoch_Time:385.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](1200/44160) loss:4.917 lr:0.000549089562 epoch_Time:385.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](1300/44160) loss:5.000 lr:0.000548931613 epoch_Time:381.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](1400/44160) loss:4.849 lr:0.000548761065 epoch_Time:385.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](1500/44160) loss:4.794 lr:0.000548577927 epoch_Time:383.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](1600/44160) loss:4.637 lr:0.000548382208 epoch_Time:380.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](1700/44160) loss:4.629 lr:0.000548173919 epoch_Time:383.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](1800/44160) loss:4.771 lr:0.000547953069 epoch_Time:380.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](1900/44160) loss:4.603 lr:0.000547719671 epoch_Time:378.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](2000/44160) loss:4.570 lr:0.000547473735 epoch_Time:380.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](2100/44160) loss:4.590 lr:0.000547215275 epoch_Time:380.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](2200/44160) loss:4.432 lr:0.000546944303 epoch_Time:378.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](2300/44160) loss:4.381 lr:0.000546660833 epoch_Time:369.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](2400/44160) loss:4.266 lr:0.000546364879 epoch_Time:363.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](2500/44160) loss:4.193 lr:0.000546056457 epoch_Time:365.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](2600/44160) loss:4.357 lr:0.000545735582 epoch_Time:364.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](2700/44160) loss:4.159 lr:0.000545402270 epoch_Time:362.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](2800/44160) loss:4.256 lr:0.000545056538 epoch_Time:364.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](2900/44160) loss:4.148 lr:0.000544698404 epoch_Time:362.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](3000/44160) loss:4.064 lr:0.000544327885 epoch_Time:360.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](3100/44160) loss:3.961 lr:0.000543945001 epoch_Time:362.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](3200/44160) loss:4.001 lr:0.000543549771 epoch_Time:360.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](3300/44160) loss:4.128 lr:0.000543142214 epoch_Time:359.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](3400/44160) loss:4.074 lr:0.000542722352 epoch_Time:359.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](3500/44160) loss:3.835 lr:0.000542290206 epoch_Time:358.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](3600/44160) loss:3.919 lr:0.000541845797 epoch_Time:358.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](3700/44160) loss:4.035 lr:0.000541389149 epoch_Time:358.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](3800/44160) loss:3.926 lr:0.000540920283 epoch_Time:356.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](3900/44160) loss:3.717 lr:0.000540439225 epoch_Time:356.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](4000/44160) loss:3.874 lr:0.000539945998 epoch_Time:355.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](4100/44160) loss:3.926 lr:0.000539440627 epoch_Time:353.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](4200/44160) loss:3.857 lr:0.000538923138 epoch_Time:353.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](4300/44160) loss:3.776 lr:0.000538393557 epoch_Time:352.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](4400/44160) loss:3.644 lr:0.000537851910 epoch_Time:352.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](4500/44160) loss:3.827 lr:0.000537298226 epoch_Time:352.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](4600/44160) loss:3.667 lr:0.000536732532 epoch_Time:351.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](4700/44160) loss:3.892 lr:0.000536154857 epoch_Time:349.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](4800/44160) loss:3.682 lr:0.000535565231 epoch_Time:350.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](4900/44160) loss:3.486 lr:0.000534963682 epoch_Time:348.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](5000/44160) loss:3.363 lr:0.000534350241 epoch_Time:347.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](5100/44160) loss:3.603 lr:0.000533724940 epoch_Time:347.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](5200/44160) loss:3.635 lr:0.000533087810 epoch_Time:346.0min:\n",
      "保存模型到 ../out/pretrain_512.pth\n",
      "Epoch:[1/1](5300/44160) loss:3.605 lr:0.000532438883 epoch_Time:345.0min:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(train_args\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     10\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (X, Y, loss_mask) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m---> 12\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     Y \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39mto(train_args\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     14\u001b[0m     loss_mask \u001b[38;5;241m=\u001b[39m loss_mask\u001b[38;5;241m.\u001b[39mto(train_args\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(train_args.epochs):\n",
    "    train_epoch(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d87f36e",
   "metadata": {},
   "source": [
    "# !重大问题，loss 怎么是Nan????"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f59696",
   "metadata": {},
   "source": [
    "由于python3.9无法用vscode调试，我又不想用pdb慢慢调，直接print每一层结果来核对吧"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4349b8cc",
   "metadata": {},
   "source": [
    "经过排查发现，是由于attn_mask 的上三角mask我写成了下三角mask，且mask未设置为-inf而是设置为-1e9 导致的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac3f481",
   "metadata": {},
   "source": [
    "### 调用预训练一部分的model generate一下\n",
    "可以看到已经初步学会了说话了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "689a4ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinimindForCausalLM(\n",
      "  (embed): Embed(\n",
      "    (embedding): Embedding(6400, 512)\n",
      "  )\n",
      "  (rmsnorm): RMSNorm()\n",
      "  (minimind_dense): Minimind_Dense(\n",
      "    (blocks): ModuleList(\n",
      "      (0-7): 8 x Minimind_Block(\n",
      "        (attention): GroupQueryAttention(\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (k_proj): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (o_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (res_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (rmsnorm1): RMSNorm()\n",
      "        (ffn): FeedForward(\n",
      "          (gate): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (up_proj): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (down_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (rmsnorm2): RMSNorm()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=6400, bias=True)\n",
      ")\n",
      "加载模型参数 ../out/pretrain_512.pth\n",
      "<|im_start|>鉴别一组中文文章的风格和特点<|im_end|> \n",
      "<|im_start|>根据输入的内容，编写一个类别标签。<|im_end|> <|im_start|>\n",
      "gernerating new token: idx = 11\n",
      "gernerating new token: idx = 12\n",
      "gernerating new token: idx = 13\n",
      "gernerating new token: idx = 14\n",
      "gernerating new token: idx = 15\n",
      "gernerating new token: idx = 16\n",
      "gernerating new token: idx = 17\n",
      "gernerating new token: idx = 18\n",
      "gernerating new token: idx = 19\n",
      "gernerating new token: idx = 20\n",
      "gernerating new token: idx = 21\n",
      "gernerating new token: idx = 22\n",
      "gernerating new token: idx = 23\n",
      "gernerating new token: idx = 24\n",
      "gernerating new token: idx = 25\n",
      "gernerating new token: idx = 26\n",
      "new tokens list :[tensor([[2]]), tensor([[588]]), tensor([[1421]]), tensor([[1657]]), tensor([[331]]), tensor([[4983]]), tensor([[2938]]), tensor([[3249]]), tensor([[275]]), tensor([[211]]), tensor([[884]]), tensor([[798]]), tensor([[1152]]), tensor([[3475]]), tensor([[275]]), tensor([[1]])]\n",
      "\n",
      "gernerating new token: idx = 11\n",
      "gernerating new token: idx = 12\n",
      "gernerating new token: idx = 13\n",
      "gernerating new token: idx = 14\n",
      "gernerating new token: idx = 15\n",
      "gernerating new token: idx = 16\n",
      "gernerating new token: idx = 17\n",
      "gernerating new token: idx = 18\n",
      "gernerating new token: idx = 19\n",
      "gernerating new token: idx = 20\n",
      "gernerating new token: idx = 21\n",
      "gernerating new token: idx = 22\n",
      "gernerating new token: idx = 23\n",
      "gernerating new token: idx = 24\n",
      "gernerating new token: idx = 25\n",
      "gernerating new token: idx = 26\n",
      "gernerating new token: idx = 27\n",
      "new tokens list :[tensor([[4240]]), tensor([[757]]), tensor([[1720]]), tensor([[1192]]), tensor([[275]]), tensor([[211]]), tensor([[4614]]), tensor([[927]]), tensor([[1748]]), tensor([[1622]]), tensor([[273]]), tensor([[2474]]), tensor([[626]]), tensor([[821]]), tensor([[1244]]), tensor([[275]]), tensor([[1]])]\n",
      "\n",
      "torch.Size([2, 28])\n",
      "输入的文本<|im_start|>鉴别一组中文文章的风格和特点<|im_end|> \n",
      "解码后的文本：<|im_start|>鉴别一组中文文章的风格和特点<|im_end|> <|im_start|>从下面的句子中删除所有的形容词。\n",
      "她喜欢打篮球。<|im_end|><pad>\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# 项目根目录：/data/zyp/jinbu/ZZY/minimind-v-learn\n",
    "root_dir = Path(\"/data/zyp/jinbu/ZZY/minimind-v-learn\")\n",
    "from model import MinimindForCausalLM, MinimindConfig\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "config_dense = MinimindConfig()\n",
    "model = MinimindForCausalLM(config_dense)\n",
    "print(model)\n",
    "tokenizer = AutoTokenizer.from_pretrained('../model/')\n",
    "# 加载预训练模型\n",
    "ckp = f'{train_args.save_dir}/pretrain_{config.embed_dim}.pth'\n",
    "print(f'加载模型参数 {ckp}')\n",
    "state_dict = torch.load(ckp, map_location=train_args.device)\n",
    "# 将模型参数加载到模型中\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model.eval()\n",
    "data=[\n",
    "        {'text':'<|im_start|>鉴别一组中文文章的风格和特点<|im_end|> '\n",
    "    },\n",
    "        {'text':'<|im_start|>根据输入的内容，编写一个类别标签。<|im_end|> <|im_start|>'\n",
    "    }\n",
    "    ]\n",
    "for i in range(2):\n",
    "    print(data[i]['text'])\n",
    "\n",
    "# 接下来将该data的内容利用tokenizer编码\n",
    "input_texts = [item['text'] for item in data]\n",
    "#填充1 固定填充\n",
    "input_ids = tokenizer(input_texts, padding='max_length', truncation=True, max_length=1024,return_tensors='pt')\n",
    "# 生成新的token\n",
    "outputs = model.generate(input_ids=input_ids['input_ids'], max_new_tokens=600, use_cache=True,)\n",
    "# 从最初的tokenizer解码outputs中取logits进行解码\n",
    "# print(outputs)  # 输出形状应为 (batch_size, sequence_length + max_new_tokens)\n",
    "print(outputs.shape)  # 输出形状应为 (batch_size, sequence_length + max_new_tokens)\n",
    "resword=tokenizer.decode(outputs[0], skip_special_tokens=False)  # 解码第一个batch的输出\n",
    "print(f\"输入的文本{input_texts[0]}\")\n",
    "print(f\"解码后的文本：{resword}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af7f0e8",
   "metadata": {},
   "source": [
    "###  分布式训练之后的成果展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3a0832b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinimindForCausalLM(\n",
      "  (embed): Embed(\n",
      "    (embedding): Embedding(6400, 512)\n",
      "  )\n",
      "  (rmsnorm): RMSNorm()\n",
      "  (minimind_dense): Minimind_Dense(\n",
      "    (blocks): ModuleList(\n",
      "      (0-7): 8 x Minimind_Block(\n",
      "        (attention): GroupQueryAttention(\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (k_proj): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (o_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (res_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (rmsnorm1): RMSNorm()\n",
      "        (ffn): FeedForward(\n",
      "          (gate): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (up_proj): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (down_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (rmsnorm2): RMSNorm()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=6400, bias=True)\n",
      ")\n",
      "加载模型参数 ../out/pretrain_512.pth\n",
      "<|im_start|>鉴别一组中文文章的风格和特点<|im_end|> \n",
      "<|im_start|>根据输入的内容，编写一个类别标签。<|im_end|> <|im_start|>\n",
      "gernerating new token: idx = 11\n",
      "gernerating new token: idx = 12\n",
      "gernerating new token: idx = 13\n",
      "gernerating new token: idx = 14\n",
      "gernerating new token: idx = 15\n",
      "gernerating new token: idx = 16\n",
      "gernerating new token: idx = 17\n",
      "gernerating new token: idx = 18\n",
      "gernerating new token: idx = 19\n",
      "gernerating new token: idx = 20\n",
      "gernerating new token: idx = 21\n",
      "gernerating new token: idx = 22\n",
      "gernerating new token: idx = 23\n",
      "gernerating new token: idx = 24\n",
      "gernerating new token: idx = 25\n",
      "gernerating new token: idx = 26\n",
      "gernerating new token: idx = 27\n",
      "gernerating new token: idx = 28\n",
      "gernerating new token: idx = 29\n",
      "gernerating new token: idx = 30\n",
      "gernerating new token: idx = 31\n",
      "gernerating new token: idx = 32\n",
      "gernerating new token: idx = 33\n",
      "gernerating new token: idx = 34\n",
      "gernerating new token: idx = 35\n",
      "gernerating new token: idx = 36\n",
      "gernerating new token: idx = 37\n",
      "gernerating new token: idx = 38\n",
      "gernerating new token: idx = 39\n",
      "gernerating new token: idx = 40\n",
      "gernerating new token: idx = 41\n",
      "gernerating new token: idx = 42\n",
      "gernerating new token: idx = 43\n",
      "gernerating new token: idx = 44\n",
      "gernerating new token: idx = 45\n",
      "gernerating new token: idx = 46\n",
      "gernerating new token: idx = 47\n",
      "gernerating new token: idx = 48\n",
      "gernerating new token: idx = 49\n",
      "gernerating new token: idx = 50\n",
      "gernerating new token: idx = 51\n",
      "gernerating new token: idx = 52\n",
      "gernerating new token: idx = 53\n",
      "gernerating new token: idx = 54\n",
      "gernerating new token: idx = 55\n",
      "gernerating new token: idx = 56\n",
      "gernerating new token: idx = 57\n",
      "gernerating new token: idx = 58\n",
      "gernerating new token: idx = 59\n",
      "new tokens list :[tensor([[2]]), tensor([[405]]), tensor([[894]]), tensor([[4088]]), tensor([[845]]), tensor([[5098]]), tensor([[1014]]), tensor([[273]]), tensor([[2052]]), tensor([[840]]), tensor([[426]]), tensor([[322]]), tensor([[273]]), tensor([[1895]]), tensor([[1170]]), tensor([[471]]), tensor([[777]]), tensor([[387]]), tensor([[4259]]), tensor([[275]]), tensor([[1436]]), tensor([[273]]), tensor([[1611]]), tensor([[2014]]), tensor([[481]]), tensor([[386]]), tensor([[5435]]), tensor([[1014]]), tensor([[273]]), tensor([[522]]), tensor([[1170]]), tensor([[471]]), tensor([[3466]]), tensor([[4644]]), tensor([[2639]]), tensor([[478]]), tensor([[129]]), tensor([[582]]), tensor([[273]]), tensor([[1112]]), tensor([[1170]]), tensor([[471]]), tensor([[778]]), tensor([[392]]), tensor([[1720]]), tensor([[4259]]), tensor([[412]]), tensor([[275]]), tensor([[1]])]\n",
      "\n",
      "gernerating new token: idx = 11\n",
      "gernerating new token: idx = 12\n",
      "gernerating new token: idx = 13\n",
      "gernerating new token: idx = 14\n",
      "gernerating new token: idx = 15\n",
      "gernerating new token: idx = 16\n",
      "gernerating new token: idx = 17\n",
      "gernerating new token: idx = 18\n",
      "gernerating new token: idx = 19\n",
      "gernerating new token: idx = 20\n",
      "gernerating new token: idx = 21\n",
      "gernerating new token: idx = 22\n",
      "gernerating new token: idx = 23\n",
      "gernerating new token: idx = 24\n",
      "gernerating new token: idx = 25\n",
      "gernerating new token: idx = 26\n",
      "gernerating new token: idx = 27\n",
      "gernerating new token: idx = 28\n",
      "gernerating new token: idx = 29\n",
      "gernerating new token: idx = 30\n",
      "gernerating new token: idx = 31\n",
      "gernerating new token: idx = 32\n",
      "gernerating new token: idx = 33\n",
      "gernerating new token: idx = 34\n",
      "gernerating new token: idx = 35\n",
      "gernerating new token: idx = 36\n",
      "gernerating new token: idx = 37\n",
      "gernerating new token: idx = 38\n",
      "gernerating new token: idx = 39\n",
      "gernerating new token: idx = 40\n",
      "gernerating new token: idx = 41\n",
      "gernerating new token: idx = 42\n",
      "gernerating new token: idx = 43\n",
      "gernerating new token: idx = 44\n",
      "gernerating new token: idx = 45\n",
      "gernerating new token: idx = 46\n",
      "gernerating new token: idx = 47\n",
      "gernerating new token: idx = 48\n",
      "gernerating new token: idx = 49\n",
      "gernerating new token: idx = 50\n",
      "gernerating new token: idx = 51\n",
      "gernerating new token: idx = 52\n",
      "gernerating new token: idx = 53\n",
      "gernerating new token: idx = 54\n",
      "gernerating new token: idx = 55\n",
      "gernerating new token: idx = 56\n",
      "gernerating new token: idx = 57\n",
      "gernerating new token: idx = 58\n",
      "gernerating new token: idx = 59\n",
      "gernerating new token: idx = 60\n",
      "gernerating new token: idx = 61\n",
      "gernerating new token: idx = 62\n",
      "gernerating new token: idx = 63\n",
      "gernerating new token: idx = 64\n",
      "gernerating new token: idx = 65\n",
      "gernerating new token: idx = 66\n",
      "gernerating new token: idx = 67\n",
      "gernerating new token: idx = 68\n",
      "gernerating new token: idx = 69\n",
      "gernerating new token: idx = 70\n",
      "gernerating new token: idx = 71\n",
      "gernerating new token: idx = 72\n",
      "gernerating new token: idx = 73\n",
      "gernerating new token: idx = 74\n",
      "gernerating new token: idx = 75\n",
      "gernerating new token: idx = 76\n",
      "gernerating new token: idx = 77\n",
      "gernerating new token: idx = 78\n",
      "gernerating new token: idx = 79\n",
      "gernerating new token: idx = 80\n",
      "gernerating new token: idx = 81\n",
      "gernerating new token: idx = 82\n",
      "gernerating new token: idx = 83\n",
      "gernerating new token: idx = 84\n",
      "gernerating new token: idx = 85\n",
      "gernerating new token: idx = 86\n",
      "gernerating new token: idx = 87\n",
      "gernerating new token: idx = 88\n",
      "gernerating new token: idx = 89\n",
      "gernerating new token: idx = 90\n",
      "gernerating new token: idx = 91\n",
      "gernerating new token: idx = 92\n",
      "gernerating new token: idx = 93\n",
      "gernerating new token: idx = 94\n",
      "gernerating new token: idx = 95\n",
      "gernerating new token: idx = 96\n",
      "gernerating new token: idx = 97\n",
      "gernerating new token: idx = 98\n",
      "gernerating new token: idx = 99\n",
      "gernerating new token: idx = 100\n",
      "gernerating new token: idx = 101\n",
      "gernerating new token: idx = 102\n",
      "gernerating new token: idx = 103\n",
      "gernerating new token: idx = 104\n",
      "gernerating new token: idx = 105\n",
      "gernerating new token: idx = 106\n",
      "gernerating new token: idx = 107\n",
      "gernerating new token: idx = 108\n",
      "gernerating new token: idx = 109\n",
      "gernerating new token: idx = 110\n",
      "gernerating new token: idx = 111\n",
      "gernerating new token: idx = 112\n",
      "gernerating new token: idx = 113\n",
      "gernerating new token: idx = 114\n",
      "gernerating new token: idx = 115\n",
      "gernerating new token: idx = 116\n",
      "gernerating new token: idx = 117\n",
      "gernerating new token: idx = 118\n",
      "gernerating new token: idx = 119\n",
      "gernerating new token: idx = 120\n",
      "gernerating new token: idx = 121\n",
      "gernerating new token: idx = 122\n",
      "gernerating new token: idx = 123\n",
      "gernerating new token: idx = 124\n",
      "gernerating new token: idx = 125\n",
      "gernerating new token: idx = 126\n",
      "gernerating new token: idx = 127\n",
      "gernerating new token: idx = 128\n",
      "gernerating new token: idx = 129\n",
      "gernerating new token: idx = 130\n",
      "gernerating new token: idx = 131\n",
      "gernerating new token: idx = 132\n",
      "gernerating new token: idx = 133\n",
      "gernerating new token: idx = 134\n",
      "gernerating new token: idx = 135\n",
      "gernerating new token: idx = 136\n",
      "gernerating new token: idx = 137\n",
      "gernerating new token: idx = 138\n",
      "gernerating new token: idx = 139\n",
      "gernerating new token: idx = 140\n",
      "gernerating new token: idx = 141\n",
      "gernerating new token: idx = 142\n",
      "gernerating new token: idx = 143\n",
      "gernerating new token: idx = 144\n",
      "gernerating new token: idx = 145\n",
      "gernerating new token: idx = 146\n",
      "new tokens list :[tensor([[542]]), tensor([[479]]), tensor([[569]]), tensor([[1318]]), tensor([[1709]]), tensor([[3725]]), tensor([[3857]]), tensor([[3937]]), tensor([[275]]), tensor([[211]]), tensor([[1075]]), tensor([[308]]), tensor([[1201]]), tensor([[308]]), tensor([[1075]]), tensor([[308]]), tensor([[1075]]), tensor([[308]]), tensor([[1075]]), tensor([[308]]), tensor([[1075]]), tensor([[308]]), tensor([[1075]]), tensor([[308]]), tensor([[1201]]), tensor([[308]]), tensor([[1075]]), tensor([[308]]), tensor([[1075]]), tensor([[308]]), tensor([[1201]]), tensor([[308]]), tensor([[1075]]), tensor([[308]]), tensor([[1075]]), tensor([[308]]), tensor([[1075]]), tensor([[308]]), tensor([[1075]]), tensor([[308]]), tensor([[1075]]), tensor([[308]]), tensor([[1075]]), tensor([[308]]), tensor([[1075]]), tensor([[308]]), tensor([[1075]]), tensor([[308]]), tensor([[1201]]), tensor([[308]]), tensor([[1075]]), tensor([[308]]), tensor([[1075]]), tensor([[308]]), tensor([[1201]]), tensor([[211]]), tensor([[1075]]), tensor([[308]]), tensor([[1075]]), tensor([[308]]), tensor([[1201]]), tensor([[308]]), tensor([[1075]]), tensor([[308]]), tensor([[1075]]), tensor([[308]]), tensor([[1075]]), tensor([[211]]), tensor([[1075]]), tensor([[308]]), tensor([[1075]]), tensor([[308]]), tensor([[1075]]), tensor([[308]]), tensor([[1201]]), tensor([[211]]), tensor([[1075]]), tensor([[308]]), tensor([[1075]]), tensor([[308]]), tensor([[1201]]), tensor([[308]]), tensor([[1075]]), tensor([[308]]), tensor([[1201]]), tensor([[308]]), tensor([[1075]]), tensor([[211]]), tensor([[1075]]), tensor([[308]]), tensor([[1075]]), tensor([[308]]), tensor([[1075]]), tensor([[211]]), tensor([[1075]]), tensor([[308]]), tensor([[1075]]), tensor([[308]]), tensor([[1075]]), tensor([[308]]), tensor([[1075]]), tensor([[211]]), tensor([[1075]]), tensor([[308]]), tensor([[1075]]), tensor([[308]]), tensor([[1201]]), tensor([[211]]), tensor([[1075]]), tensor([[308]]), tensor([[1075]]), tensor([[308]]), tensor([[1201]]), tensor([[308]]), tensor([[1075]]), tensor([[211]]), tensor([[1075]]), tensor([[211]]), tensor([[1075]]), tensor([[308]]), tensor([[1201]]), tensor([[211]]), tensor([[1075]]), tensor([[308]]), tensor([[1201]]), tensor([[211]]), tensor([[1075]]), tensor([[308]]), tensor([[1201]]), tensor([[211]]), tensor([[1075]]), tensor([[308]]), tensor([[1075]]), tensor([[308]]), tensor([[1075]]), tensor([[1]])]\n",
      "\n",
      "torch.Size([2, 147])\n",
      "输入的文本<|im_start|>鉴别一组中文文章的风格和特点<|im_end|> \n",
      "解码后的文本：<|im_start|>鉴别一组中文文章的风格和特点<|im_end|> <|im_start|>你应该怎样做一道菜，不要太多了，帮我把它改成沙拉。当然，如果你想要加一个美味的菜，需要把它放入意大利面煮沸后，再把它放到一份沙拉上。<|im_end|><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# 项目根目录：/data/zyp/jinbu/ZZY/minimind-v-learn\n",
    "root_dir = Path(\"/data/zyp/jinbu/ZZY/minimind-v-learn\")\n",
    "from model import MinimindForCausalLM, MinimindConfig\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "config_dense = MinimindConfig()\n",
    "model = MinimindForCausalLM(config_dense)\n",
    "print(model)\n",
    "tokenizer = AutoTokenizer.from_pretrained('../model/')\n",
    "# 加载预训练模型\n",
    "ckp = f'{train_args.save_dir}/pretrain_{config.embed_dim}.pth'\n",
    "print(f'加载模型参数 {ckp}')\n",
    "state_dict = torch.load(ckp, map_location=train_args.device)\n",
    "# 将模型参数加载到模型中\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model.eval()\n",
    "data=[\n",
    "        {'text':'<|im_start|>鉴别一组中文文章的风格和特点<|im_end|> '\n",
    "    },\n",
    "        {'text':'<|im_start|>根据输入的内容，编写一个类别标签。<|im_end|> <|im_start|>'\n",
    "    }\n",
    "    ]\n",
    "for i in range(2):\n",
    "    print(data[i]['text'])\n",
    "\n",
    "# 接下来将该data的内容利用tokenizer编码\n",
    "input_texts = [item['text'] for item in data]\n",
    "#填充1 固定填充\n",
    "input_ids = tokenizer(input_texts, padding='max_length', truncation=True, max_length=1024,return_tensors='pt')\n",
    "# 生成新的token\n",
    "outputs = model.generate(input_ids=input_ids['input_ids'], max_new_tokens=600, use_cache=True,)\n",
    "# 从最初的tokenizer解码outputs中取logits进行解码\n",
    "# print(outputs)  # 输出形状应为 (batch_size, sequence_length + max_new_tokens)\n",
    "print(outputs.shape)  # 输出形状应为 (batch_size, sequence_length + max_new_tokens)\n",
    "resword=tokenizer.decode(outputs[0], skip_special_tokens=False)  # 解码第一个batch的输出\n",
    "print(f\"输入的文本{input_texts[0]}\")\n",
    "print(f\"解码后的文本：{resword}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d9973f",
   "metadata": {},
   "source": [
    "# 关于vLLM部署\n",
    "vLLM是用于改进LLM的generate方法效率不足的问题的，可以参考b站视频BV1kx4y1x7bu的讲解<br>\n",
    "其主要原理是基于page attention和维护一个共有映射表，模仿操作系统来维护显存的<br>\n",
    "[vLLM快速入门](https://blog.csdn.net/weixin_42475060/article/details/148557753)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
