{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ab1dfe8",
   "metadata": {},
   "source": [
    "# 用单机多卡分布式训练进入pretrain阶段\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6453eb9d",
   "metadata": {},
   "source": [
    "[1.数据并行\\流水线并行\\张量并行的概念](https://www.cnblogs.com/Big-Yellow/p/18646083)<br>\n",
    "[2.reduce的概念详解](https://zhuanlan.zhihu.com/p/17201336684)<br>\n",
    "[3.DP/DDP原理](https://zhuanlan.zhihu.com/p/572454730)<br>\n",
    "[4.快速扫清dp/ddp概念](https://blog.csdn.net/ytusdc/article/details/122091284)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa8710b",
   "metadata": {},
   "source": [
    "# 常见的分布式训练框架\n",
    "[deepspeed讲解](https://blog.csdn.net/zwqjoy/article/details/130732601)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca64a3d9",
   "metadata": {},
   "source": [
    "# 对pretrain进行DDP训练\n",
    "- 加载model到各个GPU\n",
    "- GPU初始参数set seed\n",
    "- scale&gather\n",
    "- 训练结束"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4a9e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里只展示完整的ddp改造后的代码全貌\n",
    "import math\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from contextlib import nullcontext\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 获取当前 notebook 所在目录（trainer/）\n",
    "current_dir = os.path.dirname(os.path.abspath(\"__file__\"))  # 注意 Jupyter 中可能需要调整\n",
    "# 或者直接写死路径\n",
    "current_dir = \"/data/zyp/jinbu/ZZY/minimind-v-learn/trainer\"\n",
    "# 上一级目录就是项目根目录，拼接 model 路径\n",
    "model_dir = os.path.join(os.path.dirname(current_dir), \"model\")\n",
    "sys.path.append(model_dir)\n",
    "# 现在可以用绝对导入\n",
    "from model import MinimindForCausalLM, MinimindConfig\n",
    "\n",
    "from pathlib import Path\n",
    "# 项目根目录：/data/zyp/jinbu/ZZY/minimind-v-learn\n",
    "root_dir = Path(\"/data/zyp/jinbu/ZZY/minimind-v-learn\")\n",
    "# 将根目录添加到 Python 可搜索路径\n",
    "sys.path.append(str(root_dir))\n",
    "from dataset.lm_dataset import PretrainDataset\n",
    "\n",
    "\n",
    "\n",
    "class pretrain_args:\n",
    "    out_dir = \"../out\"\n",
    "    epochs = 1\n",
    "    batch_size = 32\n",
    "    learning_rate = 5e-4\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dtype = \"bfloat16\"\n",
    "    use_wandb = False\n",
    "    wandb_project = \"MiniMind-Pretrain\"\n",
    "    num_workers = 1\n",
    "    ddp = False\n",
    "    accumulation_steps = 8\n",
    "    grad_clip = 1.0\n",
    "    warmup_iters = 0\n",
    "    log_interval = 100\n",
    "    save_interval = 100\n",
    "    local_rank = -1\n",
    "    embed_dim = 512\n",
    "    block_num = 8\n",
    "    max_seqlen = 1024\n",
    "    use_moe = False\n",
    "    # data_path = \"../data/pretrain_data.jsonl\" #toy_dataset\n",
    "    data_path = \"../data/pretrain_hq.jsonl\" #full_dataset\n",
    "\n",
    "\n",
    "def Logger(content):\n",
    "    # ddp改造后如果是ddp模式就不进行logger打印\n",
    "    if train_args.ddp and train_args.local_rank != 0:\n",
    "        return\n",
    "    print(content)\n",
    "\n",
    "def init_model(lm_config):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('../model/')\n",
    "    model = MinimindForCausalLM(lm_config).to(train_args.device)\n",
    "    Logger(f'LLM可训练总参数量：{sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.3f} 百万')\n",
    "    return model, tokenizer\n",
    "\n",
    "# ddp的核心库\n",
    "import torch.distributed as dist \n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data import DistributedSampler\n",
    "# 初始化分布式环境\n",
    "def init_distributed_mode():\n",
    "    if not ddp: return\n",
    "    global ddp_local_rank, DEVICE\n",
    "\n",
    "    dist.init_process_group(backend=\"nccl\")\n",
    "    ddp_rank = int(os.environ[\"RANK\"])\n",
    "    ddp_local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    ddp_world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "    DEVICE = f\"cuda:{ddp_local_rank}\"\n",
    "    torch.cuda.set_device(DEVICE)\n",
    "\n",
    "\n",
    "def get_lr(current_step, total_steps, lr):\n",
    "    # 余弦退火学习率调度\n",
    "    return lr / 10 + 0.5 * lr * (1 + math.cos(math.pi * current_step / total_steps))\n",
    "\n",
    "def train_epoch(epoch):\n",
    "    loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
    "    start_time = time.time()\n",
    "    for step, (X, Y, loss_mask) in enumerate(train_loader):\n",
    "        X = X.to(train_args.device)\n",
    "        Y = Y.to(train_args.device)\n",
    "        loss_mask = loss_mask.to(train_args.device)\n",
    "\n",
    "        lr = get_lr(epoch * iter_per_epoch + step, train_args.epochs * iter_per_epoch, train_args.learning_rate)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        with ctx:\n",
    "            # print(f\"X = {X}\")\n",
    "            # print(f\"res={res}\")\n",
    "            # if torch.isnan(res.logits).any() or torch.isinf(res.logits).any():\n",
    "            #     Logger(f\"Warning: logits contains NaN/Inf at step {step}\")\n",
    "            #     # 打印logits的范围，辅助排查\n",
    "            #     Logger(f\"logits range: {res.logits.min().item()} ~ {res.logits.max().item()}\")\n",
    "            res = model(X)\n",
    "            loss = loss_fct(\n",
    "                res.logits.view(-1, res.logits.size(-1)),\n",
    "                Y.view(-1)\n",
    "            ).view(Y.size())\n",
    "            # print(f\"loss_mask.sum(): {loss_mask.sum()}\")\n",
    "            loss = (loss * loss_mask).sum() / loss_mask.sum() # 这里的loss 是有效非pad的token的平均loss\n",
    "            # loss += res.aux_loss\n",
    "            loss = loss / train_args.accumulation_steps\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % train_args.accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), train_args.grad_clip)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)  # 清空梯度，为下一个iter做准备\n",
    "\n",
    "        if step % train_args.log_interval == 0:\n",
    "            spend_time = time.time() - start_time\n",
    "            Logger(\n",
    "                'Epoch:[{}/{}]({}/{}) loss:{:.3f} lr:{:.12f} epoch_Time:{}min:'.format(\n",
    "                    epoch + 1,\n",
    "                    train_args.epochs,\n",
    "                    step,\n",
    "                    iter_per_epoch,\n",
    "                    loss.item() * train_args.accumulation_steps,\n",
    "                    optimizer.param_groups[-1]['lr'],\n",
    "                    spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60))\n",
    "\n",
    "\n",
    "        if (step + 1) % train_args.save_interval == 0:\n",
    "            model.eval()\n",
    "            moe_path = '_moe' if train_args.use_moe else ''\n",
    "            ckp = f'{train_args.save_dir}/pretrain_{config.embed_dim}{moe_path}.pth'\n",
    "            Logger(f'保存模型到 {ckp}')\n",
    "            # 增加一个ddp的判断，因为ddp模式下模型被封装在 model.module 中\n",
    "            if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
    "                state_dict = model.module.state_dict()\n",
    "            else:\n",
    "                state_dict = model.state_dict()\n",
    "\n",
    "            state_dict = {k: v.half() for k, v in state_dict.items()}  # 半精度保存\n",
    "            torch.save(state_dict, ckp)\n",
    "            model.train()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 首先是训练参数设定\n",
    "    train_args = pretrain_args()\n",
    "    train_args.save_dir = os.path.join(train_args.out_dir)\n",
    "    # 确保输出目录存在\n",
    "    os.makedirs(train_args.save_dir, exist_ok=True)\n",
    "    # 初始化模型配置\n",
    "    config = MinimindConfig(\n",
    "        embed_dim=train_args.embed_dim,\n",
    "        block_num=train_args.block_num,\n",
    "        max_seqlen=train_args.max_seqlen,\n",
    "    )\n",
    "    print(f'查看工作设备 {train_args.device}')\n",
    "\n",
    "    # runtime初始化\n",
    "    device_type = \"cuda\" if \"cuda\" in train_args.device else \"cpu\"\n",
    "    ctx = nullcontext() if device_type == \"cpu\" else torch.cuda.amp.autocast() # 在 cuda 上启动混精度训练，否则空白上下文\n",
    "\n",
    "    tokens_per_iter = args.batch_size * args.max_seqlen # 每次迭代处理的 token 数\n",
    "    iter_per_epoch = len(train_loader) # 计算每个 epoch 的迭代次数\n",
    "    # ddp初始化\n",
    "    ddp = int(os.environ.get(\"RANK\", -1)) != -1  # is this a ddp run?\n",
    "    ddp_local_rank, DEVICE = 0, \"cuda:0\"\n",
    "    # 如果是ddp模式，进行seed设定\n",
    "    base_seed = 1337\n",
    "    torch.manual_seed(base_seed)\n",
    "    torch.cuda.manual_seed(base_seed)\n",
    "    # 对每张卡的seed进行偏移\n",
    "    if ddp:\n",
    "        init_distributed_mode()\n",
    "        args.device = torch.device(DEVICE)\n",
    "        rank = dist.get_rank()\n",
    "        torch.manual_seed(base_seed + rank)\n",
    "        # 同时设置 CUDA 的随机种子\n",
    "        torch.cuda.manual_seed(base_seed + rank)\n",
    "    # 加载ds\n",
    "    model, tokenizer = init_model(config)\n",
    "    print(model)\n",
    "    print(tokenizer)\n",
    "    train_ds = PretrainDataset(\n",
    "        data_path=train_args.data_path,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seqlen=train_args.max_seqlen,\n",
    "    )\n",
    "    train_sampler = DistributedSampler(train_ds) if ddp else None   #ddp模式下使用分布式采样器，确保采样均匀   \n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=train_args.batch_size,\n",
    "        shuffle=False,  # 分布式采样器会处理打乱\n",
    "        num_workers=train_args.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        sampler=train_sampler,  # 如果是ddp模式，使用分布式采样器\n",
    "    )\n",
    "    logger(f'使用分布式采样器: {train_sampler is not None}')\n",
    "\n",
    "    # 设置精度和优化器\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(train_args.dtype in ['float16', 'bfloat16']))\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=train_args.learning_rate)\n",
    "\n",
    "    # 临时变量忽略，封装ddp模型\n",
    "    if ddp:\n",
    "        model._ddp_params_and_buffers_to_ignore = {\"pos_cis\"}\n",
    "        model = DistributedDataParallel(model, device_ids=[ddp_local_rank])\n",
    "        logger(f\"ddp模式，local_rank: {ddp_local_rank}, global_rank: {dist.get_rank()}, world_size: {dist.get_world_size()}\")\n",
    "    \n",
    "    # 开始训练\n",
    "    for epoch in range(train_args.epochs):\n",
    "    train_epoch(epoch)\n",
    "\n",
    "# 运行命令为 torchrun --nproc_per_node=4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd33678c",
   "metadata": {},
   "source": [
    "# 训练中的bugs\n",
    "1.train_ds的shuffle必须为False,否则sampler无法正确sample，会导致样本重复<br>\n",
    "2.save_interval 中，必须设定要主设备保存或者不使用ddp模式来保存，否则会导致重复保存了ckpt!!!<br>\n",
    "batch_size最多只能到64左右,由于我是公用的卡，所以不能重复用,用tmux后台训练即可"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999c38b8",
   "metadata": {},
   "source": [
    "```\n",
    "tmux ls\n",
    "tmux attach -t 0\n",
    "torchrun --nproc_per_node 8 pretrain_ddp.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c2ca46",
   "metadata": {},
   "source": [
    "##  使用wandb工具可视化loss\n",
    "由于忘了服务器wandb帐号了，所以这个就忽略了,总之wandb工具可以用来可视化训练过程\n",
    "这里我就不写了"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
