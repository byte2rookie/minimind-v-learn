{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0e6e6cc",
   "metadata": {},
   "source": [
    "# 首先要搞清楚该项目的几个阶段\n",
    "- 1.pretrain阶段\n",
    "- 2.finetune阶段\n",
    "- 3.post-train阶段\n",
    "每个阶段都有对应的数据集<br>\n",
    "我们下载到../data文件夹中进行保存<br>\n",
    "[关于LLM X DATA的综述](https://arxiv.org/pdf/2505.18458)<br>\n",
    "[LLM数据合成与增强的综述](https://zhuanlan.zhihu.com/p/10073288207)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95be07e6",
   "metadata": {},
   "source": [
    "## pretrain数据集说明\n",
    "pretrain阶段主要用来让模型学会认字<br>\n",
    "一般包含有大量的潜在语义信息<br>\n",
    "我们打印其中的内容查看：\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dbee630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"text\": \"<|im_start|>鉴别一组中文文章的风格和特点，例如官方、口语、文言等。需要提供样例文章才能准确鉴别不同的风格和特点。<|im_end|> <|im_start|>好的，现在帮我查一下今天的天气怎么样?今天的天气依据地区而异。请问你需要我帮你查询哪个地区的天气呢？<|im_end|> <|im_start|>打开闹钟功能，定一个明天早上七点的闹钟。好的，我已经帮您打开闹钟功能，闹钟将在明天早上七点准时响起。<|im_end|> <|im_start|>为以下场景写一句话描述：一个孤独的老人坐在公园长椅上看着远处。一位孤独的老人坐在公园长椅上凝视远方。<|im_end|> <|im_start|>非常感谢你的回答。请告诉我，这些数据是关于什么主题的？这些数据是关于不同年龄段的男女人口比例分布的。<|im_end|> <|im_start|>帮我想一个有趣的标题。这个挺有趣的：\\\"如何成为一名成功的魔术师\\\" 调皮的标题往往会吸引读者的注意力。<|im_end|> <|im_start|>回答一个问题，地球的半径是多少？地球的平均半径约为6371公里，这是地球自赤道到两极的距离的平均值。<|im_end|> <|im_start|>识别文本中的语气，并将其分类为喜悦、悲伤、惊异等。\\n文本：“今天是我的生日！”这个文本的语气是喜悦。<|im_end|>\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/pretrain_hq.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    first_line = f.readline()  # 读取第一行\n",
    "    print(first_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b238bc44",
   "metadata": {},
   "source": [
    "可以看到，其json内容为：\n",
    "```{\n",
    "    \"text\":\n",
    "            \"\n",
    "    <|im_start|>鉴别一组中文文章的风格和特点，例如官方、口语、文言等。需要提供样例文章才能准确鉴别不同的风格和特点。\n",
    "    <|im_end|> \n",
    "    <|im_start|>好的，现在帮我查一下今天的天气怎么样?今天的天气依据地区而异。请问你需要我帮你查询哪个地区的天气呢？\n",
    "    <|im_end|> \n",
    "    <|im_start|>打开闹钟功能，定一个明天早上七点的闹钟。好的，我已经帮您打开闹钟功能，闹钟将在明天早上七点准时响起。\n",
    "    <|im_end|> \n",
    "    <|im_start|>为以下场景写一句话描述：一个孤独的老人坐在公园长椅上看着远处。一位孤独的老人坐在公园长椅上凝视远方。\n",
    "    <|im_end|> \n",
    "    <|im_start|>非常感谢你的回答。请告诉我，这些数据是关于什么主题的？这些数据是关于不同年龄段的男女人口比例分布的。\n",
    "    <|im_end|> \n",
    "    <|im_start|>帮我想一个有趣的标题。这个挺有趣的：\\\"如何成为一名成功的魔术师\\\" 调皮的标题往往会吸引读者的注意力。\n",
    "    <|im_end|> \n",
    "    <|im_start|>回答一个问题，地球的半径是多少？地球的平均半径约为6371公里，这是地球自赤道到两极的距离的平均值。\n",
    "    <|im_end|> \n",
    "    <|im_start|>识别文本中的语气，并将其分类为喜悦、悲伤、惊异等。\\n文本：“今天是我的生日！”这个文本的语气是喜悦。\n",
    "    <|im_end|>\n",
    "            \"\n",
    "}```\n",
    "是这样子的一长串的对话文本，用<|im_start|>和<|im_end|>标记，和qwen等一致"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9313ad",
   "metadata": {},
   "source": [
    "## sft数据\n",
    "sft主要是用于对齐model的语气，口吻等的数据集\n",
    "打印数据结构：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be61ac52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"conversations\": [{\"role\": \"user\", \"content\": \"What is the most recent version of the Android operating system?\"}, {\"role\": \"assistant\", \"content\": \"As of my last update in October 2023, the most recent version of the Android operating system is Android 14. It was officially released in August 2023. Android 14 brings several new features and improvements, including enhanced privacy controls, performance optimizations, and user interface refinements. For the most current information, you can check the official Android website or the latest announcements from Google.\"}]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/sft_1024.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    first_line = f.readline()  # 读取第一行\n",
    "    print(first_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f703ee88",
   "metadata": {},
   "source": [
    "可以看到其数据结构为：\n",
    "[```{\"role\": \"user\", \"content\": \"What is the most recent version of the Android operating system?\"}```,\n",
    "\n",
    " ```{\"role\": \"assistant\", \"content\": \"As of my last update in October 2023, the most recent version of the Android operating system is Android 14. It was officially released in August 2023. Android 14 brings several new features and improvements, including enhanced privacy controls, performance optimizations, and user interface refinements. For the most current information, you can check the official Android website or the latest announcements from Google.\"}```]\n",
    "一段对话。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ab73a8",
   "metadata": {},
   "source": [
    "## post-training数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "143d64a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"chosen\": [{\"content\": \"How many moles of HBr are required to react with 2 moles of C2H6 to form 2 moles of C2H5Br along with 2 moles of H2?\", \"role\": \"user\"}, {\"content\": \"To determine the number of moles of HBr required to react with 2 moles of C2H6 (ethane) to form 2 moles of C2H5Br (ethyl bromide) and 2 moles of H2, we need to write the balanced chemical equation for the reaction.\\n\\nThe reaction is a halogenation reaction, specifically the reaction of ethane with hydrogen bromide (HBr) to form ethyl bromide and hydrogen gas. The balanced chemical equation for this reaction is:\\n\\n\\\\[ \\\\text{C}_2\\\\text{H}_6 + \\\\text{HBr} \\\\rightarrow \\\\text{C}_2\\\\text{H}_5\\\\text{Br} + \\\\text{H}_2 \\\\]\\n\\nHowever, this equation is not balanced as written. To balance it, we need to ensure that the number of atoms of each element on the reactant side is equal to the number of atoms of that element on the product side. The balanced equation is:\\n\\n\\\\[ \\\\text{C}_2\\\\text{H}_6 + \\\\text{HBr} \\\\rightarrow \\\\text{C}_2\\\\text{H}_5\\\\text{Br} + \\\\text{H}_2 \\\\]\\n\\nUpon closer inspection, we see that the equation is already balanced as written, with 1 mole of C2H6 reacting with 1 mole of HBr to produce 1 mole of C2H5Br and 1 mole of H2.\\n\\nGiven that you have 2 moles of C2H6, and the stoichiometry of the reaction is 1:1 for C2H6 to HBr, you would need 2 moles of HBr to react with 2 moles of C2H6 to form 2 moles of C2H5Br and 2 moles of H2.\\n\\nTherefore, the number of moles of HBr required is **2 moles**.\", \"role\": \"assistant\"}], \"rejected\": [{\"content\": \"How many moles of HBr are required to react with 2 moles of C2H6 to form 2 moles of C2H5Br along with 2 moles of H2?\", \"role\": \"user\"}, {\"content\": \"To answer this question, we need to write down the chemical equation representing the reaction between hydrogen bromide (HBr) and ethane (C2H6):\\n\\nC2H6(g) + 2HBr(g) \\u2192 C2H5Br(l) + H2(g)\\n\\nNow, we can set up a mole ratio based on the coefficients in front of each substance in the balanced equation:\\n\\nMoles of HBr : Moles of C2H6 :: 2 : 1\\n\\nWe also know that there are two moles of C2H6 present in our starting mixture, so we can substitute this value into the above mole ratio to determine the amount of HBr needed:\\n\\n(2 mol C2H6)/(1 mol C2H6) x 2 mol HBr = 4 mol HBr\\n\\nSo, four moles of HBr would be required to completely react with two moles of C2H6 according to the given stoichiometric relationship.\", \"role\": \"assistant\"}]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/dpo.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    first_line = f.readline()  # 读取第一行\n",
    "    print(first_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230f7346",
   "metadata": {},
   "source": [
    "内容为：```{\n",
    "    \"chosen\": \n",
    "\n",
    "    [\n",
    "\n",
    "    {\"content\": \"How many moles of HBr are required to react with 2 moles of C2H6 to form 2 moles of C2H5Br along with 2 moles of H2?\",\"role\": \"user\"}, \n",
    "\n",
    "    {\"content\": \"To determine the number of moles of HBr required to react with 2 moles of C2H6 (ethane) to form 2 moles of C2H5Br (ethyl bromide) and 2 moles of H2, we need to write the balanced chemical equation for the reaction.\\n\\nThe reaction is a halogenation reaction, specifically the reaction of ethane with hydrogen bromide (HBr) to form ethyl bromide and hydrogen gas. The balanced chemical equation for this reaction is:\\n\\n\\\\[ \\\\text{C}_2\\\\text{H}_6 + \\\\text{HBr} \\\\rightarrow \\\\text{C}_2\\\\text{H}_5\\\\text{Br} + \\\\text{H}_2 \\\\]\\n\\nHowever, this equation is not balanced as written. To balance it, we need to ensure that the number of atoms of each element on the reactant side is equal to the number of atoms of that element on the product side. The balanced equation is:\\n\\n\\\\[ \\\\text{C}_2\\\\text{H}_6 + \\\\text{HBr} \\\\rightarrow \\\\text{C}_2\\\\text{H}_5\\\\text{Br} + \\\\text{H}_2 \\\\]\\n\\nUpon closer inspection, we see that the equation is already balanced as written, with 1 mole of C2H6 reacting with 1 mole of HBr to produce 1 mole of C2H5Br and 1 mole of H2.\\n\\nGiven that you have 2 moles of C2H6, and the stoichiometry of the reaction is 1:1 for C2H6 to HBr, you would need 2 moles of HBr to react with 2 moles of C2H6 to form 2 moles of C2H5Br and 2 moles of H2.\\n\\nTherefore, the number of moles of HBr required is **2 moles**.\", \"role\": \"assistant\"}\n",
    "    ], \n",
    "\n",
    "    \"rejected\": [\n",
    "\n",
    "        {\n",
    "\n",
    "        \"content\": \"How many moles of HBr are required to react with 2 moles of C2H6 to form 2 moles of C2H5Br along with 2 moles of H2?\", \"role\": \"user\"}, \n",
    "\n",
    "        {\"content\": \"To answer this question, we need to write down the chemical equation representing the reaction between hydrogen bromide (HBr) and ethane (C2H6):\\n\\nC2H6(g) + 2HBr(g) \\u2192 C2H5Br(l) + H2(g)\\n\\nNow, we can set up a mole ratio based on the coefficients in front of each substance in the balanced equation:\\n\\nMoles of HBr : Moles of C2H6 :: 2 : 1\\n\\nWe also know that there are two moles of C2H6 present in our starting mixture, so we can substitute this value into the above mole ratio to determine the amount of HBr needed:\\n\\n(2 mol C2H6)/(1 mol C2H6) x 2 mol HBr = 4 mol HBr\\n\\nSo, four moles of HBr would be required to completely react with two moles of C2H6 according to the given stoichiometric relationship.\", \"role\": \"assistant\"}\n",
    "        \n",
    "        ]\n",
    "}```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811e15fb",
   "metadata": {},
   "source": [
    "## 制作Dataset加载器\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3969a5a3",
   "metadata": {},
   "source": [
    "### Pretrain阶段Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c578a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "class PretrainDataset(Dataset):\n",
    "    def __init__(self,data_path,tokenizer,max_seqlen=1024):\n",
    "        self.data_path = data_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seqlen = max_seqlen\n",
    "        self.samples = self.load_data(data_path)\n",
    "    \n",
    "    def load_data(self, path):\n",
    "        samples = []\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                data = json.loads(line.strip())\n",
    "                samples.append(data)\n",
    "        return samples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.samples[index]\n",
    "        text = str(sample['text'])\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_seqlen,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding.input_ids.squeeze()   \n",
    "        loss_mask = (input_ids != self.tokenizer.pad_token_id) # 确认input_ids中非填充部分，loss_mask形状类似[True, True, ..., False]\n",
    "        X=torch.tensor(input_ids[:-1],dtype=torch.long)  # 去掉最后一个token\n",
    "        Y=torch.tensor(input_ids[1:],dtype=torch.long)  # 去掉第一个\n",
    "        loss_mask = torch.tensor(loss_mask[1:], dtype=torch.long)\n",
    "        return X, Y, loss_mask\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc601788",
   "metadata": {},
   "source": [
    "### SFT阶段加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75b54e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "class SFTDataset(Dataset):\n",
    "    def __init__(self,data_path,tokenizer,max_seqlen=1024):\n",
    "        self.data_path = data_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seqlen = max_seqlen\n",
    "        self.samples = self.load_data(data_path)\n",
    "        self.bos_id = tokenizer('<|im_start|>assistant', add_special_tokens=False).input_ids\n",
    "        self.eos_id = tokenizer('<|im_end|>', add_special_tokens=False).input_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)    \n",
    "\n",
    "    def load_data(self, path):\n",
    "        samples = []\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                data = json.loads(line.strip())\n",
    "                samples.append(data)\n",
    "        return samples\n",
    "    \n",
    "    def _create_chat_prompt(self, conversations):\n",
    "            \"\"\"构建符合ChatML格式的对话\"\"\"\n",
    "            messages = []\n",
    "            for i, turn in enumerate(conversations):\n",
    "                role = 'user' if i % 2 == 0 else 'assistant'\n",
    "                messages.append({\"role\": role, \"content\": turn['content']})\n",
    "            return self.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=False\n",
    "            )\n",
    "    \n",
    "\n",
    "    #只在assistant的回答部分计算loss,于是把这几个部分用bos_id和eos_id标记出来\n",
    "    def _generate_loss_mask(self, input_ids):\n",
    "            loss_mask = [0] * len(input_ids)\n",
    "            i = 0\n",
    "            while i < len(input_ids):\n",
    "                if input_ids[i:i + len(self.bos_id)] == self.bos_id:\n",
    "                    start = i + len(self.bos_id)\n",
    "                    end = start\n",
    "                    while end < len(input_ids):\n",
    "                        if input_ids[end:end + len(self.eos_id)] == self.eos_id:\n",
    "                            break\n",
    "                        end += 1\n",
    "                    for j in range(start + 1, min(end + len(self.eos_id) + 1, self.max_length)):\n",
    "                        loss_mask[j] = 1\n",
    "                    i = end + len(self.eos_id) if end < len(input_ids) else len(input_ids)\n",
    "                else:\n",
    "                    i += 1\n",
    "            return loss_mask\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.samples[index]\n",
    "        conversations = sample['conversations']\n",
    "        input_ids = self._create_chat_prompt(conversations)\n",
    "        input_ids = self.tokenizer(input_ids, max_length=self.max_seqlen, padding='max_length', truncation=True, return_tensors='pt').input_ids\n",
    "        \n",
    "        loss_mask = self._generate_loss_mask(input_ids)\n",
    "        \n",
    "        X = torch.tensor(input_ids[:-1], dtype=torch.long)\n",
    "        Y = torch.tensor(input_ids[1:], dtype=torch.long)\n",
    "        loss_mask = torch.tensor(loss_mask[1:], dtype=torch.long)\n",
    "        return X, Y, loss_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f186c3e3",
   "metadata": {},
   "source": [
    "### DPO阶段加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441e8a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPODataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_seqlen=1024):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_seqlen\n",
    "        self.padding = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "        self.bos_id = tokenizer('<|im_start|>assistant', add_special_tokens=False).input_ids\n",
    "        self.eos_id = tokenizer('<|im_end|>', add_special_tokens=False).input_ids\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            self.data = []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                obj = json.loads(line)\n",
    "                self.data.append(obj)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        chosen = item['chosen']  # 是一个 list，里面包含若干 {role, content}\n",
    "        rejected = item['rejected']  # 同上\n",
    "        chosen_prompt = self.tokenizer.apply_chat_template(\n",
    "            chosen, tokenize=False, add_generation_prompt=False\n",
    "        )\n",
    "\n",
    "        rejected_prompt = self.tokenizer.apply_chat_template(\n",
    "            rejected, tokenize=False, add_generation_prompt=False\n",
    "        )\n",
    "        chosen_encoding = self.tokenizer(\n",
    "            chosen_prompt, truncation=True, max_length=self.max_length, padding='max_length'\n",
    "        )\n",
    "        rejected_encoding = self.tokenizer(\n",
    "            rejected_prompt, truncation=True, max_length=self.max_length, padding='max_length'\n",
    "        )\n",
    "\n",
    "        chosen_input_ids = chosen_encoding['input_ids']\n",
    "        chosen_loss_mask = self._generate_loss_mask(chosen_input_ids)\n",
    "\n",
    "        rejected_input_ids = rejected_encoding['input_ids']\n",
    "        rejected_loss_mask = self._generate_loss_mask(rejected_input_ids)\n",
    "        x_chosen = torch.tensor(chosen_input_ids[:-1], dtype=torch.long)\n",
    "        y_chosen = torch.tensor(chosen_input_ids[1:], dtype=torch.long)\n",
    "        mask_chosen = torch.tensor(chosen_loss_mask[1:], dtype=torch.long)\n",
    "        x_rejected = torch.tensor(rejected_input_ids[:-1], dtype=torch.long)\n",
    "        y_rejected = torch.tensor(rejected_input_ids[1:], dtype=torch.long)\n",
    "        mask_rejected = torch.tensor(rejected_loss_mask[1:], dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'x_chosen': x_chosen,\n",
    "            'y_chosen': y_chosen,\n",
    "            'mask_chosen': mask_chosen,\n",
    "            'x_rejected': x_rejected,\n",
    "            'y_rejected': y_rejected,\n",
    "            'mask_rejected': mask_rejected\n",
    "        }\n",
    "\n",
    "    def _generate_loss_mask(self, input_ids):\n",
    "        loss_mask = [0] * len(input_ids)\n",
    "        i = 0\n",
    "        while i < len(input_ids):\n",
    "            if input_ids[i:i + len(self.bos_id)] == self.bos_id:\n",
    "                start = i + len(self.bos_id)\n",
    "                end = start\n",
    "                while end < len(input_ids):\n",
    "                    if input_ids[end:end + len(self.eos_id)] == self.eos_id:\n",
    "                        break\n",
    "                    end += 1\n",
    "                for j in range(start + 1, min(end + len(self.eos_id) + 1, self.max_length)):\n",
    "                    loss_mask[j] = 1\n",
    "                i = end + len(self.eos_id) if end < len(input_ids) else len(input_ids)\n",
    "            else:\n",
    "                i += 1\n",
    "        return loss_mask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
