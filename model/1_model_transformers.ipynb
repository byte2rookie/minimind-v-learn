{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8078203",
   "metadata": {},
   "source": [
    "# 利用transformers库封装model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46c4db1",
   "metadata": {},
   "source": [
    "通过上一个文件，我们已经知道了minimind-dense的torch模型了<br>\n",
    "现在我们用transformers库来封装model，方便后续的上传和推理过程<br>\n",
    "要素：\n",
    "- tokenizer\n",
    "- embedding\n",
    "- minimind-block\n",
    "    - RoPE\n",
    "    - RMSNorm\n",
    "    - GQA\n",
    "    - FFN\n",
    "- lm_head\n",
    "- de-tokenizer\n",
    "\n",
    "[关于transformers库](https://cloud.tencent.com/developer/article/2367010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9dc6eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/zyp/miniconda3/envs/minimind/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import PretrainedConfig\n",
    "\n",
    "class MinimindConfig(PretrainedConfig):\n",
    "    model_type = \"minimind\"\n",
    "    def __init__(self,\n",
    "                # tokenizer相关 \n",
    "                 vocab_size = 6400,\n",
    "                 model_max_length=32768,\n",
    "                 eos_token_id = 1,\n",
    "                 bos_token_id = 2,\n",
    "                # attn 参数\n",
    "                max_seqlen=512,\n",
    "                embed_dim=512,\n",
    "                head_num=8,\n",
    "                kv_head_num=4,\n",
    "                attn_dropout=0.1,\n",
    "                attn_res_dropout=0.1,\n",
    "                Flash=False,\n",
    "                # ffn 参数,\n",
    "                ffn_dim=2048,\n",
    "                act_fn = \"silu\",\n",
    "                ffn_dropout=0.1,\n",
    "                # norm 参数\n",
    "                rmsnorm_eps=1e-6,\n",
    "                # block参数,\n",
    "                block_num=8,\n",
    "                ## MOE 参数\n",
    "                # 其他参数\n",
    "                **kwargs\n",
    "                ):\n",
    "        super(MinimindConfig,self).__init__(**kwargs)\n",
    "        # tokenizer相关\n",
    "        self.vocab_size = vocab_size\n",
    "        self.model_max_length = model_max_length\n",
    "        self.eos_token_id = eos_token_id\n",
    "        self.bos_token_id = bos_token_id\n",
    "\n",
    "        # 结构参数\n",
    "        self.block_num = block_num\n",
    "        self.embed_dim = embed_dim\n",
    "        self.head_num = head_num\n",
    "        self.head_dim = embed_dim // head_num\n",
    "        self.kv_head_num = kv_head_num\n",
    "        self.ffn_dim = ffn_dim\n",
    "        self.attn_res_dropout = attn_res_dropout\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.ffn_dropout = ffn_dropout\n",
    "        self.Flash = Flash\n",
    "        self.max_seqlen = max_seqlen\n",
    "        self.norm_eps = rmsnorm_eps\n",
    "        self.act_fn = act_fn\n",
    "        ## MOE 参数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396e2f40",
   "metadata": {},
   "source": [
    "## 1.tokenizer\n",
    "使用我们之前预训练数据集训练好的tokenizer\n",
    "\n",
    "[关于tokenizer参数的相关设置](https://zhuanlan.zhihu.com/p/341994096) <br>\n",
    "[关于left_padding和right_padding的讨论](https://zhuanlan.zhihu.com/p/646852375)<br>\n",
    "[如何改进增强长文本处理能力](https://zhuanlan.zhihu.com/p/638976034)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98580fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='./', vocab_size=6400, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|im_start|>', 'eos_token': '<|im_end|>', 'unk_token': '<|endoftext|>', 'pad_token': '<pad>', 'additional_special_tokens': ['<pad>', '<mask>', '<s>', '</s>', '<unk>', '<UNK>', '<EOS>', '<zzy>', '<|s1|>', '<|s2|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<endoftext>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t5: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t6: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t7: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t8: AddedToken(\"<UNK>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t9: AddedToken(\"<EOS>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t10: AddedToken(\"<zzy>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t11: AddedToken(\"<|s1|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t12: AddedToken(\"<|s2|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t6400: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n",
      "6400\n",
      "{'bos_token': '<|im_start|>', 'eos_token': '<|im_end|>', 'unk_token': '<|endoftext|>', 'pad_token': '<pad>', 'additional_special_tokens': ['<pad>', '<mask>', '<s>', '</s>', '<unk>', '<UNK>', '<EOS>', '<zzy>', '<|s1|>', '<|s2|>']}\n",
      "['<|im_start|>', '<|im_end|>', '<|endoftext|>', '<pad>', '<mask>', '<s>', '</s>', '<unk>', '<UNK>', '<EOS>', '<zzy>', '<|s1|>', '<|s2|>']\n",
      "<|im_start|>鉴别一组中文文章的风格和特点，例如官方、口语、文言等。需要提供样例文章才能准确鉴别不同的风格和特点。<|im_end|> <|im_start|>好的，现在帮我查一下今天的天气怎么样?今天的天气依据地区而异。请问你需要我帮你查询哪个地区的天气呢？<|im_end|> <|im_start|>打开闹钟功能，定一个明天早上七点的闹钟。好的，我已经帮您打开闹钟功能，闹钟将在明天早上七点准时响起。<|im_end|> <|im_start|>为以下场景写一句话描述：一个孤独的老人坐在公园长椅上看着远处。一位孤独的老人坐在公园长椅上凝视远方。<|im_end|> <|im_start|>非常感谢你的回答。请告诉我，这些数据是关于什么主题的？这些数据是关于不同年龄段的男女人口比例分布的。<|im_end|> <|im_start|>帮我想一个有趣的标题。这个挺有趣的：\"如何成为一名成功的魔术师\" 调皮的标题往往会吸引读者的注意力。<|im_end|> <|im_start|>回答一个问题，地球的半径是多少？地球的平均半径约为6371公里，这是地球自赤道到两极的距离的平均值。<|im_end|> <|im_start|>识别文本中的语气，并将其分类为喜悦、悲伤、惊异等.文本：“今天是我的生日！”这个文本的语气是喜悦。<|im_end|>\n",
      "<|im_start|>根据输入的内容，编写一个类别标签。这是一篇介绍如何阅读心电图的文章类别标签: 医学/心电图阅读指南<|im_end|> <|im_start|>帮我搜索一下最近的天气情况。当然，我可以帮您搜索最新的天气情况。请问您需要查询哪个城市的天气情况呢？<|im_end|> <|im_start|>帮我讲一个令人开心的笑话。好的，我帮您讲一个关于细菌的笑话。为什么细菌不会上网？因为连接总是断开了！<|im_end|> <|im_start|>现在给我生成一首关于大海的五言诗。碧波万顷月满天，海天相接处天地间。波涛滚滚江山美，海鸟翱翔日月闲。<|im_end|> <|im_start|>谢谢你，这篇文章很有用。不客气，我很高兴能够为您提供帮助。如果您还有其他问题或需求，随时可以对我说。<|im_end|> <|im_start|>你好，我想下载一个视频编辑软件，你有什么推荐吗？您好！当然，有很多选择。您想要免费软件还是愿意付费？<|im_end|> <|im_start|>为什么我的程序不输出正确结果？可能是代码逻辑有误，或者输入数据有误，需要仔细调试代码逻辑和输入数据。<|im_end|> <|im_start|>谢谢你的回答。现在我想知道这场比赛的具体时间和地点。这场比赛的时间是北京时间10月4日，地点是上海。<|im_end|>\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 274])\n",
      "{'input_ids': tensor([[   2, 5203,  723,  ...,    3,    3,    3],\n",
      "        [   2, 4496, 2685,  ...,    3,    3,    3]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "{'input_ids': tensor([[   2, 5203,  723, 2828, 3014,  517, 4187,  297, 2002,  273,  933, 2266,\n",
      "          420,  308, 4691,  308, 5699,  419,  275,  522,  598,  828,  747,  517,\n",
      "         1712, 1544, 5203,  723, 1426, 1798,  297, 2002,  275,    1,  233,    2,\n",
      "          719,  273, 1150, 1895, 1125, 1028, 6139, 4693,   43, 6139, 2034,  472,\n",
      "         2001,  536, 1826,  275, 1785, 2896,  309,  675,  405, 4094, 2668, 2001,\n",
      "         2259, 1357,  433,    1,  233,    2, 3886, 6260, 1795, 1649,  273,  451,\n",
      "          386, 2415, 2707, 3205,  555,  272, 6260, 1795,  275,  719,  273, 4965,\n",
      "          675,  532, 3886, 6260, 1795, 1649,  273, 6260, 1795, 3832, 2415, 2707,\n",
      "         3205,  555,  955,  402,  755,  669,  275,    1,  233,    2,  348,  587,\n",
      "         1876,  643, 2646,  853,  320,  386, 2969,  272, 6255, 5593, 1776,  685,\n",
      "         4862,  412, 6277, 1340,  726,  275, 1730, 2969,  272, 6255, 5593, 1776,\n",
      "          685, 4862,  412, 4935,  919, 1340,  420,  275,    1,  233,    2, 3831,\n",
      "         1181, 1021,  275, 2902,  273,  748,  841, 4127,  673,  968,  272,  433,\n",
      "          748,  841, 4127,  832, 3297,  644,  272, 1257, 1086, 3920,  641,  747,\n",
      "         3924,  272,  275,    1,  233,    2, 4877,  386, 1644, 3544,  275,  569,\n",
      "         6129, 3283, 4582,  878, 1110, 2098, 4278, 2329,  556, 1199,   14,  233,\n",
      "          913, 1838, 3544, 4885,  418, 1931, 2326, 5959,  576,  474,  275,    1,\n",
      "          233,    2, 1021,  386,  533,  273, 4461, 2220, 2760, 4540,  433, 4461,\n",
      "         2391, 2220, 2760, 5213,   34,   31,   35,   29, 3952,  273, 1650, 1292,\n",
      "          422,  429,  110,  663,  392,  742,  819,  272, 2643,  272, 2391, 1105,\n",
      "          275,    1,  233,    2, 1320, 3176, 3870,  273, 4625, 2517,  704, 2842,\n",
      "          308, 6169,  308, 2068, 1826,  419,   26,  528, 1364, 1279,  302, 1317,\n",
      "         3344, 5476, 6373,  272, 3870,  302,  704, 2842,  275,    1],\n",
      "        [   2, 4496, 2685,  273, 4242, 2296, 4281,  275, 4614, 1241,  878, 1578,\n",
      "          573,  523, 1117, 1622, 2296, 4281,   38,  233, 2106,   27,  573,  523,\n",
      "         1117, 1578, 5359,    1,  233,    2, 1895, 2580, 1028, 1265, 2259, 1469,\n",
      "          275, 1436,  273, 1957,  675,  532, 2580, 4654, 1162, 1469,  275, 1785,\n",
      "         3408, 4094, 2668, 1031, 2259, 1469, 1357,  433,    1,  233,    2, 1895,\n",
      "         1276,  386, 2138, 4369, 6053,  678,  275,  719,  273,  309,  675,  532,\n",
      "         1276,  386,  927, 1026, 4044, 6053,  678,  275, 1847, 1026, 4044, 1985,\n",
      "          412,  929,  433,  857, 3031, 2611,  814,  634,  322, 1005,    1,  233,\n",
      "            2, 1150, 1140, 6268,  927,  430,  807,  272, 1411,  764, 1141,  275,\n",
      "         1196,  113, 1811, 1553,  617,  128, 1013, 1043,  470,  273,  807,  470,\n",
      "          592,  982,  726,  470,  410,  545,  275, 1811,  733,  262, 5542, 5542,\n",
      "         2476, 1033,  601,  273,  807, 1661, 1065,  122, 5759,  696, 1013, 2612,\n",
      "          275,    1,  233,    2, 5356,  273, 1036, 3875,  358,  275,  357,  960,\n",
      "          564,  273, 2772, 2929,  907, 3799,  793,  275, 1824, 4922,  533,  491,\n",
      "         1524,  273, 3426,  369,  403,  309,  652,  275,    1,  233,    2, 2899,\n",
      "          273, 1402,  384, 3200,  386, 3311, 1740, 2284,  273, 4688, 1261,  926,\n",
      "          433, 4706, 1005, 1436,  273, 2092,  899,  275,  532, 2014, 6352, 2284,\n",
      "         1605, 4423, 2169, 1288,  433,    1,  233,    2, 1847,  309, 6038,  357,\n",
      "         2186, 1654, 1521,  433, 2746, 3155, 3019,  319, 1247,  273, 1126, 1486,\n",
      "          841,  319, 1247,  273,  522, 6036,  913,  871, 3155, 3019,  297, 1486,\n",
      "          841,  275,    1,  233,    2, 5443, 1021,  275, 1150, 4194, 4943, 1756,\n",
      "         4163, 3147, 2470,  275, 4943, 1756, 2235,  302, 1875,  790, 1134, 1013,\n",
      "           32,  696,  273, 2470,  302, 2763,  275,    1,    3,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer_path= \"./\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "print(tokenizer)\n",
    "print(tokenizer.vocab_size)\n",
    "print(tokenizer.special_tokens_map)\n",
    "print(tokenizer.all_special_tokens)\n",
    "\n",
    "# 写一个虚拟的小的数据集，只有两条数据的集\n",
    "data=[\n",
    "    {'text':'<|im_start|>鉴别一组中文文章的风格和特点，例如官方、口语、文言等。需要提供样例文章才能准确鉴别不同的风格和特点。<|im_end|> <|im_start|>好的，现在帮我查一下今天的天气怎么样?今天的天气依据地区而异。请问你需要我帮你查询哪个地区的天气呢？<|im_end|> <|im_start|>打开闹钟功能，定一个明天早上七点的闹钟。好的，我已经帮您打开闹钟功能，闹钟将在明天早上七点准时响起。<|im_end|> <|im_start|>为以下场景写一句话描述：一个孤独的老人坐在公园长椅上看着远处。一位孤独的老人坐在公园长椅上凝视远方。<|im_end|> <|im_start|>非常感谢你的回答。请告诉我，这些数据是关于什么主题的？这些数据是关于不同年龄段的男女人口比例分布的。<|im_end|> <|im_start|>帮我想一个有趣的标题。这个挺有趣的：\"如何成为一名成功的魔术师\" 调皮的标题往往会吸引读者的注意力。<|im_end|> <|im_start|>回答一个问题，地球的半径是多少？地球的平均半径约为6371公里，这是地球自赤道到两极的距离的平均值。<|im_end|> <|im_start|>识别文本中的语气，并将其分类为喜悦、悲伤、惊异等.文本：“今天是我的生日！”这个文本的语气是喜悦。<|im_end|>'\n",
    "},\n",
    "    {'text':'<|im_start|>根据输入的内容，编写一个类别标签。这是一篇介绍如何阅读心电图的文章类别标签: 医学/心电图阅读指南<|im_end|> <|im_start|>帮我搜索一下最近的天气情况。当然，我可以帮您搜索最新的天气情况。请问您需要查询哪个城市的天气情况呢？<|im_end|> <|im_start|>帮我讲一个令人开心的笑话。好的，我帮您讲一个关于细菌的笑话。为什么细菌不会上网？因为连接总是断开了！<|im_end|> <|im_start|>现在给我生成一首关于大海的五言诗。碧波万顷月满天，海天相接处天地间。波涛滚滚江山美，海鸟翱翔日月闲。<|im_end|> <|im_start|>谢谢你，这篇文章很有用。不客气，我很高兴能够为您提供帮助。如果您还有其他问题或需求，随时可以对我说。<|im_end|> <|im_start|>你好，我想下载一个视频编辑软件，你有什么推荐吗？您好！当然，有很多选择。您想要免费软件还是愿意付费？<|im_end|> <|im_start|>为什么我的程序不输出正确结果？可能是代码逻辑有误，或者输入数据有误，需要仔细调试代码逻辑和输入数据。<|im_end|> <|im_start|>谢谢你的回答。现在我想知道这场比赛的具体时间和地点。这场比赛的时间是北京时间10月4日，地点是上海。<|im_end|>'\n",
    "}\n",
    "]\n",
    "for i in range(2):\n",
    "    print(data[i]['text'])\n",
    "\n",
    "# 接下来将该data的内容利用tokenizer编码\n",
    "input_texts = [item['text'] for item in data]\n",
    "#填充1 固定填充\n",
    "input_ids1 = tokenizer(input_texts, padding='max_length', truncation=True, max_length=512,return_tensors='pt')\n",
    "#填充2 动态填充\n",
    "input_ids2 = tokenizer(input_texts, padding=True, truncation=True, max_length=512,return_tensors='pt')\n",
    "print(input_ids1['input_ids'].shape)\n",
    "print(input_ids2['input_ids'].shape)\n",
    "print(input_ids1)\n",
    "print(input_ids2)\n",
    "# 这样我们就获取到了tokenizer编码后的数据\n",
    "#现在我们深拷贝input_ids1\n",
    "import copy\n",
    "input_ids = copy.deepcopy(input_ids1['input_ids'])\n",
    "# 后续我们采用这个input_ids进行模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3dae6b",
   "metadata": {},
   "source": [
    "## 2.Embedding\n",
    "对应参数 vocab_size,embed_dim<br>\n",
    "这里是 vocab_size=6400 embed_dim=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2b494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试版\n",
    "import torch \n",
    "from torch import nn\n",
    "class Embed(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(Embed, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        return self.embedding(input_ids)\n",
    "\n",
    "# 测试\n",
    "embed_dim=512\n",
    "vocab_size=6400\n",
    "embed_model = Embed(vocab_size, embed_dim)\n",
    "output = embed_model(input_ids)\n",
    "print(output.shape)  # 输出形状应为 (batch_size, sequence_length, embed\n",
    "print(output)\n",
    "\n",
    "#待会儿直接用就行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39b15ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512, 512])\n",
      "tensor([[[-0.4922,  0.0596, -0.8469,  ...,  1.2985,  1.7686,  0.4626],\n",
      "         [-0.9253, -1.5448, -1.4958,  ..., -0.3727,  0.2492,  1.6202],\n",
      "         [-0.3709, -1.0415,  2.3231,  ...,  0.1239,  0.1778, -0.2546],\n",
      "         ...,\n",
      "         [-0.9112, -0.1993,  0.8362,  ..., -0.1183, -1.1993,  0.7295],\n",
      "         [-0.9112, -0.1993,  0.8362,  ..., -0.1183, -1.1993,  0.7295],\n",
      "         [-0.9112, -0.1993,  0.8362,  ..., -0.1183, -1.1993,  0.7295]],\n",
      "\n",
      "        [[-0.4922,  0.0596, -0.8469,  ...,  1.2985,  1.7686,  0.4626],\n",
      "         [-0.5598,  1.0516,  1.0343,  ..., -2.1943, -1.5983,  0.3416],\n",
      "         [ 2.0491, -0.5918, -0.9708,  ...,  0.3106, -0.7558, -0.4342],\n",
      "         ...,\n",
      "         [-0.9112, -0.1993,  0.8362,  ..., -0.1183, -1.1993,  0.7295],\n",
      "         [-0.9112, -0.1993,  0.8362,  ..., -0.1183, -1.1993,  0.7295],\n",
      "         [-0.9112, -0.1993,  0.8362,  ..., -0.1183, -1.1993,  0.7295]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "class Embed(nn.Module):\n",
    "    def __init__(self, config:MinimindConfig):\n",
    "        super(Embed, self).__init__()\n",
    "        vocab_size = config.vocab_size\n",
    "        embed_dim = config.embed_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        return self.embedding(input_ids)\n",
    "\n",
    "# 测试\n",
    "embed_dim=512\n",
    "vocab_size=6400\n",
    "config1=MinimindConfig(vocab_size=vocab_size, embed_dim=embed_dim)\n",
    "embed_model = Embed(config1)\n",
    "output = embed_model(input_ids)\n",
    "print(output.shape)  # 输出形状应为 (batch_size, sequence_length, embed\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91067a1f",
   "metadata": {},
   "source": [
    "## 3.RMSNorm\n",
    "计算公式为\n",
    "$$ a_i=\\frac{a_i}{RMS(a)+\\epsilon} * \\gamma \\quad where \\quad RMS(a) = \\sqrt{\\frac{1}{n}\\sum^n_{i=1}a^2_i} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6284e9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self,embed_dim,eps=1e-6):\n",
    "        super(RMSNorm,self).__init__()\n",
    "        self.embed_dim=embed_dim\n",
    "        self.eps=eps\n",
    "        self.gamma = nn.Parameter(torch.ones(embed_dim))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return x*self.gamma*torch.rsqrt(x.pow(2).mean(dim=-1,keepdim=True)+self.eps)\n",
    "\n",
    "# 测试RMSNorm\n",
    "rmsnorm_model = RMSNorm(embed_dim)\n",
    "output_rmsnorm = rmsnorm_model(output)\n",
    "print(output_rmsnorm)  # 输出形状应为 (batch_size, sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7b841d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5054,  0.0612, -0.8696,  ...,  1.3332,  1.8159,  0.4750],\n",
      "         [-0.9202, -1.5363, -1.4876,  ..., -0.3706,  0.2478,  1.6113],\n",
      "         [-0.3675, -1.0320,  2.3018,  ...,  0.1228,  0.1762, -0.2522],\n",
      "         ...,\n",
      "         [-0.9005, -0.1969,  0.8263,  ..., -0.1169, -1.1852,  0.7208],\n",
      "         [-0.9005, -0.1969,  0.8263,  ..., -0.1169, -1.1852,  0.7208],\n",
      "         [-0.9005, -0.1969,  0.8263,  ..., -0.1169, -1.1852,  0.7208]],\n",
      "\n",
      "        [[-0.5054,  0.0612, -0.8696,  ...,  1.3332,  1.8159,  0.4750],\n",
      "         [-0.5673,  1.0656,  1.0480,  ..., -2.2235, -1.6196,  0.3461],\n",
      "         [ 2.0147, -0.5819, -0.9545,  ...,  0.3054, -0.7431, -0.4269],\n",
      "         ...,\n",
      "         [-0.9005, -0.1969,  0.8263,  ..., -0.1169, -1.1852,  0.7208],\n",
      "         [-0.9005, -0.1969,  0.8263,  ..., -0.1169, -1.1852,  0.7208],\n",
      "         [-0.9005, -0.1969,  0.8263,  ..., -0.1169, -1.1852,  0.7208]]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self,config:MinimindConfig):\n",
    "        super(RMSNorm,self).__init__()\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.eps = config.norm_eps\n",
    "        self.gamma = nn.Parameter(torch.ones(embed_dim))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return x*self.gamma*torch.rsqrt(x.pow(2).mean(dim=-1,keepdim=True)+self.eps)\n",
    "\n",
    "# 测试RMSNorm\n",
    "rmsnorm_model = RMSNorm(config1)\n",
    "output_rmsnorm = rmsnorm_model(output)\n",
    "print(output_rmsnorm)  # 输出形状应为 (batch_size, sequence_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0a74de",
   "metadata": {},
   "source": [
    "## 4.RoPE\n",
    "主要两步骤\n",
    "- 获取$ m\\theta $ ,计算好precompute_pos_cis\n",
    "- 将pos_cis应用\n",
    "\n",
    "主要公式为：\n",
    "$$\n",
    "\\begin{align}\n",
    "f_q(\\boldsymbol{x}_m, m) &= (\\boldsymbol{W}_q \\boldsymbol{x}_m) e^{im\\theta} \\\\\n",
    "f_k(\\boldsymbol{x}_n, n) &= (\\boldsymbol{W}_k \\boldsymbol{x}_n) e^{in\\theta} \\\\\n",
    "g(\\boldsymbol{x}_m, \\boldsymbol{x}_n, m - n) &= \\text{Re}\\left[ (\\boldsymbol{W}_q \\boldsymbol{x}_m)^* (\\boldsymbol{W}_k \\boldsymbol{x}_n) e^{i(n - m)\\theta} \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$\\Theta=\\left\\{\\theta_i=10000^{-2(i-1)/d},i\\in[1,2,\\ldots,d/2]\\right\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f782278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000+0.0000e+00j,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n",
      "          ...,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n",
      "          1.0000+0.0000e+00j],\n",
      "        [ 0.5403+8.4147e-01j,  1.0000+1.0000e-10j,  1.0000+1.0000e-20j,\n",
      "          ...,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n",
      "          1.0000+0.0000e+00j],\n",
      "        [-0.4161+9.0930e-01j,  1.0000+2.0000e-10j,  1.0000+2.0000e-20j,\n",
      "          ...,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n",
      "          1.0000+0.0000e+00j],\n",
      "        ...,\n",
      "        [ 0.9981+6.1950e-02j,  1.0000+5.0900e-08j,  1.0000+5.0900e-18j,\n",
      "          ...,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n",
      "          1.0000+0.0000e+00j],\n",
      "        [ 0.4871+8.7333e-01j,  1.0000+5.1000e-08j,  1.0000+5.1000e-18j,\n",
      "          ...,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n",
      "          1.0000+0.0000e+00j],\n",
      "        [-0.4717+8.8177e-01j,  1.0000+5.1100e-08j,  1.0000+5.1100e-18j,\n",
      "          ...,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n",
      "          1.0000+0.0000e+00j]])\n",
      "torch.Size([2, 512, 512])\n",
      "torch.Size([2, 512, 256, 2])\n",
      "torch.Size([2, 512, 256, 2])\n"
     ]
    }
   ],
   "source": [
    "def precompute_pos_cis(embed_dim=512,max_seqlen=512,theta=1e5):\n",
    "    freqs= 1/theta**torch.arange(0,embed_dim,2)[:embed_dim//2].float()\n",
    "    m=torch.arange(max_seqlen,device=freqs.device)\n",
    "    freqs= torch.outer(m,freqs).float() #获取了mtheta\n",
    "    pos_cis = torch.polar(torch.ones_like(freqs),freqs) #将mtheta化为极坐标模式\n",
    "    return pos_cis\n",
    "\n",
    "def apply_rotary(xq,xk,pos_cis):\n",
    "    xq_=torch.view_as_complex(xq.float().reshape(*xq.shape[:-1],-1,2))\n",
    "    xk_=torch.view_as_complex(xk.float().reshape(*xk.shape[:-1],-1,2))\n",
    "    #输入的pos_cis一般比xq,xk都要大，需要把pos_cis的形状和xq对齐\n",
    "    #xq一般都是(bs,seqlen,head,head_dim)\n",
    "    def unite_shape(pos_cis,  x):\n",
    "        ndim = x.ndim\n",
    "        assert 0 <= 1 < ndim\n",
    "        assert pos_cis.shape == (x.shape[1],  x.shape[-1]), f\"pos_cis shape {pos_cis.shape} does not match x shape {x.shape}\"\n",
    "        shape = [d if i == 1 or i == ndim - 1 else 1 for i,  d in enumerate(x.shape)]\n",
    "        return pos_cis.view(*shape)\n",
    "    pos_cis = unite_shape(pos_cis, xq_)\n",
    "    xq_ = torch.view_as_real(xq_ * pos_cis).flatten(3)\n",
    "    xk_ = torch.view_as_real(xk_ * pos_cis).flatten(3)\n",
    "    return xq_, xk_\n",
    "\n",
    "#测试一下\n",
    "pos_cis = precompute_pos_cis(embed_dim=embed_dim, max_seqlen=512)\n",
    "print(pos_cis)\n",
    "print(output_rmsnorm.shape)  # 输出形状应为 (batch_size, sequence_length, embed_dim)\n",
    "xq, xk = apply_rotary(output_rmsnorm, output_rmsnorm, pos_cis)\n",
    "print(xq.shape)  # 输出形状应为 (batch_size,\n",
    "print(xk.shape)  # 输出形状应为 (batch_size, sequence_length, embed_dim)\n",
    "# 测试一下apply_rotary的效果\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ce0281",
   "metadata": {},
   "source": [
    "## 5.GQA\n",
    "![LLM-结构](../images/LLM-structure.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b39a070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat_kv是必须要用到的,对齐GQA里的KV与Q的形状\n",
    "def repeat_kv(x,rep_num):\n",
    "    if rep_num == 1:\n",
    "        return x\n",
    "    bs,seqlen,head,head_dim=x.shape\n",
    "    return x[:,:,:,None,:].expand(bs,seqlen,head,rep_num,head_dim).reshape(bs,seqlen,head*rep_num,head_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c4c1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "class GroupQueryAttention(nn.Module):\n",
    "    def __init__(self,embed_dim,head_num,kv_head_num,attn_dropout=0.1,attn_res_dropout=0.1,Flash=False,max_seqlen=512):\n",
    "        super(GroupQueryAttention,self).__init__()\n",
    "        #基本属性\n",
    "        self.embed_dim = embed_dim\n",
    "        self.head_num = head_num\n",
    "        self.kv_head_num = kv_head_num\n",
    "        self.head_dim = embed_dim // head_num\n",
    "        assert embed_dim % head_num == 0, \"embed_dim must be divisible by head_num\"\n",
    "        self.rep_num = head_num // kv_head_num\n",
    "        assert head_num % kv_head_num == 0, \"kv_head_num must be divisible by head_num\"\n",
    "        self.Flash = hasattr(torch.nn.functional,'scaled_dot_product_attention') and Flash\n",
    "        #网络层\n",
    "        self.q_proj = nn.Linear(embed_dim,self.head_num * self.head_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim,self.kv_head_num * self.head_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim,self.kv_head_num * self.head_dim)\n",
    "        self.o_proj = nn.Linear(self.head_num*self.head_dim,self.embed_dim)\n",
    "        self.attn_dropout = nn.Dropout(attn_dropout)\n",
    "        self.res_dropout = nn.Dropout(attn_res_dropout)\n",
    "        #临时\n",
    "        mask = torch.full((1,1, max_seqlen, max_seqlen), float('-1e9'))\n",
    "        mask = torch.tril(mask, diagonal=0)\n",
    "        self.register_buffer('mask', mask)\n",
    "    def forward(self,x,\n",
    "                pos_cis=None,\n",
    "                past_key_value=None,\n",
    "                use_cache=False):\n",
    "        bs,seqlen,embed_dim = x.shape\n",
    "        xq = self.q_proj(x)\n",
    "        xk = self.k_proj(x)\n",
    "        xv = self.v_proj(x)\n",
    "\n",
    "        xq = xq.view(bs, seqlen, self.head_num, self.head_dim)\n",
    "        xk = xk.view(bs, seqlen, self.kv_head_num, self.head_dim)\n",
    "        xv = xv.view(bs, seqlen, self.kv_head_num, self.head_dim)\n",
    "        if pos_cis is None:\n",
    "            pos_cis = precompute_pos_cis(embed_dim=self.head_dim, max_seqlen=seqlen)\n",
    "        xq, xk = apply_rotary(xq, xk, pos_cis)\n",
    "        if past_key_value is not None:\n",
    "            xk = torch.cat([past_key_value[0], xk], dim=1)\n",
    "            xv = torch.cat([past_key_value[1], xv], dim=1)\n",
    "        past_kv = (xk, xv) if use_cache else None\n",
    "        xq = xq.transpose(1,2)\n",
    "        xk = repeat_kv(xk, self.rep_num).transpose(1,2)\n",
    "        xv = repeat_kv(xv, self.rep_num).transpose(1,2)\n",
    "        if self.Flash:\n",
    "            attn_output = F.scaled_dot_product_attention(\n",
    "                xq, xk, xv, attn_mask=None, dropout_p=self.dropout,is_causal=True)\n",
    "        else:\n",
    "            scores = torch.matmul(xq, xk.transpose(-2, -1)) / (math.sqrt(self.head_dim))\n",
    "            scores+= self.mask[:, :, :seqlen, :seqlen]\n",
    "            attn_weights = F.softmax(scores, dim=-1)\n",
    "            attn_weights = self.attn_dropout(attn_weights)\n",
    "            attn_output = torch.matmul(attn_weights, xv)\n",
    "        attn_output = attn_output.transpose(1, 2).reshape(bs, seqlen, -1)\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        attn_output = self.res_dropout(attn_output)\n",
    "        return attn_output, past_kv\n",
    "\n",
    "# 测试GroupQueryAttention\n",
    "embed_dim = 512\n",
    "gqa_model = GroupQueryAttention(embed_dim=embed_dim, head_num=8, kv_head_num=4, attn_dropout=0.1, Flash=False, max_seqlen=512)\n",
    "output_gqa, past_kv = gqa_model(output_rmsnorm, pos_cis=None, use_cache=False)\n",
    "print(output_gqa.shape)  # 输出形状应为 (batch_size, sequence_length, embed_dim)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e3c4ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "class GroupQueryAttention(nn.Module):\n",
    "    def __init__(self,config:MinimindConfig):\n",
    "        super(GroupQueryAttention,self).__init__()\n",
    "        #基本属性\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.head_num = config.head_num\n",
    "        self.kv_head_num = config.kv_head_num\n",
    "        self.head_dim = embed_dim // self.head_num\n",
    "        assert embed_dim % self.head_num == 0, \"embed_dim must be divisible by head_num\"\n",
    "        self.rep_num = self.head_num // self.kv_head_num\n",
    "        assert self.head_num % self.kv_head_num == 0, \"kv_head_num must be divisible by head_num\"\n",
    "        self.Flash = hasattr(torch.nn.functional,'scaled_dot_product_attention') and config.Flash\n",
    "        #网络层\n",
    "        self.q_proj = nn.Linear(embed_dim,self.head_num * self.head_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim,self.kv_head_num * self.head_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim,self.kv_head_num * self.head_dim)\n",
    "        self.o_proj = nn.Linear(self.head_num*self.head_dim,self.embed_dim)\n",
    "        self.attn_dropout = nn.Dropout(config.attn_dropout)\n",
    "        self.res_dropout = nn.Dropout(config.attn_res_dropout)\n",
    "        self.max_seqlen = config.max_seqlen\n",
    "        #临时\n",
    "        mask = torch.full((1,1, self.max_seqlen, self.max_seqlen), float('-1e9'))\n",
    "        mask = torch.tril(mask, diagonal=0)\n",
    "        self.register_buffer('mask', mask)\n",
    "    def forward(self,x,\n",
    "                pos_cis=None,\n",
    "                past_key_value=None,\n",
    "                use_cache=False):\n",
    "        bs,seqlen,embed_dim = x.shape\n",
    "        xq = self.q_proj(x)\n",
    "        xk = self.k_proj(x)\n",
    "        xv = self.v_proj(x)\n",
    "\n",
    "        xq = xq.view(bs, seqlen, self.head_num, self.head_dim)\n",
    "        xk = xk.view(bs, seqlen, self.kv_head_num, self.head_dim)\n",
    "        xv = xv.view(bs, seqlen, self.kv_head_num, self.head_dim)\n",
    "        if pos_cis is None:\n",
    "            pos_cis = precompute_pos_cis(embed_dim=self.head_dim, max_seqlen=seqlen)\n",
    "        xq, xk = apply_rotary(xq, xk, pos_cis)\n",
    "        if past_key_value is not None:\n",
    "            xk = torch.cat([past_key_value[0], xk], dim=1)\n",
    "            xv = torch.cat([past_key_value[1], xv], dim=1)\n",
    "        past_kv = (xk, xv) if use_cache else None\n",
    "        xq = xq.transpose(1,2)\n",
    "        xk = repeat_kv(xk, self.rep_num).transpose(1,2)\n",
    "        xv = repeat_kv(xv, self.rep_num).transpose(1,2)\n",
    "        if self.Flash:\n",
    "            attn_output = F.scaled_dot_product_attention(\n",
    "                xq, xk, xv, attn_mask=None, dropout_p=self.dropout,is_causal=True)\n",
    "        else:\n",
    "            scores = torch.matmul(xq, xk.transpose(-2, -1)) / (math.sqrt(self.head_dim))\n",
    "            scores+= self.mask[:, :, :seqlen, :seqlen]\n",
    "            attn_weights = F.softmax(scores, dim=-1)\n",
    "            attn_weights = self.attn_dropout(attn_weights)\n",
    "            attn_output = torch.matmul(attn_weights, xv)\n",
    "        attn_output = attn_output.transpose(1, 2).reshape(bs, seqlen, -1)\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        attn_output = self.res_dropout(attn_output)\n",
    "        return attn_output, past_kv\n",
    "\n",
    "# 测试GroupQueryAttention\n",
    "config2 = MinimindConfig()\n",
    "gqa_model = GroupQueryAttention(config2)\n",
    "output_gqa, past_kv = gqa_model(output_rmsnorm, pos_cis=None, use_cache=False)\n",
    "print(output_gqa.shape)  # 输出形状应为 ("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5889cf2",
   "metadata": {},
   "source": [
    "## 6.FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44f5d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.activations import ACT2FN\n",
    "from torch import nn\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,embed_dim,ffn_dim,ffn_dropout=0.1,act_fn ='silu'):\n",
    "        super(FeedForward,self).__init__()\n",
    "        self.gate = nn.Linear(embed_dim, ffn_dim)\n",
    "        self.up_proj = nn.Linear(embed_dim,ffn_dim)\n",
    "        self.down_proj = nn.Linear(ffn_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(ffn_dropout)\n",
    "        self.act_fn = ACT2FN[act_fn]\n",
    "    def forward(self,x):\n",
    "        return self.down_proj(self.dropout(self.act_fn(self.gate(x)) * self.up_proj(x)))\n",
    "\n",
    "# 测试FeedForward\n",
    "ffn_dim = 2048\n",
    "embed_dim = 512\n",
    "ffn_model = FeedForward(embed_dim=embed_dim, ffn_dim=ffn_dim, ffn_dropout=0.1)\n",
    "output_ffn = ffn_model(output_gqa)\n",
    "print(output_ffn.shape)  # 输出形状应为 (batch_size, sequence_length, embed_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b1e58b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "from transformers.activations import ACT2FN\n",
    "from torch import nn\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,config:MinimindConfig):\n",
    "        super(FeedForward,self).__init__()\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.ffn_dim = config.ffn_dim\n",
    "        self.ffn_dropout = config.ffn_dropout\n",
    "        self.act_fn = config.act_fn\n",
    "        self.gate = nn.Linear(self.embed_dim, self.ffn_dim)\n",
    "        self.up_proj = nn.Linear(self.embed_dim,self.ffn_dim)\n",
    "        self.down_proj = nn.Linear(self.ffn_dim, self.embed_dim)\n",
    "        self.dropout = nn.Dropout(self.ffn_dropout)\n",
    "        self.act_fn = ACT2FN[self.act_fn]\n",
    "    def forward(self,x):\n",
    "        return self.down_proj(self.dropout(self.act_fn(self.gate(x)) * self.up_proj(x)))\n",
    "\n",
    "ffn2 = FeedForward(config2)\n",
    "output_ffn = ffn2(output_gqa)\n",
    "print(output_ffn.shape)  # 输出形状应为 (batch_size, sequence_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c701d3",
   "metadata": {},
   "source": [
    "# 7.Minimind_Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e314b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Minimind_Block(nn.Module):\n",
    "    def __init__(self,layer_id,embed_dim,head_num,kv_head_num,ffn_dim,attn_dropout=0.1,attn_res_dropout=0.1,ffn_dropout=0.1,Flash=False,max_seqlen=512,act_fn=\"silu\"):\n",
    "        super(Minimind_Block,self).__init__()\n",
    "        self.layer_id = layer_id\n",
    "        self.embed_dim = embed_dim\n",
    "        self.head_num = head_num\n",
    "        self.kv_head_num = kv_head_num\n",
    "        self.ffn_dim = ffn_dim\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.res_attn_dropout = attn_res_dropout\n",
    "        self.ffn_dropout = ffn_dropout\n",
    "        self.Flash = Flash\n",
    "        self.max_seqlen = max_seqlen\n",
    "        self.act_fn = act_fn\n",
    "        \n",
    "        self.attention = GroupQueryAttention(embed_dim, head_num, kv_head_num, attn_dropout,attn_res_dropout, Flash, max_seqlen)\n",
    "        self.rmsnorm1 = RMSNorm(embed_dim)\n",
    "        self.ffn = FeedForward(embed_dim, ffn_dim, ffn_dropout,act_fn)\n",
    "        self.rmsnorm2 = RMSNorm(embed_dim)\n",
    "    def forward(self, x, pos_cis=None, past_key_value=None, use_cache=False):\n",
    "        # norm1\n",
    "        x = self.rmsnorm1(x)\n",
    "        # attention\n",
    "        attn_output, past_kv = self.attention(x, pos_cis=pos_cis, past_key_value=past_key_value, use_cache=use_cache)\n",
    "        # residual connection\n",
    "        x = x + attn_output\n",
    "        # norm2\n",
    "        x = self.rmsnorm2(x)\n",
    "        # feed forward\n",
    "        ffn_output = self.ffn(x)\n",
    "        # residual connection\n",
    "        x = x + ffn_output\n",
    "        return x, past_kv\n",
    "# 测试Minimind_Block\n",
    "layer_id = 1\n",
    "embed_dim = 512\n",
    "head_num = 8\n",
    "kv_head_num = 4\n",
    "ffn_dim = 2048\n",
    "minimind_block = Minimind_Block(layer_id, embed_dim, head_num, kv_head_num, ffn_dim, Flash=False, max_seqlen=512)\n",
    "output_minimind, past_kv = minimind_block(output_ffn, pos_cis=None, use_cache=True)\n",
    "print(output_minimind.shape)  # 输出形状应为 (batch_size, sequence_length, embed_dim)\n",
    "print(past_kv[0].shape)  # 输出形状应为 (batch_size, sequence_length, kv_head_num * head_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19c54a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "class Minimind_Block(nn.Module):\n",
    "    def __init__(self,layer_id,config:MinimindConfig):\n",
    "        super(Minimind_Block,self).__init__()\n",
    "        self.layer_id = layer_id\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.head_num = config.head_num\n",
    "        self.kv_head_num = config.kv_head_num\n",
    "        self.ffn_dim = config.ffn_dim\n",
    "        self.attn_dropout = config.attn_dropout\n",
    "        self.res_attn_dropout = config.attn_res_dropout\n",
    "        self.ffn_dropout = config.ffn_dropout\n",
    "        self.Flash = config.Flash\n",
    "        self.max_seqlen = config.max_seqlen\n",
    "        self.act_fn = config.act_fn\n",
    "        \n",
    "        self.attention = GroupQueryAttention(config)\n",
    "        self.rmsnorm1 = RMSNorm(config)\n",
    "        self.ffn = FeedForward(config)\n",
    "        self.rmsnorm2 = RMSNorm(config)\n",
    "    def forward(self, x, pos_cis=None, past_key_value=None, use_cache=False):\n",
    "        # norm1\n",
    "        x = self.rmsnorm1(x)\n",
    "        # attention\n",
    "        attn_output, past_kv = self.attention(x, pos_cis=pos_cis, past_key_value=past_key_value, use_cache=use_cache)\n",
    "        # residual connection\n",
    "        x = x + attn_output\n",
    "        # norm2\n",
    "        x = self.rmsnorm2(x)\n",
    "        # feed forward\n",
    "        ffn_output = self.ffn(x)\n",
    "        # residual connection\n",
    "        x = x + ffn_output\n",
    "        return x, past_kv\n",
    "\n",
    "# 测试Minimind_Block\n",
    "config3 = MinimindConfig()\n",
    "layer_id = 1\n",
    "minimind_block = Minimind_Block(layer_id,config3)\n",
    "output_minimind, past_kv = minimind_block(output_ffn, pos_cis=None, use_cache=True)\n",
    "print(output_minimind.shape)  # 输出形状应为"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ba1e57",
   "metadata": {},
   "source": [
    "## 8.Minimind_Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dde71b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Minimind_Dense(nn.Module):\n",
    "    def __init__(self,block_num,embed_dim,head_num,kv_head_num,ffn_dim,attn_dropout=0.1,attn_res_dropout=0.1,ffn_dropout=0.1,Flash=False,max_seqlen=512,act_fn=\"silu\"):\n",
    "        super(Minimind_Dense,self).__init__()\n",
    "        self.block_num = block_num\n",
    "        self.embed_dim = embed_dim\n",
    "        self.head_num = head_num\n",
    "        self.kv_head_num = kv_head_num\n",
    "        self.ffn_dim = ffn_dim\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.res_attn_dropout = attn_res_dropout\n",
    "        self.ffn_dropout = ffn_dropout\n",
    "        self.Flash = Flash\n",
    "        self.max_seqlen = max_seqlen\n",
    "        self.act_fn = act_fn\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Minimind_Block(layer_id, embed_dim, head_num, kv_head_num, ffn_dim, attn_dropout,attn_res_dropout,ffn_dropout, Flash, max_seqlen,act_fn)\n",
    "            for layer_id in range(block_num)\n",
    "        ])\n",
    "    def forward(self, x, pos_cis=None, past_key_values=None, use_cache=False):\n",
    "        if past_key_values is None:\n",
    "            past_key_values = [None] * self.block_num\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x, past_kv = block(x, pos_cis=pos_cis, past_key_value=past_key_values[i], use_cache=use_cache)\n",
    "            if use_cache:\n",
    "                past_key_values[i] = past_kv\n",
    "        return x, past_key_values\n",
    "# 测试Minimind_Dense\n",
    "block_num = 6\n",
    "minimind_dense = Minimind_Dense(block_num, embed_dim, head_num, kv_head_num, ffn_dim, dropout=0.1, Flash=False, max_seqlen=512)\n",
    "output_minimind_dense, past_key_values = minimind_dense(output_minimind, pos_cis=None, use_cache=True)\n",
    "print(output_minimind_dense.shape)  # 输出形状应为 (batch_size, sequence_length, embed_dim)\n",
    "print(len(past_key_values))  # 输出块数\n",
    "print(past_key_values[0][0].shape)  # 输出形状应为 (batch_size, sequence_length, kv_head_num * head_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f930f2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Minimind_Dense(nn.Module):\n",
    "    def __init__(self,config:MinimindConfig):\n",
    "        super(Minimind_Dense,self).__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Minimind_Block(layer_id, config)\n",
    "            for layer_id in range(config.block_num)\n",
    "        ])\n",
    "    def forward(self, x, pos_cis=None, past_key_values=None, use_cache=False):\n",
    "        if past_key_values is None:\n",
    "            past_key_values = [None] * self.block_num\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x, past_kv = block(x, pos_cis=pos_cis, past_key_value=past_key_values[i], use_cache=use_cache)\n",
    "            if use_cache:\n",
    "                past_key_values[i] = past_kv\n",
    "        return x, past_key_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c563315e",
   "metadata": {},
   "source": [
    "## transformers包装Minimind_Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6086159",
   "metadata": {},
   "source": [
    "![.](../images/LLM-structure.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cd41499",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel,GenerationMixin\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "class MinimindForCausalLM(PreTrainedModel,GenerationMixin):\n",
    "    config_class = MinimindConfig\n",
    "    base_model_prefix = \"minimind\"\n",
    "    def __init__(self,params:MinimindConfig = None):\n",
    "        self.params = params if params is not None else MinimindConfig()\n",
    "        super(MinimindForCausalLM,self).__init__(self.params)\n",
    "        self.embed = Embed(config=self.params)\n",
    "        self.rmsnorm = RMSNorm(config=self.params)\n",
    "        self.minimind_dense = Minimind_Dense(config=self.params)\n",
    "        self.lm_head = nn.Linear(params.embed_dim, params.vocab_size)\n",
    "        ## 临时属性\n",
    "        pos_cis = precompute_pos_cis(embed_dim=params.head_dim, max_seqlen=params.max_seqlen)\n",
    "        self.register_buffer('pos_cis', pos_cis,persistent=False)\n",
    "        self.OUT = CausalLMOutputWithPast()\n",
    "    \n",
    "    def forward(self,input_ids=None,\n",
    "                past_key_values=None,\n",
    "                use_cache = False,\n",
    "                **args):\n",
    "        past_key_values = past_key_values if past_key_values is not None else [None] * self.params.block_num\n",
    "        start_pos = args.get('start_pos', 0)\n",
    "        hidden_states = self.embed(input_ids)\n",
    "        pos_cis = self.pos_cis[start_pos:start_pos+hidden_states.size(1)]\n",
    "        past_kvs = []\n",
    "        ### \n",
    "        print(\"######FORWARD GENERATION######\")\n",
    "        print(f\"input_ids shape: {input_ids.shape if input_ids is not None else 'None'}\")\n",
    "        for i, block in enumerate(self.minimind_dense.blocks):\n",
    "            hidden_states, past_kv = block(hidden_states, pos_cis=pos_cis, past_key_value=past_key_values[i], use_cache=use_cache)\n",
    "            if use_cache:\n",
    "                past_kvs.append(past_kv)\n",
    "            else:\n",
    "                past_kvs.append(None)\n",
    "            print(f\"Block {i+1}/{self.params.block_num} processed, hidden_states shape: {hidden_states.shape}\")\n",
    "        hidden_states = self.rmsnorm(hidden_states)\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        print(\"######FORWARD GENERATION END######\")\n",
    "        self.OUT.__setitem__('logits', logits)\n",
    "        self.OUT.__setitem__('past_key_values', past_kvs)\n",
    "        self.OUT.__setitem__('hidden_states', hidden_states)\n",
    "        return self.OUT\n",
    "    \n",
    "    # 推理函数（包含top-p或者top-k等策略）\n",
    "    @torch.inference_mode()\n",
    "    def generate(self,  input_ids,  eos_token_id=1,  max_new_tokens=512,  temperature=0.75,  top_p=0.90, \n",
    "                stream=False,  rp=1,  use_cache=True,  pad_token_id=3,  **args):\n",
    "        # 流式生成 （返回生成器, 自由控制输出）\n",
    "        if stream:\n",
    "            return self._stream(input_ids,  eos_token_id,  max_new_tokens,  temperature,  top_p,  rp,  use_cache,  **args)\n",
    "        # 直接生成 （一步到位）\n",
    "        generated = []\n",
    "        for i in range(input_ids.size(0)):\n",
    "            non_pad = input_ids[i][input_ids[i] != pad_token_id].unsqueeze(0)\n",
    "            out = self._stream(non_pad,  eos_token_id,  max_new_tokens,  temperature,  top_p,  rp,  use_cache,  **args)\n",
    "            tokens_list = [tokens[:,  -1:] for tokens in out]\n",
    "            print(f'new tokens list :{tokens_list}\\n')\n",
    "            gen = torch.cat(tokens_list,  dim=-1) if tokens_list else non_pad\n",
    "            full_sequence = torch.cat([non_pad,  gen],  dim=-1)\n",
    "            generated.append(full_sequence)\n",
    "        max_length = max(seq.size(1) for seq in generated)\n",
    "        generated = [\n",
    "            torch.cat([seq,  torch.full((1,  max_length - seq.size(1)),  pad_token_id,  dtype=seq.dtype,  device=seq.device)], dim=-1) \n",
    "            for seq in generated\n",
    "        ]\n",
    "        return torch.cat(generated,  dim=0)\n",
    "\n",
    "    # 流式输出\n",
    "    def _stream(self,  input_ids,  eos_token_id,  max_new_tokens,  temperature,  top_p,  rp,  use_cache,  **args):\n",
    "        start,  first_seq,  past_kvs = input_ids.shape[1],  True,  None\n",
    "        new_token_idx = 0 #  new token 计数器\n",
    "        while input_ids.shape[1] < max_new_tokens - 1:\n",
    "            print(f'gernerating new token: idx = {start + new_token_idx}')\n",
    "            if first_seq or not use_cache: # 若第一次生成序列 or 无 KV Cache,  每次生成传入整个 token id 序列\n",
    "                out,  first_seq = self(input_ids,  past_key_values=past_kvs,  use_cache=use_cache,  **args),  False\n",
    "            else: # 若非第一次生成 and 有 KV Cache, 每次传入最后一个 token id 与 KV Cache 进行推理加速\n",
    "                out = self(input_ids[:,  -1:],  past_key_values=past_kvs,  use_cache=use_cache, \n",
    "                           start_pos=input_ids.shape[1] - 1,  **args)\n",
    "            logits,  past_kvs = out.logits[:,  -1,  :],  out.past_key_values # logits.shape: (batch_size,  seq_len,  vocab_size), 获取最后一位 logits\n",
    "            logits[:,  list(set(input_ids.tolist()[0]))] /= rp # 对生成 token 进行惩罚, 降低后续重复生成几率\n",
    "            logits /= (temperature + 1e-9) # 调整温度, 控制生成多样性\n",
    "            if top_p is not None and top_p < 1.0: # top-p 采样\n",
    "                sorted_logits,  sorted_indices = torch.sort(logits,  descending=True,  dim=-1)\n",
    "                sorted_probs = F.softmax(sorted_logits,  dim=-1)\n",
    "                cumulative_probs = torch.cumsum(sorted_probs,  dim=-1)\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[:,  1:] = sorted_indices_to_remove[:,  :-1].clone()\n",
    "                sorted_indices_to_remove[:,  0] = False\n",
    "                indices_to_remove = sorted_indices_to_remove.scatter(1,  sorted_indices,  sorted_indices_to_remove)\n",
    "                logits[indices_to_remove] = -float('Inf')\n",
    "            input_ids_next = torch.multinomial(F.softmax(logits,  dim=-1),  num_samples=1) # 从保留的 token 中采样\n",
    "            input_ids = torch.cat((input_ids,  input_ids_next),  dim=1)\n",
    "            new_token_idx += 1\n",
    "            yield input_ids[:,  start:]\n",
    "            if input_ids_next.item() == eos_token_id:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2513ea54",
   "metadata": {},
   "source": [
    "### 最终测试model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4ed230c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinimindForCausalLM(\n",
      "  (embed): Embed(\n",
      "    (embedding): Embedding(6400, 512)\n",
      "  )\n",
      "  (rmsnorm): RMSNorm()\n",
      "  (minimind_dense): Minimind_Dense(\n",
      "    (blocks): ModuleList(\n",
      "      (0-7): 8 x Minimind_Block(\n",
      "        (attention): GroupQueryAttention(\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (k_proj): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (o_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (res_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (rmsnorm1): RMSNorm()\n",
      "        (ffn): FeedForward(\n",
      "          (gate): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (up_proj): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (down_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (rmsnorm2): RMSNorm()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=6400, bias=True)\n",
      ")\n",
      "new tokens list :[]\n",
      "\n",
      "new tokens list :[]\n",
      "\n",
      "torch.Size([2, 548])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|im_start|>根据输入的内容，编写一个类别标签。这是一篇介绍如何阅读心电图的文章类别标签: 医学/心电图阅读指南<|im_end|> <|im_start|>帮我搜索一下最近的天气情况。当然，我可以帮您搜索最新的天气情况。请问您需要查询哪个城市的天气情况呢？<|im_end|> <|im_start|>帮我讲一个令人开心的笑话。好的，我帮您讲一个关于细菌的笑话。为什么细菌不会上网？因为连接总是断开了！<|im_end|> <|im_start|>现在给我生成一首关于大海的五言诗。碧波万顷月满天，海天相接处天地间。波涛滚滚江山美，海鸟翱翔日月闲。<|im_end|> <|im_start|>谢谢你，这篇文章很有用。不客气，我很高兴能够为您提供帮助。如果您还有其他问题或需求，随时可以对我说。<|im_end|> <|im_start|>你好，我想下载一个视频编辑软件，你有什么推荐吗？您好！当然，有很多选择。您想要免费软件还是愿意付费？<|im_end|> <|im_start|>为什么我的程序不输出正确结果？可能是代码逻辑有误，或者输入数据有误，需要仔细调试代码逻辑和输入数据。<|im_end|> <|im_start|>谢谢你的回答。现在我想知道这场比赛的具体时间和地点。这场比赛的时间是北京时间10月4日，地点是上海。<|im_end|><|im_start|>根据输入的内容，编写一个类别标签。这是一篇介绍如何阅读心电图的文章类别标签: 医学/心电图阅读指南<|im_end|> <|im_start|>帮我搜索一下最近的天气情况。当然，我可以帮您搜索最新的天气情况。请问您需要查询哪个城市的天气情况呢？<|im_end|> <|im_start|>帮我讲一个令人开心的笑话。好的，我帮您讲一个关于细菌的笑话。为什么细菌不会上网？因为连接总是断开了！<|im_end|> <|im_start|>现在给我生成一首关于大海的五言诗。碧波万顷月满天，海天相接处天地间。波涛滚滚江山美，海鸟翱翔日月闲。<|im_end|> <|im_start|>谢谢你，这篇文章很有用。不客气，我很高兴能够为您提供帮助。如果您还有其他问题或需求，随时可以对我说。<|im_end|> <|im_start|>你好，我想下载一个视频编辑软件，你有什么推荐吗？您好！当然，有很多选择。您想要免费软件还是愿意付费？<|im_end|> <|im_start|>为什么我的程序不输出正确结果？可能是代码逻辑有误，或者输入数据有误，需要仔细调试代码逻辑和输入数据。<|im_end|> <|im_start|>谢谢你的回答。现在我想知道这场比赛的具体时间和地点。这场比赛的时间是北京时间10月4日，地点是上海。<|im_end|><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_dense = MinimindConfig()\n",
    "model = MinimindForCausalLM(config_dense)\n",
    "print(model)\n",
    "# 生成新的token\n",
    "outputs2 = model.generate(input_ids=input_ids, max_new_tokens=10, use_cache=True,)\n",
    "# 从最初的tokenizer解码outputs中取logits进行解码\n",
    "print(outputs2.shape)  # 输出形状应为 (batch_size, sequence_length + max_new_tokens)\n",
    "tokenizer.decode(outputs2[1], skip_special_tokens=False)  # 解码第一个batch的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06c5e769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   2, 2899]]), 'token_type_ids': tensor([[0, 0]]), 'attention_mask': tensor([[1, 1]])}\n",
      "gernerating new token: idx = 2\n",
      "######FORWARD GENERATION######\n",
      "input_ids shape: torch.Size([1, 2])\n",
      "Block 1/8 processed, hidden_states shape: torch.Size([1, 2, 512])\n",
      "Block 2/8 processed, hidden_states shape: torch.Size([1, 2, 512])\n",
      "Block 3/8 processed, hidden_states shape: torch.Size([1, 2, 512])\n",
      "Block 4/8 processed, hidden_states shape: torch.Size([1, 2, 512])\n",
      "Block 5/8 processed, hidden_states shape: torch.Size([1, 2, 512])\n",
      "Block 6/8 processed, hidden_states shape: torch.Size([1, 2, 512])\n",
      "Block 7/8 processed, hidden_states shape: torch.Size([1, 2, 512])\n",
      "Block 8/8 processed, hidden_states shape: torch.Size([1, 2, 512])\n",
      "######FORWARD GENERATION END######\n",
      "gernerating new token: idx = 3\n",
      "######FORWARD GENERATION######\n",
      "input_ids shape: torch.Size([1, 1])\n",
      "Block 1/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 2/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 3/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 4/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 5/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 6/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 7/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 8/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "######FORWARD GENERATION END######\n",
      "gernerating new token: idx = 4\n",
      "######FORWARD GENERATION######\n",
      "input_ids shape: torch.Size([1, 1])\n",
      "Block 1/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 2/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 3/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 4/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 5/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 6/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 7/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 8/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "######FORWARD GENERATION END######\n",
      "gernerating new token: idx = 5\n",
      "######FORWARD GENERATION######\n",
      "input_ids shape: torch.Size([1, 1])\n",
      "Block 1/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 2/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 3/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 4/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 5/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 6/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 7/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 8/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "######FORWARD GENERATION END######\n",
      "gernerating new token: idx = 6\n",
      "######FORWARD GENERATION######\n",
      "input_ids shape: torch.Size([1, 1])\n",
      "Block 1/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 2/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 3/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 4/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 5/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 6/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 7/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 8/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "######FORWARD GENERATION END######\n",
      "gernerating new token: idx = 7\n",
      "######FORWARD GENERATION######\n",
      "input_ids shape: torch.Size([1, 1])\n",
      "Block 1/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 2/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 3/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 4/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 5/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 6/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 7/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 8/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "######FORWARD GENERATION END######\n",
      "gernerating new token: idx = 8\n",
      "######FORWARD GENERATION######\n",
      "input_ids shape: torch.Size([1, 1])\n",
      "Block 1/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 2/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 3/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 4/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 5/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 6/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 7/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "Block 8/8 processed, hidden_states shape: torch.Size([1, 1, 512])\n",
      "######FORWARD GENERATION END######\n",
      "new tokens list :[tensor([[4954]]), tensor([[1020]]), tensor([[1917]]), tensor([[3527]]), tensor([[2746]]), tensor([[6202]]), tensor([[2986]])]\n",
      "\n",
      "torch.Size([1, 9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|im_start|>你好会在率慢的感觉可能是完成任务哥'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_dense2 = MinimindConfig()\n",
    "model2 = MinimindForCausalLM(config_dense2)\n",
    "input_test2=tokenizer(\"<|im_start|>你好\", return_tensors='pt')\n",
    "print(input_test2)\n",
    "outputs3 = model2.generate(input_ids=input_test2['input_ids'], max_new_tokens=10, use_cache=True,)\n",
    "print(outputs3.shape)  # 输出形状应为 (batch_size, sequence_length +\n",
    "tokenizer.decode(outputs3[0], skip_special_tokens=False)  # 解码第一个batch的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80e10b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>鉴别一组中文文章的风格和特点，例如官方、口语、文言等。需要提供样例文章才能准确鉴别不同的风格和特点。<|im_end|> <|im_start|>好的，现在帮我查一下今天的天气怎么样?今天的天气依据地区而异。请问你需要我帮你查询哪个地区的天气呢？<|im_end|> <|im_start|>打开闹钟功能，定一个明天早上七点的闹钟。好的，我已经帮您打开闹钟功能，闹钟将在明天早上七点准时响起。<|im_end|> <|im_start|>为以下场景写一句话描述：一个孤独的老人坐在公园长椅上看着远处。一位孤独的老人坐在公园长椅上凝视远方。<|im_end|> <|im_start|>非常感谢你的回答。请告诉我，这些数据是关于什么主题的？这些数据是关于不同年龄段的男女人口比例分布的。<|im_end|> <|im_start|>帮我想一个有趣的标题。这个挺有趣的：\"如何成为一名成功的魔术师\" 调皮的标题往往会吸引读者的注意力。<|im_end|> <|im_start|>回答一个问题，地球的半径是多少？地球的平均半径约为6371公里，这是地球自赤道到两极的距离的平均值。<|im_end|> <|im_start|>识别文本中的语气，并将其分类为喜悦、悲伤、惊异等.文本：“今天是我的生日！”这个文本的语气是喜悦。<|im_end|>\n",
      "<|im_start|>根据输入的内容，编写一个类别标签。这是一篇介绍如何阅读心电图的文章类别标签: 医学/心电图阅读指南<|im_end|> <|im_start|>帮我搜索一下最近的天气情况。当然，我可以帮您搜索最新的天气情况。请问您需要查询哪个城市的天气情况呢？<|im_end|> <|im_start|>帮我讲一个令人开心的笑话。好的，我帮您讲一个关于细菌的笑话。为什么细菌不会上网？因为连接总是断开了！<|im_end|> <|im_start|>现在给我生成一首关于大海的五言诗。碧波万顷月满天，海天相接处天地间。波涛滚滚江山美，海鸟翱翔日月闲。<|im_end|> <|im_start|>谢谢你，这篇文章很有用。不客气，我很高兴能够为您提供帮助。如果您还有其他问题或需求，随时可以对我说。<|im_end|> <|im_start|>你好，我想下载一个视频编辑软件，你有什么推荐吗？您好！当然，有很多选择。您想要免费软件还是愿意付费？<|im_end|> <|im_start|>为什么我的程序不输出正确结果？可能是代码逻辑有误，或者输入数据有误，需要仔细调试代码逻辑和输入数据。<|im_end|> <|im_start|>谢谢你的回答。现在我想知道这场比赛的具体时间和地点。这场比赛的时间是北京时间10月4日，地点是上海。<|im_end|>\n",
      "new tokens list :[]\n",
      "\n",
      "new tokens list :[]\n",
      "\n",
      "torch.Size([2, 548])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|im_start|>鉴别一组中文文章的风格和特点，例如官方、口语、文言等。需要提供样例文章才能准确鉴别不同的风格和特点。<|im_end|> <|im_start|>好的，现在帮我查一下今天的天气怎么样?今天的天气依据地区而异。请问你需要我帮你查询哪个地区的天气呢？<|im_end|> <|im_start|>打开闹钟功能，定一个明天早上七点的闹钟。好的，我已经帮您打开闹钟功能，闹钟将在明天早上七点准时响起。<|im_end|> <|im_start|>为以下场景写一句话描述：一个孤独的老人坐在公园长椅上看着远处。一位孤独的老人坐在公园长椅上凝视远方。<|im_end|> <|im_start|>非常感谢你的回答。请告诉我，这些数据是关于什么主题的？这些数据是关于不同年龄段的男女人口比例分布的。<|im_end|> <|im_start|>帮我想一个有趣的标题。这个挺有趣的：\"如何成为一名成功的魔术师\" 调皮的标题往往会吸引读者的注意力。<|im_end|> <|im_start|>回答一个问题，地球的半径是多少？地球的平均半径约为6371公里，这是地球自赤道到两极的距离的平均值。<|im_end|> <|im_start|>识别文本中的语气，并将其分类为喜悦、悲伤、惊异等.文本：“今天是我的生日！”这个文本的语气是喜悦。<|im_end|><|im_start|>鉴别一组中文文章的风格和特点，例如官方、口语、文言等。需要提供样例文章才能准确鉴别不同的风格和特点。<|im_end|> <|im_start|>好的，现在帮我查一下今天的天气怎么样?今天的天气依据地区而异。请问你需要我帮你查询哪个地区的天气呢？<|im_end|> <|im_start|>打开闹钟功能，定一个明天早上七点的闹钟。好的，我已经帮您打开闹钟功能，闹钟将在明天早上七点准时响起。<|im_end|> <|im_start|>为以下场景写一句话描述：一个孤独的老人坐在公园长椅上看着远处。一位孤独的老人坐在公园长椅上凝视远方。<|im_end|> <|im_start|>非常感谢你的回答。请告诉我，这些数据是关于什么主题的？这些数据是关于不同年龄段的男女人口比例分布的。<|im_end|> <|im_start|>帮我想一个有趣的标题。这个挺有趣的：\"如何成为一名成功的魔术师\" 调皮的标题往往会吸引读者的注意力。<|im_end|> <|im_start|>回答一个问题，地球的半径是多少？地球的平均半径约为6371公里，这是地球自赤道到两极的距离的平均值。<|im_end|> <|im_start|>识别文本中的语气，并将其分类为喜悦、悲伤、惊异等.文本：“今天是我的生日！”这个文本的语气是喜悦。<|im_end|>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 写一个虚拟的小的数据集，只有两条数据的集\n",
    "data2=[\n",
    "    {'text':'<|im_start|>鉴别一组中文文章的风格和特点<|im_end|> '\n",
    "},\n",
    "    {'text':'<|im_start|>根据输入的内容，编写一个类别标签。<|im_end|> <|im_start|>'\n",
    "}\n",
    "]\n",
    "for i in range(2):\n",
    "    print(data[i]['text'])\n",
    "\n",
    "# 接下来将该data的内容利用tokenizer编码\n",
    "input_texts = [item['text'] for item in data]\n",
    "input_test3 = tokenizer(input_texts, padding='max_length', truncation=True, max_length=512,return_tensors='pt')\n",
    "outputs4 = model2.generate(input_ids=input_test3['input_ids'], max_new_tokens=10, use_cache=True,)\n",
    "print(outputs4.shape)  # 输出形状应为 (batch_size, sequence_length +\n",
    "tokenizer.decode(outputs4[0], skip_special_tokens=False)  # 解码第一个batch的输出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1f1a0d",
   "metadata": {},
   "source": [
    "# 存在的问题\n",
    "1.为什么outputs2  直接吐出的就是原句子\n",
    "\n",
    "2.为什么outputs4  吐出来的new_tokens list竟然是空的，也就是没有任何词语吐出来\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b0e5fa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
