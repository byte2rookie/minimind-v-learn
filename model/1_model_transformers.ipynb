{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8078203",
   "metadata": {},
   "source": [
    "# 利用transformers库封装model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46c4db1",
   "metadata": {},
   "source": [
    "通过上一个文件，我们已经知道了minimind-dense的torch模型了<br>\n",
    "现在我们用transformers库来封装model，方便后续的上传和推理过程<br>\n",
    "要素：\n",
    "- tokenizer\n",
    "- embedding\n",
    "- minimind-block\n",
    "    - RoPE\n",
    "    - RMSNorm\n",
    "    - GQA\n",
    "    - FFN\n",
    "- lm_head\n",
    "- de-tokenizer\n",
    "\n",
    "[关于transformers库](https://cloud.tencent.com/developer/article/2367010)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396e2f40",
   "metadata": {},
   "source": [
    "## 1.tokenizer\n",
    "使用我们之前预训练数据集训练好的tokenizer\n",
    "\n",
    "[关于tokenizer参数的相关设置](https://zhuanlan.zhihu.com/p/341994096) <br>\n",
    "[关于left_padding和right_padding的讨论](https://zhuanlan.zhihu.com/p/646852375)<br>\n",
    "[如何改进增强长文本处理能力](https://zhuanlan.zhihu.com/p/638976034)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98580fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/zyp/miniconda3/envs/minimind/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='./', vocab_size=6400, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|im_start|>', 'eos_token': '<|im_end|>', 'unk_token': '<|endoftext|>', 'pad_token': '<pad>', 'additional_special_tokens': ['<pad>', '<mask>', '<s>', '</s>', '<unk>', '<UNK>', '<EOS>', '<zzy>', '<|s1|>', '<|s2|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<endoftext>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t5: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t6: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t7: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t8: AddedToken(\"<UNK>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t9: AddedToken(\"<EOS>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t10: AddedToken(\"<zzy>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t11: AddedToken(\"<|s1|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t12: AddedToken(\"<|s2|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t6400: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n",
      "6400\n",
      "{'bos_token': '<|im_start|>', 'eos_token': '<|im_end|>', 'unk_token': '<|endoftext|>', 'pad_token': '<pad>', 'additional_special_tokens': ['<pad>', '<mask>', '<s>', '</s>', '<unk>', '<UNK>', '<EOS>', '<zzy>', '<|s1|>', '<|s2|>']}\n",
      "['<|im_start|>', '<|im_end|>', '<|endoftext|>', '<pad>', '<mask>', '<s>', '</s>', '<unk>', '<UNK>', '<EOS>', '<zzy>', '<|s1|>', '<|s2|>']\n",
      "<|im_start|>鉴别一组中文文章的风格和特点，例如官方、口语、文言等。需要提供样例文章才能准确鉴别不同的风格和特点。<|im_end|> <|im_start|>好的，现在帮我查一下今天的天气怎么样?今天的天气依据地区而异。请问你需要我帮你查询哪个地区的天气呢？<|im_end|> <|im_start|>打开闹钟功能，定一个明天早上七点的闹钟。好的，我已经帮您打开闹钟功能，闹钟将在明天早上七点准时响起。<|im_end|> <|im_start|>为以下场景写一句话描述：一个孤独的老人坐在公园长椅上看着远处。一位孤独的老人坐在公园长椅上凝视远方。<|im_end|> <|im_start|>非常感谢你的回答。请告诉我，这些数据是关于什么主题的？这些数据是关于不同年龄段的男女人口比例分布的。<|im_end|> <|im_start|>帮我想一个有趣的标题。这个挺有趣的：\"如何成为一名成功的魔术师\" 调皮的标题往往会吸引读者的注意力。<|im_end|> <|im_start|>回答一个问题，地球的半径是多少？地球的平均半径约为6371公里，这是地球自赤道到两极的距离的平均值。<|im_end|> <|im_start|>识别文本中的语气，并将其分类为喜悦、悲伤、惊异等.文本：“今天是我的生日！”这个文本的语气是喜悦。<|im_end|>\n",
      "<|im_start|>根据输入的内容，编写一个类别标签。这是一篇介绍如何阅读心电图的文章类别标签: 医学/心电图阅读指南<|im_end|> <|im_start|>帮我搜索一下最近的天气情况。当然，我可以帮您搜索最新的天气情况。请问您需要查询哪个城市的天气情况呢？<|im_end|> <|im_start|>帮我讲一个令人开心的笑话。好的，我帮您讲一个关于细菌的笑话。为什么细菌不会上网？因为连接总是断开了！<|im_end|> <|im_start|>现在给我生成一首关于大海的五言诗。碧波万顷月满天，海天相接处天地间。波涛滚滚江山美，海鸟翱翔日月闲。<|im_end|> <|im_start|>谢谢你，这篇文章很有用。不客气，我很高兴能够为您提供帮助。如果您还有其他问题或需求，随时可以对我说。<|im_end|> <|im_start|>你好，我想下载一个视频编辑软件，你有什么推荐吗？您好！当然，有很多选择。您想要免费软件还是愿意付费？<|im_end|> <|im_start|>为什么我的程序不输出正确结果？可能是代码逻辑有误，或者输入数据有误，需要仔细调试代码逻辑和输入数据。<|im_end|> <|im_start|>谢谢你的回答。现在我想知道这场比赛的具体时间和地点。这场比赛的时间是北京时间10月4日，地点是上海。<|im_end|>\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 274])\n",
      "{'input_ids': tensor([[   2, 5203,  723,  ...,    3,    3,    3],\n",
      "        [   2, 4496, 2685,  ...,    3,    3,    3]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "{'input_ids': tensor([[   2, 5203,  723, 2828, 3014,  517, 4187,  297, 2002,  273,  933, 2266,\n",
      "          420,  308, 4691,  308, 5699,  419,  275,  522,  598,  828,  747,  517,\n",
      "         1712, 1544, 5203,  723, 1426, 1798,  297, 2002,  275,    1,  233,    2,\n",
      "          719,  273, 1150, 1895, 1125, 1028, 6139, 4693,   43, 6139, 2034,  472,\n",
      "         2001,  536, 1826,  275, 1785, 2896,  309,  675,  405, 4094, 2668, 2001,\n",
      "         2259, 1357,  433,    1,  233,    2, 3886, 6260, 1795, 1649,  273,  451,\n",
      "          386, 2415, 2707, 3205,  555,  272, 6260, 1795,  275,  719,  273, 4965,\n",
      "          675,  532, 3886, 6260, 1795, 1649,  273, 6260, 1795, 3832, 2415, 2707,\n",
      "         3205,  555,  955,  402,  755,  669,  275,    1,  233,    2,  348,  587,\n",
      "         1876,  643, 2646,  853,  320,  386, 2969,  272, 6255, 5593, 1776,  685,\n",
      "         4862,  412, 6277, 1340,  726,  275, 1730, 2969,  272, 6255, 5593, 1776,\n",
      "          685, 4862,  412, 4935,  919, 1340,  420,  275,    1,  233,    2, 3831,\n",
      "         1181, 1021,  275, 2902,  273,  748,  841, 4127,  673,  968,  272,  433,\n",
      "          748,  841, 4127,  832, 3297,  644,  272, 1257, 1086, 3920,  641,  747,\n",
      "         3924,  272,  275,    1,  233,    2, 4877,  386, 1644, 3544,  275,  569,\n",
      "         6129, 3283, 4582,  878, 1110, 2098, 4278, 2329,  556, 1199,   14,  233,\n",
      "          913, 1838, 3544, 4885,  418, 1931, 2326, 5959,  576,  474,  275,    1,\n",
      "          233,    2, 1021,  386,  533,  273, 4461, 2220, 2760, 4540,  433, 4461,\n",
      "         2391, 2220, 2760, 5213,   34,   31,   35,   29, 3952,  273, 1650, 1292,\n",
      "          422,  429,  110,  663,  392,  742,  819,  272, 2643,  272, 2391, 1105,\n",
      "          275,    1,  233,    2, 1320, 3176, 3870,  273, 4625, 2517,  704, 2842,\n",
      "          308, 6169,  308, 2068, 1826,  419,   26,  528, 1364, 1279,  302, 1317,\n",
      "         3344, 5476, 6373,  272, 3870,  302,  704, 2842,  275,    1],\n",
      "        [   2, 4496, 2685,  273, 4242, 2296, 4281,  275, 4614, 1241,  878, 1578,\n",
      "          573,  523, 1117, 1622, 2296, 4281,   38,  233, 2106,   27,  573,  523,\n",
      "         1117, 1578, 5359,    1,  233,    2, 1895, 2580, 1028, 1265, 2259, 1469,\n",
      "          275, 1436,  273, 1957,  675,  532, 2580, 4654, 1162, 1469,  275, 1785,\n",
      "         3408, 4094, 2668, 1031, 2259, 1469, 1357,  433,    1,  233,    2, 1895,\n",
      "         1276,  386, 2138, 4369, 6053,  678,  275,  719,  273,  309,  675,  532,\n",
      "         1276,  386,  927, 1026, 4044, 6053,  678,  275, 1847, 1026, 4044, 1985,\n",
      "          412,  929,  433,  857, 3031, 2611,  814,  634,  322, 1005,    1,  233,\n",
      "            2, 1150, 1140, 6268,  927,  430,  807,  272, 1411,  764, 1141,  275,\n",
      "         1196,  113, 1811, 1553,  617,  128, 1013, 1043,  470,  273,  807,  470,\n",
      "          592,  982,  726,  470,  410,  545,  275, 1811,  733,  262, 5542, 5542,\n",
      "         2476, 1033,  601,  273,  807, 1661, 1065,  122, 5759,  696, 1013, 2612,\n",
      "          275,    1,  233,    2, 5356,  273, 1036, 3875,  358,  275,  357,  960,\n",
      "          564,  273, 2772, 2929,  907, 3799,  793,  275, 1824, 4922,  533,  491,\n",
      "         1524,  273, 3426,  369,  403,  309,  652,  275,    1,  233,    2, 2899,\n",
      "          273, 1402,  384, 3200,  386, 3311, 1740, 2284,  273, 4688, 1261,  926,\n",
      "          433, 4706, 1005, 1436,  273, 2092,  899,  275,  532, 2014, 6352, 2284,\n",
      "         1605, 4423, 2169, 1288,  433,    1,  233,    2, 1847,  309, 6038,  357,\n",
      "         2186, 1654, 1521,  433, 2746, 3155, 3019,  319, 1247,  273, 1126, 1486,\n",
      "          841,  319, 1247,  273,  522, 6036,  913,  871, 3155, 3019,  297, 1486,\n",
      "          841,  275,    1,  233,    2, 5443, 1021,  275, 1150, 4194, 4943, 1756,\n",
      "         4163, 3147, 2470,  275, 4943, 1756, 2235,  302, 1875,  790, 1134, 1013,\n",
      "           32,  696,  273, 2470,  302, 2763,  275,    1,    3,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer_path= \"./\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "print(tokenizer)\n",
    "print(tokenizer.vocab_size)\n",
    "print(tokenizer.special_tokens_map)\n",
    "print(tokenizer.all_special_tokens)\n",
    "\n",
    "# 写一个虚拟的小的数据集，只有两条数据的集\n",
    "data=[\n",
    "    {'text':'<|im_start|>鉴别一组中文文章的风格和特点，例如官方、口语、文言等。需要提供样例文章才能准确鉴别不同的风格和特点。<|im_end|> <|im_start|>好的，现在帮我查一下今天的天气怎么样?今天的天气依据地区而异。请问你需要我帮你查询哪个地区的天气呢？<|im_end|> <|im_start|>打开闹钟功能，定一个明天早上七点的闹钟。好的，我已经帮您打开闹钟功能，闹钟将在明天早上七点准时响起。<|im_end|> <|im_start|>为以下场景写一句话描述：一个孤独的老人坐在公园长椅上看着远处。一位孤独的老人坐在公园长椅上凝视远方。<|im_end|> <|im_start|>非常感谢你的回答。请告诉我，这些数据是关于什么主题的？这些数据是关于不同年龄段的男女人口比例分布的。<|im_end|> <|im_start|>帮我想一个有趣的标题。这个挺有趣的：\"如何成为一名成功的魔术师\" 调皮的标题往往会吸引读者的注意力。<|im_end|> <|im_start|>回答一个问题，地球的半径是多少？地球的平均半径约为6371公里，这是地球自赤道到两极的距离的平均值。<|im_end|> <|im_start|>识别文本中的语气，并将其分类为喜悦、悲伤、惊异等.文本：“今天是我的生日！”这个文本的语气是喜悦。<|im_end|>'\n",
    "},\n",
    "    {'text':'<|im_start|>根据输入的内容，编写一个类别标签。这是一篇介绍如何阅读心电图的文章类别标签: 医学/心电图阅读指南<|im_end|> <|im_start|>帮我搜索一下最近的天气情况。当然，我可以帮您搜索最新的天气情况。请问您需要查询哪个城市的天气情况呢？<|im_end|> <|im_start|>帮我讲一个令人开心的笑话。好的，我帮您讲一个关于细菌的笑话。为什么细菌不会上网？因为连接总是断开了！<|im_end|> <|im_start|>现在给我生成一首关于大海的五言诗。碧波万顷月满天，海天相接处天地间。波涛滚滚江山美，海鸟翱翔日月闲。<|im_end|> <|im_start|>谢谢你，这篇文章很有用。不客气，我很高兴能够为您提供帮助。如果您还有其他问题或需求，随时可以对我说。<|im_end|> <|im_start|>你好，我想下载一个视频编辑软件，你有什么推荐吗？您好！当然，有很多选择。您想要免费软件还是愿意付费？<|im_end|> <|im_start|>为什么我的程序不输出正确结果？可能是代码逻辑有误，或者输入数据有误，需要仔细调试代码逻辑和输入数据。<|im_end|> <|im_start|>谢谢你的回答。现在我想知道这场比赛的具体时间和地点。这场比赛的时间是北京时间10月4日，地点是上海。<|im_end|>'\n",
    "}\n",
    "]\n",
    "for i in range(2):\n",
    "    print(data[i]['text'])\n",
    "\n",
    "# 接下来将该data的内容利用tokenizer编码\n",
    "input_texts = [item['text'] for item in data]\n",
    "#填充1 固定填充\n",
    "input_ids1 = tokenizer(input_texts, padding='max_length', truncation=True, max_length=512,return_tensors='pt')\n",
    "#填充2 动态填充\n",
    "input_ids2 = tokenizer(input_texts, padding=True, truncation=True, max_length=512,return_tensors='pt')\n",
    "print(input_ids1['input_ids'].shape)\n",
    "print(input_ids2['input_ids'].shape)\n",
    "print(input_ids1)\n",
    "print(input_ids2)\n",
    "# 这样我们就获取到了tokenizer编码后的数据\n",
    "#现在我们深拷贝input_ids1\n",
    "import copy\n",
    "input_ids = copy.deepcopy(input_ids1['input_ids'])\n",
    "# 后续我们采用这个input_ids进行模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3dae6b",
   "metadata": {},
   "source": [
    "## 2.Embedding\n",
    "对应参数 vocab_size,embed_dim<br>\n",
    "这里是 vocab_size=6400 embed_dim=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c2b494f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512, 512])\n",
      "tensor([[[-0.4812, -1.0430,  0.0950,  ..., -0.4464, -1.6892, -0.0207],\n",
      "         [ 0.9512, -0.7596, -0.2028,  ...,  1.4719, -0.7992, -0.2419],\n",
      "         [-0.2898,  1.0540,  1.7324,  ..., -0.5300, -0.6745,  2.3153],\n",
      "         ...,\n",
      "         [-0.3506,  0.2100, -0.4885,  ..., -0.0817, -1.0394,  1.8971],\n",
      "         [-0.3506,  0.2100, -0.4885,  ..., -0.0817, -1.0394,  1.8971],\n",
      "         [-0.3506,  0.2100, -0.4885,  ..., -0.0817, -1.0394,  1.8971]],\n",
      "\n",
      "        [[-0.4812, -1.0430,  0.0950,  ..., -0.4464, -1.6892, -0.0207],\n",
      "         [ 0.5114,  0.1778, -0.6816,  ..., -1.8110,  0.9594, -0.2983],\n",
      "         [-0.9218, -0.0879,  0.3628,  ...,  0.1268,  1.1345, -0.3110],\n",
      "         ...,\n",
      "         [-0.3506,  0.2100, -0.4885,  ..., -0.0817, -1.0394,  1.8971],\n",
      "         [-0.3506,  0.2100, -0.4885,  ..., -0.0817, -1.0394,  1.8971],\n",
      "         [-0.3506,  0.2100, -0.4885,  ..., -0.0817, -1.0394,  1.8971]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 测试版\n",
    "import torch \n",
    "from torch import nn\n",
    "class Embed(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(Embed, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        return self.embedding(input_ids)\n",
    "\n",
    "# 测试\n",
    "embed_dim=512\n",
    "vocab_size=6400\n",
    "embed_model = Embed(vocab_size, embed_dim)\n",
    "output = embed_model(input_ids)\n",
    "print(output.shape)  # 输出形状应为 (batch_size, sequence_length, embed\n",
    "print(output)\n",
    "\n",
    "#待会儿直接用就行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91067a1f",
   "metadata": {},
   "source": [
    "## 3.RMSNorm\n",
    "计算公式为\n",
    "$$ a_i=\\frac{a_i}{RMS(a)+\\epsilon} * \\gamma \\quad where \\quad RMS(a) = \\sqrt{\\frac{1}{n}\\sum^n_{i=1}a^2_i} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6284e9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4764, -1.0327,  0.0940,  ..., -0.4420, -1.6725, -0.0205],\n",
      "         [ 0.9355, -0.7471, -0.1994,  ...,  1.4477, -0.7860, -0.2379],\n",
      "         [-0.2757,  1.0029,  1.6483,  ..., -0.5042, -0.6418,  2.2029],\n",
      "         ...,\n",
      "         [-0.3418,  0.2047, -0.4762,  ..., -0.0797, -1.0133,  1.8495],\n",
      "         [-0.3418,  0.2047, -0.4762,  ..., -0.0797, -1.0133,  1.8495],\n",
      "         [-0.3418,  0.2047, -0.4762,  ..., -0.0797, -1.0133,  1.8495]],\n",
      "\n",
      "        [[-0.4764, -1.0327,  0.0940,  ..., -0.4420, -1.6725, -0.0205],\n",
      "         [ 0.5065,  0.1761, -0.6751,  ..., -1.7936,  0.9502, -0.2954],\n",
      "         [-0.9472, -0.0904,  0.3728,  ...,  0.1303,  1.1657, -0.3195],\n",
      "         ...,\n",
      "         [-0.3418,  0.2047, -0.4762,  ..., -0.0797, -1.0133,  1.8495],\n",
      "         [-0.3418,  0.2047, -0.4762,  ..., -0.0797, -1.0133,  1.8495],\n",
      "         [-0.3418,  0.2047, -0.4762,  ..., -0.0797, -1.0133,  1.8495]]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self,embed_dim,eps=1e-6):\n",
    "        super(RMSNorm,self).__init__()\n",
    "        self.embed_dim=embed_dim\n",
    "        self.eps=eps\n",
    "        self.gamma = nn.Parameter(torch.ones(embed_dim))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return x*self.gamma*torch.rsqrt(x.pow(2).mean(dim=-1,keepdim=True)+self.eps)\n",
    "\n",
    "# 测试RMSNorm\n",
    "rmsnorm_model = RMSNorm(embed_dim)\n",
    "output_rmsnorm = rmsnorm_model(output)\n",
    "print(output_rmsnorm)  # 输出形状应为 (batch_size, sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0a74de",
   "metadata": {},
   "source": [
    "## 4.RoPE\n",
    "主要两步骤\n",
    "- 获取$ m\\theta $ ,计算好precompute_pos_cis\n",
    "- 将pos_cis应用\n",
    "\n",
    "主要公式为：\n",
    "$$\n",
    "\\begin{align}\n",
    "f_q(\\boldsymbol{x}_m, m) &= (\\boldsymbol{W}_q \\boldsymbol{x}_m) e^{im\\theta} \\\\\n",
    "f_k(\\boldsymbol{x}_n, n) &= (\\boldsymbol{W}_k \\boldsymbol{x}_n) e^{in\\theta} \\\\\n",
    "g(\\boldsymbol{x}_m, \\boldsymbol{x}_n, m - n) &= \\text{Re}\\left[ (\\boldsymbol{W}_q \\boldsymbol{x}_m)^* (\\boldsymbol{W}_k \\boldsymbol{x}_n) e^{i(n - m)\\theta} \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$\\Theta=\\left\\{\\theta_i=10000^{-2(i-1)/d},i\\in[1,2,\\ldots,d/2]\\right\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f782278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000+0.0000e+00j,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n",
      "          ...,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n",
      "          1.0000+0.0000e+00j],\n",
      "        [ 0.5403+8.4147e-01j,  1.0000+1.0000e-10j,  1.0000+1.0000e-20j,\n",
      "          ...,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n",
      "          1.0000+0.0000e+00j],\n",
      "        [-0.4161+9.0930e-01j,  1.0000+2.0000e-10j,  1.0000+2.0000e-20j,\n",
      "          ...,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n",
      "          1.0000+0.0000e+00j],\n",
      "        ...,\n",
      "        [ 0.9981+6.1950e-02j,  1.0000+5.0900e-08j,  1.0000+5.0900e-18j,\n",
      "          ...,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n",
      "          1.0000+0.0000e+00j],\n",
      "        [ 0.4871+8.7333e-01j,  1.0000+5.1000e-08j,  1.0000+5.1000e-18j,\n",
      "          ...,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n",
      "          1.0000+0.0000e+00j],\n",
      "        [-0.4717+8.8177e-01j,  1.0000+5.1100e-08j,  1.0000+5.1100e-18j,\n",
      "          ...,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n",
      "          1.0000+0.0000e+00j]])\n",
      "torch.Size([2, 512, 512])\n",
      "torch.Size([2, 512, 256, 2])\n",
      "torch.Size([2, 512, 256, 2])\n"
     ]
    }
   ],
   "source": [
    "def precompute_pos_cis(embed_dim=512,max_seqlen=512,theta=1e5):\n",
    "    freqs= 1/theta**torch.arange(0,embed_dim,2)[:embed_dim//2].float()\n",
    "    m=torch.arange(max_seqlen,device=freqs.device)\n",
    "    freqs= torch.outer(m,freqs).float() #获取了mtheta\n",
    "    pos_cis = torch.polar(torch.ones_like(freqs),freqs) #将mtheta化为极坐标模式\n",
    "    return pos_cis\n",
    "\n",
    "def apply_rotary(xq,xk,pos_cis):\n",
    "    xq_=torch.view_as_complex(xq.float().reshape(*xq.shape[:-1],-1,2))\n",
    "    xk_=torch.view_as_complex(xk.float().reshape(*xk.shape[:-1],-1,2))\n",
    "    #输入的pos_cis一般比xq,xk都要大，需要把pos_cis的形状和xq对齐\n",
    "    #xq一般都是(bs,seqlen,head,head_dim)\n",
    "    def unite_shape(pos_cis,  x):\n",
    "        ndim = x.ndim\n",
    "        assert 0 <= 1 < ndim\n",
    "        assert pos_cis.shape == (x.shape[1],  x.shape[-1]), f\"pos_cis shape {pos_cis.shape} does not match x shape {x.shape}\"\n",
    "        shape = [d if i == 1 or i == ndim - 1 else 1 for i,  d in enumerate(x.shape)]\n",
    "        return pos_cis.view(*shape)\n",
    "    pos_cis = unite_shape(pos_cis, xq_)\n",
    "    xq_ = torch.view_as_real(xq_ * pos_cis).flatten(3)\n",
    "    xk_ = torch.view_as_real(xk_ * pos_cis).flatten(3)\n",
    "    return xq_, xk_\n",
    "\n",
    "#测试一下\n",
    "pos_cis = precompute_pos_cis(embed_dim=embed_dim, max_seqlen=512)\n",
    "print(pos_cis)\n",
    "print(output_rmsnorm.shape)  # 输出形状应为 (batch_size, sequence_length, embed_dim)\n",
    "xq, xk = apply_rotary(output_rmsnorm, output_rmsnorm, pos_cis)\n",
    "print(xq.shape)  # 输出形状应为 (batch_size,\n",
    "print(xk.shape)  # 输出形状应为 (batch_size, sequence_length, embed_dim)\n",
    "# 测试一下apply_rotary的效果\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ce0281",
   "metadata": {},
   "source": [
    "## 5.GQA\n",
    "![LLM-结构](../images/LLM-structure.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b39a070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat_kv是必须要用到的,对齐GQA里的KV与Q的形状\n",
    "def repeat_kv(x,rep_num):\n",
    "    if rep_num == 1:\n",
    "        return x\n",
    "    bs,seqlen,head,head_dim=x.shape\n",
    "    return x[:,:,:,None,:].expand(bs,seqlen,head,rep_num,head_dim).reshape(bs,seqlen,head*rep_num,head_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79c4c1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "class GroupQueryAttention(nn.Module):\n",
    "    def __init__(self,embed_dim,head_num,kv_head_num,dropout=0.1,Flash=False,max_seqlen=512):\n",
    "        super(GroupQueryAttention,self).__init__()\n",
    "        #基本属性\n",
    "        self.embed_dim = embed_dim\n",
    "        self.head_num = head_num\n",
    "        self.kv_head_num = kv_head_num\n",
    "        self.head_dim = embed_dim // head_num\n",
    "        assert embed_dim % head_num == 0, \"embed_dim must be divisible by head_num\"\n",
    "        self.rep_num = head_num // kv_head_num\n",
    "        assert head_num % kv_head_num == 0, \"kv_head_num must be divisible by head_num\"\n",
    "        self.dropout= dropout\n",
    "        self.Flash = hasattr(torch.nn.functional,'scaled_dot_product_attention') and Flash\n",
    "        #网络层\n",
    "        self.q_proj = nn.Linear(embed_dim,self.head_num * self.head_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim,self.kv_head_num * self.head_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim,self.kv_head_num * self.head_dim)\n",
    "        self.o_proj = nn.Linear(self.head_num*self.head_dim,self.embed_dim)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.res_dropout = nn.Dropout(dropout)\n",
    "        #临时\n",
    "        mask = torch.full((1,1, max_seqlen, max_seqlen), float('-1e9'))\n",
    "        mask = torch.tril(mask, diagonal=0)\n",
    "        self.register_buffer('mask', mask)\n",
    "    def forward(self,x,\n",
    "                pos_cis=None,\n",
    "                past_key_value=None,\n",
    "                use_cache=False):\n",
    "        bs,seqlen,embed_dim = x.shape\n",
    "        xq = self.q_proj(x)\n",
    "        xk = self.k_proj(x)\n",
    "        xv = self.v_proj(x)\n",
    "\n",
    "        xq = xq.view(bs, seqlen, self.head_num, self.head_dim)\n",
    "        xk = xk.view(bs, seqlen, self.kv_head_num, self.head_dim)\n",
    "        xv = xv.view(bs, seqlen, self.kv_head_num, self.head_dim)\n",
    "        if pos_cis is None:\n",
    "            pos_cis = precompute_pos_cis(embed_dim=self.head_dim, max_seqlen=seqlen)\n",
    "        xq, xk = apply_rotary(xq, xk, pos_cis)\n",
    "        if past_key_value is not None:\n",
    "            xk = torch.cat([past_key_value[0], xk], dim=1)\n",
    "            xv = torch.cat([past_key_value[1], xv], dim=1)\n",
    "        past_kv = (xk, xv) if use_cache else None\n",
    "        xq = xq.transpose(1,2)\n",
    "        xk = repeat_kv(xk, self.rep_num).transpose(1,2)\n",
    "        xv = repeat_kv(xv, self.rep_num).transpose(1,2)\n",
    "        if self.Flash:\n",
    "            attn_output = F.scaled_dot_product_attention(\n",
    "                xq, xk, xv, attn_mask=None, dropout_p=self.dropout,is_causal=True)\n",
    "        else:\n",
    "            scores = torch.matmul(xq, xk.transpose(-2, -1)) / (math.sqrt(self.head_dim))\n",
    "            scores+= self.mask[:, :, :seqlen, :seqlen]\n",
    "            attn_weights = F.softmax(scores, dim=-1)\n",
    "            attn_weights = self.attn_dropout(attn_weights)\n",
    "            attn_output = torch.matmul(attn_weights, xv)\n",
    "        attn_output = attn_output.transpose(1, 2).reshape(bs, seqlen, -1)\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        attn_output = self.res_dropout(attn_output)\n",
    "        return attn_output, past_kv\n",
    "\n",
    "# 测试GroupQueryAttention\n",
    "embed_dim = 512\n",
    "gqa_model = GroupQueryAttention(embed_dim=embed_dim, head_num=8, kv_head_num=4, dropout=0.1, Flash=False, max_seqlen=512)\n",
    "output_gqa, past_kv = gqa_model(output_rmsnorm, pos_cis=None, use_cache=False)\n",
    "print(output_gqa.shape)  # 输出形状应为 (batch_size, sequence_length, embed_dim)       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5889cf2",
   "metadata": {},
   "source": [
    "## 6.FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c44f5d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,embed_dim,ffn_dim,dropout=0.1):\n",
    "        super(FeedForward,self).__init__()\n",
    "        self.gate = nn.Linear(embed_dim, ffn_dim)\n",
    "        self.up_proj = nn.Linear(embed_dim,ffn_dim)\n",
    "        self.down_proj = nn.Linear(ffn_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self,x):\n",
    "        return self.down_proj(self.dropout(F.silu(self.gate(x)) * self.up_proj(x)))\n",
    "\n",
    "# 测试FeedForward\n",
    "ffn_dim = 2048\n",
    "embed_dim = 512\n",
    "ffn_model = FeedForward(embed_dim=embed_dim, ffn_dim=ffn_dim, dropout=0.1)\n",
    "output_ffn = ffn_model(output_gqa)\n",
    "print(output_ffn.shape)  # 输出形状应为 (batch_size, sequence_length, embed_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c701d3",
   "metadata": {},
   "source": [
    "# 7.Minimind_Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e314b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512, 512])\n",
      "torch.Size([2, 512, 4, 64])\n"
     ]
    }
   ],
   "source": [
    "class Minimind_Block(nn.Module):\n",
    "    def __init__(self,layer_id,embed_dim,head_num,kv_head_num,ffn_dim,dropout=0.1,Flash=False,max_seqlen=512):\n",
    "        super(Minimind_Block,self).__init__()\n",
    "        self.layer_id = layer_id\n",
    "        self.embed_dim = embed_dim\n",
    "        self.head_num = head_num\n",
    "        self.kv_head_num = kv_head_num\n",
    "        self.ffn_dim = ffn_dim\n",
    "        self.dropout = dropout\n",
    "        self.Flash = Flash\n",
    "        self.max_seqlen = max_seqlen\n",
    "        \n",
    "        self.attention = GroupQueryAttention(embed_dim, head_num, kv_head_num, dropout, Flash, max_seqlen)\n",
    "        self.rmsnorm1 = RMSNorm(embed_dim)\n",
    "        self.ffn = FeedForward(embed_dim, ffn_dim, dropout)\n",
    "        self.rmsnorm2 = RMSNorm(embed_dim)\n",
    "    def forward(self, x, pos_cis=None, past_key_value=None, use_cache=False):\n",
    "        # norm1\n",
    "        x = self.rmsnorm1(x)\n",
    "        # attention\n",
    "        attn_output, past_kv = self.attention(x, pos_cis=pos_cis, past_key_value=past_key_value, use_cache=use_cache)\n",
    "        # residual connection\n",
    "        x = x + attn_output\n",
    "        # norm2\n",
    "        x = self.rmsnorm2(x)\n",
    "        # feed forward\n",
    "        ffn_output = self.ffn(x)\n",
    "        # residual connection\n",
    "        x = x + ffn_output\n",
    "        return x, past_kv\n",
    "# 测试Minimind_Block\n",
    "layer_id = 1\n",
    "embed_dim = 512\n",
    "head_num = 8\n",
    "kv_head_num = 4\n",
    "ffn_dim = 2048\n",
    "minimind_block = Minimind_Block(layer_id, embed_dim, head_num, kv_head_num, ffn_dim, dropout=0.1, Flash=False, max_seqlen=512)\n",
    "output_minimind, past_kv = minimind_block(output_ffn, pos_cis=None, use_cache=True)\n",
    "print(output_minimind.shape)  # 输出形状应为 (batch_size, sequence_length, embed_dim)\n",
    "print(past_kv[0].shape)  # 输出形状应为 (batch_size, sequence_length, kv_head_num * head_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ba1e57",
   "metadata": {},
   "source": [
    "## 8.Minimind_Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dde71b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512, 512])\n",
      "6\n",
      "torch.Size([2, 512, 4, 64])\n"
     ]
    }
   ],
   "source": [
    "class Minimind_Dense(nn.Module):\n",
    "    def __init__(self,block_num,embed_dim,head_num,kv_head_num,ffn_dim,dropout=0.1,Flash=False,max_seqlen=512):\n",
    "        super(Minimind_Dense,self).__init__()\n",
    "        self.block_num = block_num\n",
    "        self.embed_dim = embed_dim\n",
    "        self.head_num = head_num\n",
    "        self.kv_head_num = kv_head_num\n",
    "        self.ffn_dim = ffn_dim\n",
    "        self.dropout = dropout\n",
    "        self.Flash = Flash\n",
    "        self.max_seqlen = max_seqlen\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Minimind_Block(layer_id, embed_dim, head_num, kv_head_num, ffn_dim, dropout, Flash, max_seqlen)\n",
    "            for layer_id in range(block_num)\n",
    "        ])\n",
    "    def forward(self, x, pos_cis=None, past_key_values=None, use_cache=False):\n",
    "        if past_key_values is None:\n",
    "            past_key_values = [None] * self.block_num\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x, past_kv = block(x, pos_cis=pos_cis, past_key_value=past_key_values[i], use_cache=use_cache)\n",
    "            if use_cache:\n",
    "                past_key_values[i] = past_kv\n",
    "        return x, past_key_values\n",
    "# 测试Minimind_Dense\n",
    "block_num = 6\n",
    "minimind_dense = Minimind_Dense(block_num, embed_dim, head_num, kv_head_num, ffn_dim, dropout=0.1, Flash=False, max_seqlen=512)\n",
    "output_minimind_dense, past_key_values = minimind_dense(output_minimind, pos_cis=None, use_cache=True)\n",
    "print(output_minimind_dense.shape)  # 输出形状应为 (batch_size, sequence_length, embed_dim)\n",
    "print(len(past_key_values))  # 输出块数\n",
    "print(past_key_values[0][0].shape)  # 输出形状应为 (batch_size, sequence_length, kv_head_num * head_dim\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
