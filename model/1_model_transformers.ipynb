{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8078203",
   "metadata": {},
   "source": [
    "# 利用transformers库封装model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46c4db1",
   "metadata": {},
   "source": [
    "通过上一个文件，我们已经知道了minimind-dense的torch模型了<br>\n",
    "现在我们用transformers库来封装model，方便后续的上传和推理过程<br>\n",
    "要素：\n",
    "- tokenizer\n",
    "- embedding\n",
    "- minimind-block\n",
    "    - RoPE\n",
    "    - RMSNorm\n",
    "    - GQA\n",
    "    - FFN\n",
    "- lm_head\n",
    "- de-tokenizer\n",
    "\n",
    "[关于transformers库](https://cloud.tencent.com/developer/article/2367010)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396e2f40",
   "metadata": {},
   "source": [
    "## 1.tokenizer\n",
    "使用我们之前预训练数据集训练好的tokenizer\n",
    "\n",
    "[关于tokenizer参数的相关设置](https://zhuanlan.zhihu.com/p/341994096) <br>\n",
    "[关于left_padding和right_padding的讨论](https://zhuanlan.zhihu.com/p/646852375)<br>\n",
    "[如何改进增强长文本处理能力](https://zhuanlan.zhihu.com/p/638976034)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98580fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/zyp/miniconda3/envs/minimind/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='./', vocab_size=6400, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|im_start|>', 'eos_token': '<|im_end|>', 'unk_token': '<|endoftext|>', 'pad_token': '<pad>', 'additional_special_tokens': ['<pad>', '<mask>', '<s>', '</s>', '<unk>', '<UNK>', '<EOS>', '<zzy>', '<|s1|>', '<|s2|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<endoftext>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t5: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t6: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t7: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t8: AddedToken(\"<UNK>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t9: AddedToken(\"<EOS>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t10: AddedToken(\"<zzy>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t11: AddedToken(\"<|s1|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t12: AddedToken(\"<|s2|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t6400: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n",
      "6400\n",
      "{'bos_token': '<|im_start|>', 'eos_token': '<|im_end|>', 'unk_token': '<|endoftext|>', 'pad_token': '<pad>', 'additional_special_tokens': ['<pad>', '<mask>', '<s>', '</s>', '<unk>', '<UNK>', '<EOS>', '<zzy>', '<|s1|>', '<|s2|>']}\n",
      "['<|im_start|>', '<|im_end|>', '<|endoftext|>', '<pad>', '<mask>', '<s>', '</s>', '<unk>', '<UNK>', '<EOS>', '<zzy>', '<|s1|>', '<|s2|>']\n",
      "<|im_start|>鉴别一组中文文章的风格和特点，例如官方、口语、文言等。需要提供样例文章才能准确鉴别不同的风格和特点。<|im_end|> <|im_start|>好的，现在帮我查一下今天的天气怎么样?今天的天气依据地区而异。请问你需要我帮你查询哪个地区的天气呢？<|im_end|> <|im_start|>打开闹钟功能，定一个明天早上七点的闹钟。好的，我已经帮您打开闹钟功能，闹钟将在明天早上七点准时响起。<|im_end|> <|im_start|>为以下场景写一句话描述：一个孤独的老人坐在公园长椅上看着远处。一位孤独的老人坐在公园长椅上凝视远方。<|im_end|> <|im_start|>非常感谢你的回答。请告诉我，这些数据是关于什么主题的？这些数据是关于不同年龄段的男女人口比例分布的。<|im_end|> <|im_start|>帮我想一个有趣的标题。这个挺有趣的：\"如何成为一名成功的魔术师\" 调皮的标题往往会吸引读者的注意力。<|im_end|> <|im_start|>回答一个问题，地球的半径是多少？地球的平均半径约为6371公里，这是地球自赤道到两极的距离的平均值。<|im_end|> <|im_start|>识别文本中的语气，并将其分类为喜悦、悲伤、惊异等.文本：“今天是我的生日！”这个文本的语气是喜悦。<|im_end|>\n",
      "<|im_start|>根据输入的内容，编写一个类别标签。这是一篇介绍如何阅读心电图的文章类别标签: 医学/心电图阅读指南<|im_end|> <|im_start|>帮我搜索一下最近的天气情况。当然，我可以帮您搜索最新的天气情况。请问您需要查询哪个城市的天气情况呢？<|im_end|> <|im_start|>帮我讲一个令人开心的笑话。好的，我帮您讲一个关于细菌的笑话。为什么细菌不会上网？因为连接总是断开了！<|im_end|> <|im_start|>现在给我生成一首关于大海的五言诗。碧波万顷月满天，海天相接处天地间。波涛滚滚江山美，海鸟翱翔日月闲。<|im_end|> <|im_start|>谢谢你，这篇文章很有用。不客气，我很高兴能够为您提供帮助。如果您还有其他问题或需求，随时可以对我说。<|im_end|> <|im_start|>你好，我想下载一个视频编辑软件，你有什么推荐吗？您好！当然，有很多选择。您想要免费软件还是愿意付费？<|im_end|> <|im_start|>为什么我的程序不输出正确结果？可能是代码逻辑有误，或者输入数据有误，需要仔细调试代码逻辑和输入数据。<|im_end|> <|im_start|>谢谢你的回答。现在我想知道这场比赛的具体时间和地点。这场比赛的时间是北京时间10月4日，地点是上海。<|im_end|>\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 274])\n",
      "{'input_ids': tensor([[   2, 5203,  723,  ...,    3,    3,    3],\n",
      "        [   2, 4496, 2685,  ...,    3,    3,    3]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "{'input_ids': tensor([[   2, 5203,  723, 2828, 3014,  517, 4187,  297, 2002,  273,  933, 2266,\n",
      "          420,  308, 4691,  308, 5699,  419,  275,  522,  598,  828,  747,  517,\n",
      "         1712, 1544, 5203,  723, 1426, 1798,  297, 2002,  275,    1,  233,    2,\n",
      "          719,  273, 1150, 1895, 1125, 1028, 6139, 4693,   43, 6139, 2034,  472,\n",
      "         2001,  536, 1826,  275, 1785, 2896,  309,  675,  405, 4094, 2668, 2001,\n",
      "         2259, 1357,  433,    1,  233,    2, 3886, 6260, 1795, 1649,  273,  451,\n",
      "          386, 2415, 2707, 3205,  555,  272, 6260, 1795,  275,  719,  273, 4965,\n",
      "          675,  532, 3886, 6260, 1795, 1649,  273, 6260, 1795, 3832, 2415, 2707,\n",
      "         3205,  555,  955,  402,  755,  669,  275,    1,  233,    2,  348,  587,\n",
      "         1876,  643, 2646,  853,  320,  386, 2969,  272, 6255, 5593, 1776,  685,\n",
      "         4862,  412, 6277, 1340,  726,  275, 1730, 2969,  272, 6255, 5593, 1776,\n",
      "          685, 4862,  412, 4935,  919, 1340,  420,  275,    1,  233,    2, 3831,\n",
      "         1181, 1021,  275, 2902,  273,  748,  841, 4127,  673,  968,  272,  433,\n",
      "          748,  841, 4127,  832, 3297,  644,  272, 1257, 1086, 3920,  641,  747,\n",
      "         3924,  272,  275,    1,  233,    2, 4877,  386, 1644, 3544,  275,  569,\n",
      "         6129, 3283, 4582,  878, 1110, 2098, 4278, 2329,  556, 1199,   14,  233,\n",
      "          913, 1838, 3544, 4885,  418, 1931, 2326, 5959,  576,  474,  275,    1,\n",
      "          233,    2, 1021,  386,  533,  273, 4461, 2220, 2760, 4540,  433, 4461,\n",
      "         2391, 2220, 2760, 5213,   34,   31,   35,   29, 3952,  273, 1650, 1292,\n",
      "          422,  429,  110,  663,  392,  742,  819,  272, 2643,  272, 2391, 1105,\n",
      "          275,    1,  233,    2, 1320, 3176, 3870,  273, 4625, 2517,  704, 2842,\n",
      "          308, 6169,  308, 2068, 1826,  419,   26,  528, 1364, 1279,  302, 1317,\n",
      "         3344, 5476, 6373,  272, 3870,  302,  704, 2842,  275,    1],\n",
      "        [   2, 4496, 2685,  273, 4242, 2296, 4281,  275, 4614, 1241,  878, 1578,\n",
      "          573,  523, 1117, 1622, 2296, 4281,   38,  233, 2106,   27,  573,  523,\n",
      "         1117, 1578, 5359,    1,  233,    2, 1895, 2580, 1028, 1265, 2259, 1469,\n",
      "          275, 1436,  273, 1957,  675,  532, 2580, 4654, 1162, 1469,  275, 1785,\n",
      "         3408, 4094, 2668, 1031, 2259, 1469, 1357,  433,    1,  233,    2, 1895,\n",
      "         1276,  386, 2138, 4369, 6053,  678,  275,  719,  273,  309,  675,  532,\n",
      "         1276,  386,  927, 1026, 4044, 6053,  678,  275, 1847, 1026, 4044, 1985,\n",
      "          412,  929,  433,  857, 3031, 2611,  814,  634,  322, 1005,    1,  233,\n",
      "            2, 1150, 1140, 6268,  927,  430,  807,  272, 1411,  764, 1141,  275,\n",
      "         1196,  113, 1811, 1553,  617,  128, 1013, 1043,  470,  273,  807,  470,\n",
      "          592,  982,  726,  470,  410,  545,  275, 1811,  733,  262, 5542, 5542,\n",
      "         2476, 1033,  601,  273,  807, 1661, 1065,  122, 5759,  696, 1013, 2612,\n",
      "          275,    1,  233,    2, 5356,  273, 1036, 3875,  358,  275,  357,  960,\n",
      "          564,  273, 2772, 2929,  907, 3799,  793,  275, 1824, 4922,  533,  491,\n",
      "         1524,  273, 3426,  369,  403,  309,  652,  275,    1,  233,    2, 2899,\n",
      "          273, 1402,  384, 3200,  386, 3311, 1740, 2284,  273, 4688, 1261,  926,\n",
      "          433, 4706, 1005, 1436,  273, 2092,  899,  275,  532, 2014, 6352, 2284,\n",
      "         1605, 4423, 2169, 1288,  433,    1,  233,    2, 1847,  309, 6038,  357,\n",
      "         2186, 1654, 1521,  433, 2746, 3155, 3019,  319, 1247,  273, 1126, 1486,\n",
      "          841,  319, 1247,  273,  522, 6036,  913,  871, 3155, 3019,  297, 1486,\n",
      "          841,  275,    1,  233,    2, 5443, 1021,  275, 1150, 4194, 4943, 1756,\n",
      "         4163, 3147, 2470,  275, 4943, 1756, 2235,  302, 1875,  790, 1134, 1013,\n",
      "           32,  696,  273, 2470,  302, 2763,  275,    1,    3,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer_path= \"./\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "print(tokenizer)\n",
    "print(tokenizer.vocab_size)\n",
    "print(tokenizer.special_tokens_map)\n",
    "print(tokenizer.all_special_tokens)\n",
    "\n",
    "# 写一个虚拟的小的数据集，只有两条数据的集\n",
    "data=[\n",
    "    {'text':'<|im_start|>鉴别一组中文文章的风格和特点，例如官方、口语、文言等。需要提供样例文章才能准确鉴别不同的风格和特点。<|im_end|> <|im_start|>好的，现在帮我查一下今天的天气怎么样?今天的天气依据地区而异。请问你需要我帮你查询哪个地区的天气呢？<|im_end|> <|im_start|>打开闹钟功能，定一个明天早上七点的闹钟。好的，我已经帮您打开闹钟功能，闹钟将在明天早上七点准时响起。<|im_end|> <|im_start|>为以下场景写一句话描述：一个孤独的老人坐在公园长椅上看着远处。一位孤独的老人坐在公园长椅上凝视远方。<|im_end|> <|im_start|>非常感谢你的回答。请告诉我，这些数据是关于什么主题的？这些数据是关于不同年龄段的男女人口比例分布的。<|im_end|> <|im_start|>帮我想一个有趣的标题。这个挺有趣的：\"如何成为一名成功的魔术师\" 调皮的标题往往会吸引读者的注意力。<|im_end|> <|im_start|>回答一个问题，地球的半径是多少？地球的平均半径约为6371公里，这是地球自赤道到两极的距离的平均值。<|im_end|> <|im_start|>识别文本中的语气，并将其分类为喜悦、悲伤、惊异等.文本：“今天是我的生日！”这个文本的语气是喜悦。<|im_end|>'\n",
    "},\n",
    "    {'text':'<|im_start|>根据输入的内容，编写一个类别标签。这是一篇介绍如何阅读心电图的文章类别标签: 医学/心电图阅读指南<|im_end|> <|im_start|>帮我搜索一下最近的天气情况。当然，我可以帮您搜索最新的天气情况。请问您需要查询哪个城市的天气情况呢？<|im_end|> <|im_start|>帮我讲一个令人开心的笑话。好的，我帮您讲一个关于细菌的笑话。为什么细菌不会上网？因为连接总是断开了！<|im_end|> <|im_start|>现在给我生成一首关于大海的五言诗。碧波万顷月满天，海天相接处天地间。波涛滚滚江山美，海鸟翱翔日月闲。<|im_end|> <|im_start|>谢谢你，这篇文章很有用。不客气，我很高兴能够为您提供帮助。如果您还有其他问题或需求，随时可以对我说。<|im_end|> <|im_start|>你好，我想下载一个视频编辑软件，你有什么推荐吗？您好！当然，有很多选择。您想要免费软件还是愿意付费？<|im_end|> <|im_start|>为什么我的程序不输出正确结果？可能是代码逻辑有误，或者输入数据有误，需要仔细调试代码逻辑和输入数据。<|im_end|> <|im_start|>谢谢你的回答。现在我想知道这场比赛的具体时间和地点。这场比赛的时间是北京时间10月4日，地点是上海。<|im_end|>'\n",
    "}\n",
    "]\n",
    "for i in range(2):\n",
    "    print(data[i]['text'])\n",
    "\n",
    "# 接下来将该data的内容利用tokenizer编码\n",
    "input_texts = [item['text'] for item in data]\n",
    "#填充1 固定填充\n",
    "input_ids1 = tokenizer(input_texts, padding='max_length', truncation=True, max_length=512,return_tensors='pt')\n",
    "#填充2 动态填充\n",
    "input_ids2 = tokenizer(input_texts, padding=True, truncation=True, max_length=512,return_tensors='pt')\n",
    "print(input_ids1['input_ids'].shape)\n",
    "print(input_ids2['input_ids'].shape)\n",
    "print(input_ids1)\n",
    "print(input_ids2)\n",
    "# 这样我们就获取到了tokenizer编码后的数据\n",
    "#现在我们深拷贝input_ids1\n",
    "import copy\n",
    "input_ids = copy.deepcopy(input_ids1['input_ids'])\n",
    "# 后续我们采用这个input_ids进行模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3dae6b",
   "metadata": {},
   "source": [
    "## 2.Embedding\n",
    "对应参数 vocab_size,embed_dim<br>\n",
    "这里是 vocab_size=6400 embed_dim=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2b494f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512, 512])\n",
      "tensor([[[ 1.7802,  0.9120, -0.1349,  ...,  0.8308, -0.4473,  0.0710],\n",
      "         [ 1.1532,  0.6046, -1.1357,  ...,  0.7705, -0.1885, -0.4764],\n",
      "         [ 1.8367,  1.0961, -1.1010,  ...,  1.5748,  0.8876,  3.1995],\n",
      "         ...,\n",
      "         [ 0.1466, -0.3770, -1.9389,  ...,  1.7321,  0.2608, -0.6201],\n",
      "         [ 0.1466, -0.3770, -1.9389,  ...,  1.7321,  0.2608, -0.6201],\n",
      "         [ 0.1466, -0.3770, -1.9389,  ...,  1.7321,  0.2608, -0.6201]],\n",
      "\n",
      "        [[ 1.7802,  0.9120, -0.1349,  ...,  0.8308, -0.4473,  0.0710],\n",
      "         [ 1.2645,  0.1701,  0.1653,  ..., -1.1962, -0.0725, -0.0082],\n",
      "         [ 0.5300, -0.6007,  0.2410,  ..., -0.2597, -2.1780, -0.4279],\n",
      "         ...,\n",
      "         [ 0.1466, -0.3770, -1.9389,  ...,  1.7321,  0.2608, -0.6201],\n",
      "         [ 0.1466, -0.3770, -1.9389,  ...,  1.7321,  0.2608, -0.6201],\n",
      "         [ 0.1466, -0.3770, -1.9389,  ...,  1.7321,  0.2608, -0.6201]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 测试版\n",
    "import torch \n",
    "from torch import nn\n",
    "class Embed(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(Embed, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        return self.embedding(input_ids)\n",
    "\n",
    "# 测试\n",
    "embed_dim=512\n",
    "vocab_size=6400\n",
    "embed_model = Embed(vocab_size, embed_dim)\n",
    "output = embed_model(input_ids)\n",
    "print(output.shape)  # 输出形状应为 (batch_size, sequence_length, embed\n",
    "print(output)\n",
    "\n",
    "#待会儿直接用就行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91067a1f",
   "metadata": {},
   "source": [
    "## 3.RMSNorm\n",
    "计算公式为\n",
    "$$ a_i=\\frac{a_i}{RMS(a)+\\epsilon} * \\gamma \\quad where \\quad RMS(a) = \\sqrt{\\frac{1}{n}\\sum^n_{i=1}a^2_i} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6284e9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.8379,  0.9416, -0.1393,  ...,  0.8577, -0.4618,  0.0733],\n",
      "         [ 1.1659,  0.6112, -1.1482,  ...,  0.7789, -0.1906, -0.4816],\n",
      "         [ 1.8901,  1.1280, -1.1330,  ...,  1.6205,  0.9134,  3.2925],\n",
      "         ...,\n",
      "         [ 0.1421, -0.3655, -1.8794,  ...,  1.6789,  0.2528, -0.6011],\n",
      "         [ 0.1421, -0.3655, -1.8794,  ...,  1.6789,  0.2528, -0.6011],\n",
      "         [ 0.1421, -0.3655, -1.8794,  ...,  1.6789,  0.2528, -0.6011]],\n",
      "\n",
      "        [[ 1.8379,  0.9416, -0.1393,  ...,  0.8577, -0.4618,  0.0733],\n",
      "         [ 1.2402,  0.1668,  0.1621,  ..., -1.1732, -0.0711, -0.0080],\n",
      "         [ 0.5466, -0.6194,  0.2485,  ..., -0.2678, -2.2458, -0.4413],\n",
      "         ...,\n",
      "         [ 0.1421, -0.3655, -1.8794,  ...,  1.6789,  0.2528, -0.6011],\n",
      "         [ 0.1421, -0.3655, -1.8794,  ...,  1.6789,  0.2528, -0.6011],\n",
      "         [ 0.1421, -0.3655, -1.8794,  ...,  1.6789,  0.2528, -0.6011]]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self,embed_dim,eps=1e-6):\n",
    "        super(RMSNorm,self).__init__()\n",
    "        self.embed_dim=embed_dim\n",
    "        self.eps=eps\n",
    "        self.gamma = nn.Parameter(torch.ones(embed_dim))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return x*self.gamma*torch.rsqrt(x.pow(2).mean(dim=-1,keepdim=True)+self.eps)\n",
    "\n",
    "# 测试RMSNorm\n",
    "rmsnorm_model = RMSNorm(embed_dim)\n",
    "output_rmsnorm = rmsnorm_model(output)\n",
    "print(output_rmsnorm)  # 输出形状应为 (batch_size, sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0a74de",
   "metadata": {},
   "source": [
    "## 4.RoPE\n",
    "主要两步骤\n",
    "- 获取$ m\\theta $ ,计算好precompute_pos_cis\n",
    "- 将pos_cis应用\n",
    "\n",
    "主要公式为：\n",
    "$$\n",
    "\\begin{align}\n",
    "f_q(\\boldsymbol{x}_m, m) &= (\\boldsymbol{W}_q \\boldsymbol{x}_m) e^{im\\theta} \\\\\n",
    "f_k(\\boldsymbol{x}_n, n) &= (\\boldsymbol{W}_k \\boldsymbol{x}_n) e^{in\\theta} \\\\\n",
    "g(\\boldsymbol{x}_m, \\boldsymbol{x}_n, m - n) &= \\text{Re}\\left[ (\\boldsymbol{W}_q \\boldsymbol{x}_m)^* (\\boldsymbol{W}_k \\boldsymbol{x}_n) e^{i(n - m)\\theta} \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$\\Theta=\\left\\{\\theta_i=10000^{-2(i-1)/d},i\\in[1,2,\\ldots,d/2]\\right\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f782278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000+0.0000e+00j,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n",
      "          ...,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n",
      "          1.0000+0.0000e+00j],\n",
      "        [ 0.5403+8.4147e-01j,  1.0000+1.0000e-10j,  1.0000+1.0000e-20j,\n",
      "          ...,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n",
      "          1.0000+0.0000e+00j],\n",
      "        [-0.4161+9.0930e-01j,  1.0000+2.0000e-10j,  1.0000+2.0000e-20j,\n",
      "          ...,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n",
      "          1.0000+0.0000e+00j],\n",
      "        ...,\n",
      "        [ 0.9981+6.1950e-02j,  1.0000+5.0900e-08j,  1.0000+5.0900e-18j,\n",
      "          ...,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n",
      "          1.0000+0.0000e+00j],\n",
      "        [ 0.4871+8.7333e-01j,  1.0000+5.1000e-08j,  1.0000+5.1000e-18j,\n",
      "          ...,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n",
      "          1.0000+0.0000e+00j],\n",
      "        [-0.4717+8.8177e-01j,  1.0000+5.1100e-08j,  1.0000+5.1100e-18j,\n",
      "          ...,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n",
      "          1.0000+0.0000e+00j]])\n",
      "torch.Size([2, 512, 512])\n",
      "torch.Size([2, 512, 256, 2])\n",
      "torch.Size([2, 512, 256, 2])\n"
     ]
    }
   ],
   "source": [
    "def precompute_pos_cis(embed_dim=512,max_seqlen=512,theta=1e5):\n",
    "    freqs= 1/theta**torch.arange(0,embed_dim,2)[:embed_dim//2].float()\n",
    "    m=torch.arange(max_seqlen,device=freqs.device)\n",
    "    freqs= torch.outer(m,freqs).float() #获取了mtheta\n",
    "    pos_cis = torch.polar(torch.ones_like(freqs),freqs) #将mtheta化为极坐标模式\n",
    "    return pos_cis\n",
    "\n",
    "def apply_rotary(xq,xk,pos_cis):\n",
    "    xq_=torch.view_as_complex(xq.float().reshape(*xq.shape[:-1],-1,2))\n",
    "    xk_=torch.view_as_complex(xk.float().reshape(*xk.shape[:-1],-1,2))\n",
    "    #输入的pos_cis一般比xq,xk都要大，需要把pos_cis的形状和xq对齐\n",
    "    #xq一般都是(bs,seqlen,head,head_dim)\n",
    "    def unite_shape(pos_cis,  x):\n",
    "        ndim = x.ndim\n",
    "        assert 0 <= 1 < ndim\n",
    "        assert pos_cis.shape == (x.shape[1],  x.shape[-1])\n",
    "        shape = [d if i == 1 or i == ndim - 1 else 1 for i,  d in enumerate(x.shape)]\n",
    "        return pos_cis.view(*shape)\n",
    "    pos_cis = unite_shape(pos_cis, xq_)\n",
    "    xq_ = torch.view_as_real(xq_ * pos_cis).flatten(3)\n",
    "    xk_ = torch.view_as_real(xk_ * pos_cis).flatten(3)\n",
    "    return xq_, xk_\n",
    "\n",
    "#测试一下\n",
    "pos_cis = precompute_pos_cis(embed_dim=embed_dim, max_seqlen=512)\n",
    "print(pos_cis)\n",
    "print(output_rmsnorm.shape)  # 输出形状应为 (batch_size, sequence_length, embed_dim)\n",
    "xq, xk = apply_rotary(output_rmsnorm, output_rmsnorm, pos_cis)\n",
    "print(xq.shape)  # 输出形状应为 (batch_size,\n",
    "print(xk.shape)  # 输出形状应为 (batch_size, sequence_length, embed_dim)\n",
    "# 测试一下apply_rotary的效果\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ce0281",
   "metadata": {},
   "source": [
    "## 5.GQA\n",
    "![LLM-结构](../images/LLM-structure.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b39a070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat_kv是必须要用到的,对齐GQA里的KV与Q的形状\n",
    "def repeat_kv(x,rep_num):\n",
    "    if rep_num == 1:\n",
    "        return x\n",
    "    bs,seqlen,head,head_dim=x.shape\n",
    "    return x[:,:,:,None,:].expand(bs,seqlen,head,rep_num,head_dim).reshape(bs,seqlen,head*rep_num,head_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c4c1f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mGroupQueryAttention\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,embed_dim,head_num,kv_head_num,attn_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m(GroupQueryAttention,\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class GroupQueryAttention(nn.Module):\n",
    "    def __init__(self,embed_dim,head_num,kv_head_num,attn_dropout=0.1,max_seqlen=512,Flash=False):\n",
    "        super(GroupQueryAttention,self).__init__()\n",
    "        ## 基本属性\n",
    "        self.embed_dim = embed_dim\n",
    "        self.head_num = head_num\n",
    "        self.kv_head_num = kv_head_num\n",
    "        self.dropout= attn_dropout\n",
    "        self.head_dim = embed_dim // head_num\n",
    "        assert embed_dim % head_num == 0, \"embed_dim must be divisible by head_num\"\n",
    "        self.rep_num = head_num // kv_head_num\n",
    "        assert head_num % kv_head_num == 0, \"head_num must be divisible by kv_head_num\"\n",
    "\n",
    "        self.Flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention') and Flash\n",
    "        ## 网络层\n",
    "        self.q_proj = nn.Linear(embed_dim, self.head_num * self.head_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, self.kv_head_num * self.head_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, self.kv_head_num * self.head_dim)\n",
    "        self.o_proj = nn.Linear(self.head_num * self.head_dim, embed_dim)\n",
    "        self.attn_dropout = nn.Dropout(attn_dropout)\n",
    "        self.res_dropout = nn.Dropout(attn_dropout)\n",
    "\n",
    "        ## 掩码\n",
    "        mask = torch.full((1,1,max_seqlen,max_seqlen), float('-1e9'))\n",
    "        mask=torch.tril(mask, diagonal=0)\n",
    "        self.register_buffer('mask', mask)\n",
    "    \n",
    "    def forward(self,x,\n",
    "                pos_cis=None,\n",
    "                past_key_value=None,\n",
    "                use_cache=False):\n",
    "        bs,seqlen,embed_dim = x.shape\n",
    "        xq = self.q_proj(x).view(bs,seqlen,self.head_num,self.head_dim)\n",
    "        xk = self.k_proj(x).view(bs,seqlen,self.kv_head_num,self.head_dim)\n",
    "        xv = self.v_proj(x).view(bs,seqlen,self.kv_head_num,self.head_dim)\n",
    "        xk = repeat_kv(xk,self.rep_num)\n",
    "        xv = repeat_kv(xv,self.rep_num)\n",
    "\n",
    "        if pos_cis is None:\n",
    "            pos_cis = precompute_pos_cis(embed_dim=self.embed_dim, max_seqlen=seqlen)\n",
    "        xq, xk = apply_rotary(xq, xk, pos_cis)\n",
    "        xq,xk,xv = xq.transpose(1,2), xk.transpose(1,2), xv.transpose(1,2)\n",
    "        if past_key_value is not None:\n",
    "            xk = torch.cat([past_key_value[0], xk], dim=1)\n",
    "            xv = torch.cat([past_key_value[1], xv], dim=1)\n",
    "        past_key_value = (xk, xv) if use_cache else None\n",
    "        if self.Flash:\n",
    "            attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "                xq, xk, xv, dropout_p=self.dropout, is_causal=True)\n",
    "        else:\n",
    "            attn_weights=torch.matmul(xq,xk.transpose(-2,-1))/self.scale\n",
    "            attn_weights=attn_weights+self.mask[:,:,:seqlen,:seqlen]\n",
    "            attn_weights=self.attn_dropout(F.softmax(attn_weights,dim=-1))\n",
    "            attn_output=torch.matmul(attn_weights,xv)\n",
    "            attn_output=self.res_dropout(attn_output)\n",
    "            attn_output=attn_output.transpose(1,2).reshape(bs,seqlen,self.head_num*self.head_dim)\n",
    "        attn_output=self.o_proj(attn_output)\n",
    "        return attn_output,past_key_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44f5d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
