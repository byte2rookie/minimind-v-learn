{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa6350ee",
   "metadata": {},
   "source": [
    "# 搭建我们的model\n",
    "![minimind结构](../images/LLM-structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a382d839",
   "metadata": {},
   "source": [
    "## 搭建的参照：\n",
    "参考llama3和qwen，设计的LLM结构<br>\n",
    "我们按照自底向上的方式，一层一层的复现即可"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ef13b2",
   "metadata": {},
   "source": [
    "### 0-Tokenizer\n",
    "由于tokenizer是输入model之前的一层，因此我们暂时不使用该层，我们假设事先得到了一些输入的word2vec转成的向量<br>\n",
    "假定tokenizer映射的词表大小为6400,和我们训练的tokenizer大小一致"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabb9908",
   "metadata": {},
   "source": [
    "### 1-Embedding\n",
    "输入的embedding层，对应将原始的input_ids压缩为密集的编码形式<br>\n",
    "关于embedding:[什么是embedding层](https://zhuanlan.zhihu.com/p/164502624)，[embedding的前世今生](https://zhuanlan.zhihu.com/p/1916927561000255869)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30e35ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2030, 6373, 3852, 6064, 3649, 4024, 6350, 4501, 5165, 4303],\n",
      "        [2358, 2897, 5469, 1784, 4995, 4043,  932, 6109,  556, 1158]])\n",
      "torch.Size([2, 10, 512])\n",
      "tensor([[[ 0.2758,  0.4148,  1.1885,  ..., -0.6679,  0.1490,  0.5232],\n",
      "         [-2.6458, -0.8047, -0.6296,  ..., -0.6315,  0.1417,  0.6417],\n",
      "         [ 1.3939,  1.1044, -0.7613,  ..., -1.7296, -0.4728, -0.7427],\n",
      "         ...,\n",
      "         [ 2.5562, -0.4796,  1.1946,  ..., -0.6231,  1.1949,  1.8410],\n",
      "         [ 0.2589,  0.8934, -0.5616,  ...,  0.6162, -0.1266,  2.2751],\n",
      "         [-0.7783,  0.4408,  0.5889,  ...,  0.0514, -1.5111,  1.1941]],\n",
      "\n",
      "        [[-1.1211,  0.5346, -0.3174,  ..., -1.5841, -1.0099, -0.6460],\n",
      "         [ 0.4476, -0.7232,  0.0389,  ...,  4.0461, -1.1284,  0.4496],\n",
      "         [-1.0826, -2.5474,  0.2178,  ..., -1.2754, -0.4207,  0.0776],\n",
      "         ...,\n",
      "         [-0.4938, -1.9396, -0.2887,  ..., -2.2465, -0.6882,  1.4206],\n",
      "         [-0.6993, -1.3383, -0.6565,  ..., -0.1087,  1.3399,  0.8225],\n",
      "         [-0.3235, -1.1498, -0.3859,  ..., -0.1344, -1.2051,  1.3635]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "input_ids=torch.randint(0,6400,(2,10))  # 假设输入的input_ids为2个句子，每个句子10个token\n",
    "print(input_ids)  # 输出形状应为(2, 10)\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_dim):\n",
    "        super(Embedding,self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,embed_dim)\n",
    "    def forward(self,input_ids):\n",
    "        return self.embedding(input_ids)\n",
    "# Example usage\n",
    "vocab_size = 6400  # 假设词表大小为6400\n",
    "embed_dim = 512  # 假设嵌入维度为512\n",
    "embedding_layer = Embedding(vocab_size, embed_dim)\n",
    "embedded_input = embedding_layer(input_ids)\n",
    "print(embedded_input.shape)  # 输出形状应为(2, 10, 512)\n",
    "print(embedded_input)  # 输出嵌入后的向量\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f554b85b",
   "metadata": {},
   "source": [
    "### 2-MiniMind-Block\n",
    "Minimind-Block 也就是transformer-block<br>\n",
    "其主要作用是通过transformer来学习并提取input_ids的有效feature<br>\n",
    "minimind-block主要组件为\n",
    "- RMSNorm\n",
    "- GQA(attention)\n",
    "- RoPE(位置编码)\n",
    "- FFN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11e1af0",
   "metadata": {},
   "source": [
    "#### 2.1 RMSNorm\n",
    "RMSNorm是LLama模型中提出的一种新颖的归一化方式[BatchNorm,LayerNorm,RMSNorm介绍](https://blog.csdn.net/wxc971231/article/details/139925707)<br>\n",
    "[苏剑林-关于norm的放置,pre or post?](https://kexue.fm/archives/9009)\n",
    "- **LayerNorm**\n",
    "    主要计算公式为：\n",
    "    $$\\frac{x-E(x)}{\\sqrt{Var(x)+\\epsilon}} * \\beta$$\n",
    "    其涉及到计算Ex和Varx,计算量偏大\n",
    "- **RMSNorm**\n",
    "    主要计算公式为：\n",
    "    $$a_i=\\frac{a_i}{RMS(a)+\\epsilon} * \\gamma \\quad where \\quad RMS(a) = \\sqrt{\\frac{1}{n}\\sum^n_{i=1}a^2_i}$$\n",
    "    RMSNorm的主要优点是降低了计算量，并且提高了计算的稳定程度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95032383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.3649, -1.8068],\n",
      "         [-0.5323, -0.3894]],\n",
      "\n",
      "        [[ 0.8681,  0.3576],\n",
      "         [ 1.2855,  0.3212]]])\n",
      "tensor([[[-0.8524, -1.1284],\n",
      "         [-1.1413, -0.8351]],\n",
      "\n",
      "        [[ 1.3076,  0.5386],\n",
      "         [ 1.3720,  0.3428]]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self,embed_dim,eps=1e-6):\n",
    "        super(RMSNorm,self).__init__()\n",
    "        self.embed_dim=embed_dim\n",
    "        self.eps=eps\n",
    "        self.weight=nn.Parameter(torch.ones(embed_dim))\n",
    "    def forward(self,X):\n",
    "        \"\"\"\n",
    "        X= (batch_size, seq_len, embed_dim)\n",
    "        \"\"\"\n",
    "        return X*self.weight*torch.rsqrt(torch.mean(X.pow(2),dim=-1,keepdim=True)+self.eps)\n",
    "# Example usage\n",
    "embed_dim=2\n",
    "X= torch.randn(2, 2, embed_dim)  # 假设输入的X为2个句子，每个句子10个token，每个token的嵌入维度为512\n",
    "rmsnorm_layer = RMSNorm(embed_dim)\n",
    "normalized_output = rmsnorm_layer(X)\n",
    "print(X)\n",
    "print(normalized_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9778e504",
   "metadata": {},
   "source": [
    "#### 2.2 RoPE旋转编码\n",
    "##### Rotary Position Embedding, RoPE\n",
    "\n",
    "旋转位置编码是一种能将相对位置信息集成到 self-attention 中, 进而提升 transformer 架构性能的位置编码方式, 和绝对位置编码相比, RoPE 具有很好的外推性, 是目前的主流位置编码方式.\n",
    "\n",
    "外推性的解释, 通俗来说就是训练的时候限制了 512 的上下文长度，那么推理时如果面对超过该长度的文本，LLM 可能无法正确处理.\n",
    "\n",
    "- **绝对位置编码**\n",
    "\n",
    "绝对位置编码是早期 Transformer 架构采用的绝对位置编码方案，及那个每个位置映射为固定的向量表示.\n",
    "\n",
    "$$f_{t:t\\in\\{q,k,v\\}}(\\boldsymbol{x}_i,i)=\\boldsymbol{W}_{t:t\\in\\{q,k,v\\}}(\\boldsymbol{x}_i+\\boldsymbol{p}_i)$$\n",
    "\n",
    "其中编码向量 $p_i$ 的计算使用如下公式：\n",
    "\n",
    "$$\\boldsymbol{p}_{i,2t}=\\sin\\left(k/1000^{2t/d}\\right), \\boldsymbol{p}_{i,2t+1}=\\cos\\left(k/1000^{2t/d}\\right)$$\n",
    "\n",
    "正如其名，绝对位置编码只考虑了输入序列中的绝对位置关系，对于 token 之间的相对信息则没有纳入考虑.\n",
    "\n",
    "- **旋转位置编码**\n",
    "\n",
    "假定 query 和 key 的内积操作可以被函数 g 表示，该函数 g 的输入是词嵌入向量 $x_m, x_n$ 和它们之间的相对位置 $m-n$:\n",
    "\n",
    "$$<f_q(x_m ,m), f_k(x_n, n)>=g(x_m, x_n, m, n)$$\n",
    "\n",
    "旋转位置编码就是找到一个使上式成立的位置编码方式. \n",
    "\n",
    "出于认识的目的，我们省略复杂的数学推导，直接看 RoPE 的的结论：\n",
    "\n",
    "存在这样一个正交矩阵：\n",
    "\n",
    "$$\\boldsymbol{R}_{\\Theta,m}^d=\\underbrace{\\begin{pmatrix}\\cos m\\theta_0&-\\sin m\\theta_0&0&0&\\cdots&0&0\\\\\\sin m\\theta_0&\\cos m\\theta_0&0&0&\\cdots&0&0\\\\0&0&\\cos m\\theta_1&-\\sin m\\theta_1&\\cdots&0&0\\\\0&0&\\sin m\\theta_1&\\cos m\\theta_1&\\cdots&0&0\\\\\\vdots&\\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\0&0&0&0&\\cdots&\\cos m\\theta_{d/2-1}&-\\sin m\\theta_{d/2-1}&-\\sin m\\theta_{d/2-1}\\end{pmatrix}}_{\\boldsymbol{W}_m}$$\n",
    "\n",
    "其中，$\\Theta=\\left\\{\\theta_i=10000^{-2(i-1)/d},i\\in[1,2,\\ldots,d/2]\\right\\}$\n",
    "\n",
    "我们可以将 query 和 key 的内积操作转换为与原始向量 $x$ 相关的以下等价形式：\n",
    "\n",
    "$$\n",
    "\\boldsymbol{q}_m^\\mathbf{T}\\boldsymbol{k}_n=\\left(\\boldsymbol{R}_{\\Theta,m}^d\\boldsymbol{W}_q\\boldsymbol{x}_m\\right)^\\mathbf{T}\\left(\\boldsymbol{R}_{\\Theta,n}^d\\boldsymbol{W}_k\\boldsymbol{x}_n\\right)=\\boldsymbol{x}_m^\\mathbf{T}\\boldsymbol{W}_q\\boldsymbol{R}_{\\Theta,n-m}^d\\boldsymbol{W}_k\\boldsymbol{x}_n\n",
    "$$\n",
    "\n",
    "其中， $\\boldsymbol{R}_{\\Theta,n-m}^d=\\left(\\boldsymbol{R}_{\\Theta,m}^d\\right)^\\mathbf{T}\\boldsymbol{R}_{\\Theta,n}^d$.\n",
    "\n",
    "由于 $\\boldsymbol{R}_{\\Theta,m}^d$ 的稀疏性，直接使用矩阵乘法会浪费算力，因此代码中采用下述方式实现：\n",
    "\n",
    "$$\\boldsymbol{R}_{\\Theta,m}^{d}\\boldsymbol{x}=\\begin{pmatrix}x_{0}\\\\x_{1}\\\\x_{2}\\\\x_{3}\\\\\\vdots\\\\x_{d-2}\\\\x_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos m\\theta_{0}\\\\\\cos m\\theta_{0}\\\\\\cos m\\theta_{1}\\\\\\cos m\\theta_{1}\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos m\\theta_{d/2-1}\\end{pmatrix}+\\begin{pmatrix}-x_{1}\\\\x_{0}\\\\-x_{3}\\\\x_{2}\\\\\\vdots\\\\-x_{d-1}\\\\x_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin m\\theta_{0}\\\\\\sin m\\theta_{0}\\\\\\sin m\\theta_{1}\\\\\\sin m\\theta_{1}\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin m\\theta_{d/2-1}\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c81bae5",
   "metadata": {},
   "source": [
    "简而言之，RoPE就是用绝对编码的形式，表示出相对编码的关系，这样同时具有了绝对编码的简洁和相对编码的位置信息泛化性<br>\n",
    "此处的ROPE的实现主要参考的是LLama的RoPE实现\n",
    "[LLAMA实现](https://blog.csdn.net/m0_55846238/article/details/145728695)<br>\n",
    "对旋转编码理解困难，可以参考[无痛理解RoPE](https://zhuanlan.zhihu.com/p/8306958113)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4d7b05",
   "metadata": {},
   "source": [
    "具体而言，旋转编码有两种实现，一种是Qwen的实现，一种是LLama的实现<br>\n",
    "我们首先试着参考llama的实现来理解,结合[LLAMA实现](https://blog.csdn.net/m0_55846238/article/details/145728695)<br>\n",
    "llama的实现主要通过\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f_q(\\boldsymbol{x}_m, m) &= (\\boldsymbol{W}_q \\boldsymbol{x}_m) e^{im\\theta} \\\\\n",
    "f_k(\\boldsymbol{x}_n, n) &= (\\boldsymbol{W}_k \\boldsymbol{x}_n) e^{in\\theta} \\\\\n",
    "g(\\boldsymbol{x}_m, \\boldsymbol{x}_n, m - n) &= \\text{Re}\\left[ (\\boldsymbol{W}_q \\boldsymbol{x}_m)^* (\\boldsymbol{W}_k \\boldsymbol{x}_n) e^{i(n - m)\\theta} \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "这一原理实现的，主要流程为：计算所有的$\\theta_i$，再利用$\\cos m\\theta_i$的表格来依次进行q,k,v的位置信息的转化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee1f760f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n",
      "torch.Size([3])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 4, 2])\n",
      "torch.Size([2, 3, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "def precompute_pos_cis(embed_dim,seqlen,theta=1e5):\n",
    "    \"\"\"\n",
    "    先计算出所有的theta\n",
    "    embed_dim: embedding的维度\n",
    "    seqlen: 序列长度\n",
    "    theta: theta的值，默认1e5\n",
    "    embed_dim和seqlen都是用于计算cis的\n",
    "    \"\"\"\n",
    "    freqs = 1/(theta**(torch.arange(0,embed_dim,2)[:embed_dim//2].float())/embed_dim)\n",
    "    print(freqs.shape)\n",
    "    m=torch.arange(seqlen,device=freqs.device)\n",
    "    print(m.shape)\n",
    "    freqs=torch.outer(m,freqs).float() #计算mtheta\n",
    "    pos_cis=torch.polar(torch.ones_like(freqs),freqs) # polor是将实数转为复数，也就是把freqs极坐标化了\n",
    "    print(pos_cis.shape)\n",
    "    return pos_cis\n",
    "\n",
    "def apply_rotary(xq,xk,pos_cis):\n",
    "    \"\"\"\n",
    "    xq: (batch_size, seq_len, head_num,embed_dim)\n",
    "    xk: (batch_size, seq_len, head_num,embed_dim)\n",
    "    pos_cis: (seqlen, embed_dim//2)\n",
    "    \"\"\"\n",
    "    ## pos_cis的形状一般是比xq,xk要大一些的，所以可能会遇到需要对齐的情况\n",
    "    xq_=torch.view_as_complex(xq.float().reshape(*xq.shape[:-1],-1,2))\n",
    "    xk_=torch.view_as_complex(xk.float().reshape(*xk.shape[:-1],-1,2))\n",
    "    def unite_shape(pos_cis,x):\n",
    "        ndim=x.ndim\n",
    "        assert ndim>=1\n",
    "        # 确保x形状为[bs,seqlen,n_heads,embed_dim]\n",
    "        assert pos_cis.shape==(x.shape[1],x.shape[-1])\n",
    "        shape = [d if i == 1 or i == ndim - 1 else 1 for i,  d in enumerate(x.shape)]\n",
    "        return pos_cis.view(*shape)\n",
    "    pos_cis=unite_shape(pos_cis,xq_)\n",
    "    xq_out= torch.view_as_real(xq_ * pos_cis).flatten(3)\n",
    "    xk_out= torch.view_as_real(xk_ * pos_cis).flatten(3)\n",
    "    return xq_out,xk_out\n",
    "\n",
    "# Example usage\n",
    "inputs=torch.randn(2,3,4,2) #模拟3个heads的情况\n",
    "pos_cis=precompute_pos_cis(2,3)\n",
    "xq,xk=apply_rotary(inputs,inputs,pos_cis)\n",
    "print(inputs.shape)  # 输出形状应为(2, 3, 4, 2)\n",
    "print(xq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcae72e1",
   "metadata": {},
   "source": [
    "#### 2.3 GQA&MHA\n",
    "这里就进入到我们熟悉的注意力环节了<br>\n",
    "GQA就是把MHA弄成了多个Query对应一个Key来进行的<br>\n",
    "回顾block的样子![llm](../images/LLM-structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf2e0db",
   "metadata": {},
   "source": [
    "首先是GQA中必须的辅助函数，将Q和KV对齐维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ecbcdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "def repeat_kv(x,rep_num):\n",
    "    \"\"\"\n",
    "    将x对应的k,v重复rep_num次,用来对其Q\n",
    "    \"\"\"\n",
    "    if rep_num == 1:\n",
    "        return X\n",
    "    bs,seqlen,head_num,head_dim= x.shape\n",
    "    return x[:,:,:,None,:].expand(bs,seqlen,head_num,rep_num,head_dim).reshape(bs,seqlen,head_num*rep_num,head_dim)\n",
    "\n",
    "# Example usage\n",
    "X=torch.randn(2,3,4,2) #模拟3个heads的情况\n",
    "rep_num=2\n",
    "repeated_X=repeat_kv(X,rep_num)\n",
    "print(repeated_X.shape)  # 输出形状应为(2, 3, 8, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "31dbf465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([2, 10, 512])\n",
      "torch.Size([2, 10, 8, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import math\n",
    "class GroupQueryAttention(nn.Module):\n",
    "    def __init__(self,embed_dim,head_num,kv_head_num,dropout=0.1,Flash=False,max_seqlen=1024):\n",
    "        super(GroupQueryAttention,self).__init__()\n",
    "        ## 属性\n",
    "        self.embed_dim=embed_dim\n",
    "        self.head_num=head_num\n",
    "        self.kv_head_num=kv_head_num\n",
    "        self.head_dim=embed_dim//head_num\n",
    "        self.kv_head_dim=self.head_dim\n",
    "        self.rep_num=self.head_num//self.kv_head_num\n",
    "        assert self.rep_num * self.kv_head_num == self.head_num, \"head_num must be divisible by kv_head_num\"\n",
    "        self.dropout=dropout\n",
    "        self.Flash=hasattr(F,\"scaled_dot_product_attention\") and Flash\n",
    "        assert embed_dim == head_num * self.head_dim, \"embed_dim must be divisible by head_num\"\n",
    "        self.scale= math.sqrt(self.head_dim)\n",
    "        ## 网络\n",
    "        self.q_proj=nn.Linear(embed_dim,self.head_dim*self.head_num)\n",
    "        self.k_proj=nn.Linear(embed_dim,self.kv_head_num*self.head_dim)\n",
    "        self.v_proj=nn.Linear(embed_dim,self.kv_head_num*self.head_dim)\n",
    "        self.o_proj=nn.Linear(self.head_num*self.head_dim,self.embed_dim)\n",
    "        self.attn_dropout=nn.Dropout(dropout)\n",
    "        self.res_dropout=nn.Dropout(dropout)\n",
    "        ## 临时性质参数，如mask和pos_cis\n",
    "        ## 因果掩码初始化\n",
    "        ## mask形状为(bs, head_num,seqlen, seqlen)\n",
    "        mask=torch.full((1,1,max_seqlen,max_seqlen),float(\"-1e9\"))\n",
    "        mask=torch.tril(mask,diagonal=0)\n",
    "        self.register_buffer(\"mask\",mask)\n",
    "    \n",
    "    def forward(self,X,\n",
    "                pos_cis=None,\n",
    "                use_cache=False,\n",
    "                past_key_value=None,\n",
    "                ):\n",
    "        bs,seqlen,embed_dim=X.shape\n",
    "        xq=self.q_proj(X).view(bs,seqlen,self.head_num,self.head_dim)\n",
    "        xk=self.k_proj(X).view(bs,seqlen,self.kv_head_num,self.head_dim)\n",
    "        xv=self.v_proj(X).view(bs,seqlen,self.kv_head_num,self.head_dim)\n",
    "        xk=repeat_kv(xk,self.rep_num)\n",
    "        xv=repeat_kv(xv,self.rep_num)\n",
    "\n",
    "        if pos_cis is None:\n",
    "            pos_cis=precompute_pos_cis(self.head_dim,seqlen)\n",
    "        xq,xk=apply_rotary(xq,xk,pos_cis)\n",
    "        if  past_key_value is not None:\n",
    "            xk=torch.cat([past_key_value[0],xk],dim=1)\n",
    "            xv=torch.cat([past_key_value[1],xv],dim=1)\n",
    "        past_key_value=(xk,xv) if use_cache else None\n",
    "\n",
    "        xq,xk,xv= xq.transpose(1,2),xk.transpose(1,2),xv.transpose(1,2)\n",
    "\n",
    "        if self.Flash:\n",
    "            attn_output,attn_weights= F.scaled_dot_product_attention(\n",
    "                xq,xk,xv,dropout_p=self.dropout,is_causal=True\n",
    "            )\n",
    "        else:\n",
    "            attn_weights=torch.matmul(xq,xk.transpose(-2,-1))/self.scale\n",
    "            attn_weights=attn_weights+self.mask[:,:,:seqlen,:seqlen]\n",
    "            attn_weights=self.attn_dropout(F.softmax(attn_weights,dim=-1))\n",
    "            attn_output=torch.matmul(attn_weights,xv)\n",
    "            attn_output=self.res_dropout(attn_output)\n",
    "            attn_output=attn_output.transpose(1,2).reshape(bs,seqlen,self.head_num*self.head_dim)\n",
    "        attn_output=self.o_proj(attn_output)\n",
    "        return attn_output,past_key_value\n",
    "# Example usage\n",
    "embed_dim = 512  # 假设嵌入维度为512\n",
    "head_num = 8  # 假设头数为8\n",
    "kv_head_num = 4  # 假设键值头数为4\n",
    "dropout = 0.1  # 假设dropout率为0.1\n",
    "max_seqlen = 1024  # 假设最大序列长度为102\n",
    "\n",
    "gqa=GroupQueryAttention(embed_dim, head_num, kv_head_num, dropout, max_seqlen=max_seqlen)\n",
    "output, past_key_value = gqa(embedded_input,use_cache=True)\n",
    "print(output.shape)  # 输出形状应为(2, 10, 512)\n",
    "print(past_key_value[0].shape)  # 输出past_key_value的形状应为((2, 10, 4, 128), (2, 10, 4, 128))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93145038",
   "metadata": {},
   "source": [
    "#### 2.4 FFN\n",
    "FFN的选取是MOE和Dense的主要区别，MoE是多个FFN\n",
    "由于我们这个版本仅仅只是torch的原生版本，暂时不使用transformers库包装，所以我们采用dense模型来进行演示，MoE将在后续上传"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "672c362b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4633, 0.2942, 0.3766, 0.3820, 0.4669, 0.8147, 0.9017, 0.5830,\n",
      "          0.8753, 0.4055]],\n",
      "\n",
      "        [[0.4591, 0.6238, 0.5254, 0.7336, 0.1214, 0.4545, 0.3634, 0.3871,\n",
      "          0.8580, 0.5933]]])\n",
      "tensor([[[-0.5581,  0.0881,  0.0698,  0.3024,  0.3239, -0.0000,  0.1254,\n",
      "           0.3087,  0.1068, -0.0631]],\n",
      "\n",
      "        [[-0.3845,  0.2091, -0.0874,  0.5110,  0.2865, -0.0427,  0.1898,\n",
      "           0.0488,  0.0000, -0.0000]]], grad_fn=<MulBackward0>)\n",
      "torch.Size([2, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,embed_dim,ffn_dim,dropout=0.1):\n",
    "        super(FeedForward,self).__init__()\n",
    "        ## 基本属性\n",
    "        self.embed_dim=embed_dim\n",
    "        self.ffn_dim=ffn_dim\n",
    "        self.dropout=dropout\n",
    "        ## 网络\n",
    "        self.gate=nn.Linear(self.embed_dim,self.ffn_dim)\n",
    "        self.up_proj=nn.Linear(self.embed_dim,self.ffn_dim)\n",
    "        self.down_proj=nn.Linear(self.ffn_dim,self.embed_dim)\n",
    "        self.res_dropout=nn.Dropout(self.dropout)\n",
    "    def forward(self,X):\n",
    "        res= self.gate(X)\n",
    "        res= F.silu(res) + self.up_proj(X)\n",
    "        res= self.down_proj(res)\n",
    "        res= self.res_dropout(res)\n",
    "        return res\n",
    "# Example usage\n",
    "embed_dim = 10  # 假设嵌入维度为512\n",
    "ffn_dim = 2048  # 假设前馈网络维度为204\n",
    "dropout = 0.1  # 假设dropout率为0.1\n",
    "ffn_layer = FeedForward(embed_dim, ffn_dim, dropout)\n",
    "input_ids=torch.rand(2, 1, embed_dim)  # 假设输入的X为2个句子，每个句子10个token，每个token的嵌入维度为512\n",
    "ffn_output = ffn_layer(input_ids)\n",
    "print(input_ids)  # 输出形状应为(2, 10, 10)\n",
    "print(ffn_output)  # 输出嵌入后的向量\n",
    "print(ffn_output.shape)  # 输出形状应为(2, 10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c783ab",
   "metadata": {},
   "source": [
    "### 3.搭建Block\n",
    "![llm](../images/LLM-structure.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e89a3da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 32])\n",
      "torch.Size([32])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "class MiniMind_Block_Dense(nn.Module):\n",
    "    def __init__(self,block_id,embed_dim,head_num,kv_head_num,ffn_dim,attn_dropout=0.1,ffn_dropout=0.1,Flash=False,max_seqlen=1024):\n",
    "        super(MiniMind_Block_Dense,self).__init__()\n",
    "        ## 基本属性\n",
    "        self.block_id=block_id\n",
    "        self.embed_dim=embed_dim\n",
    "        self.head_num=head_num\n",
    "        self.kv_head_num=kv_head_num\n",
    "        self.head_dim=embed_dim//head_num\n",
    "        self.rep_num=self.head_num//self.kv_head_num\n",
    "        assert self.rep_num * self.kv_head_num == self.head_num, \"head_num must be divisible by kv_head_num\"\n",
    "        assert embed_dim == head_num * self.head_dim,\"embed_dim must be divisible by head_num\"\n",
    "\n",
    "        self.ffn_dim=ffn_dim\n",
    "        self.attn_dropout=attn_dropout\n",
    "        self.ffn_dropout=ffn_dropout\n",
    "        self.Flash=Flash\n",
    "        self.max_seqlen=max_seqlen\n",
    "\n",
    "        self.attn=GroupQueryAttention(embed_dim,head_num,kv_head_num,attn_dropout,Flash=Flash,max_seqlen=max_seqlen)\n",
    "        self.ffn=FeedForward(embed_dim,ffn_dim,ffn_dropout)\n",
    "        self.norm1=RMSNorm(embed_dim)\n",
    "        self.norm2=RMSNorm(embed_dim)\n",
    "\n",
    "        ## 临时参数\n",
    "        pos_cis=precompute_pos_cis(self.head_dim,self.max_seqlen)\n",
    "        self.register_buffer(\"pos_cis\",pos_cis)\n",
    "\n",
    "    def forward(self,X,\n",
    "                pos_cis=None,\n",
    "                use_cache=False,\n",
    "                past_key_value=None,\n",
    "                ):\n",
    "        \"\"\"\n",
    "        X: (bs,seqlen,embed_dim)\n",
    "        pos_cis: (seqlen, embed_dim//2)\n",
    "        use_cache: 是否使用缓存\n",
    "        past_key_value: 缓存的键值对\n",
    "        \"\"\"\n",
    "        # 1. Attention\n",
    "        X=self.norm1(X)\n",
    "        attn_output,past_key_value=self.attn(X,pos_cis=pos_cis,use_cache=use_cache,past_key_value=past_key_value)\n",
    "        X=X+attn_output\n",
    "\n",
    "        # 2. FFN\n",
    "        X=self.norm2(X)\n",
    "        ffn_output=self.ffn(X)\n",
    "        X=X+ffn_output\n",
    "\n",
    "        return X,past_key_value\n",
    "\n",
    "# Example usage\n",
    "embed_dim = 512  # 假设嵌入维度为512\n",
    "head_num = 8  # 假设头数为8\n",
    "kv_head_num = 4  # 假设键值头数为4\n",
    "ffn_dim = 2048  # 假设前馈网络维度为204\n",
    "attn_dropout = 0.1  # 假设注意力dropout率为\n",
    "ffn_dropout = 0.1  # 假设前馈网络dropout率为0.1\n",
    "max_seqlen = 1024  # 假设最大序列长度为102\n",
    "block = MiniMind_Block_Dense(0,embed_dim, head_num, kv_head_num, ffn_dim, attn_dropout, ffn_dropout, max_seqlen=max_seqlen)\n",
    "input_ids = torch.rand(2, 10, embed_dim)  # 假设输入的X为2个句子，每个句子10个token，每个token的嵌入维度为512\n",
    "output, past_key_value = block(input_ids, use_cache=True)\n",
    "print(output.shape)  # 输出形状应为(2, 10,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3603a0c4",
   "metadata": {},
   "source": [
    "### 4.MiniMind_Dense\n",
    "拼成一个Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "87129ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([2, 10, 128000])\n",
      "tensor([[[9.0723e-06, 8.2376e-06, 5.8199e-06,  ..., 1.2135e-05,\n",
      "          1.2126e-05, 5.1952e-06],\n",
      "         [8.5588e-06, 5.5778e-06, 5.2231e-06,  ..., 1.0262e-05,\n",
      "          8.8919e-06, 2.3128e-06],\n",
      "         [7.4519e-06, 8.5862e-06, 5.8931e-06,  ..., 8.3159e-06,\n",
      "          1.3823e-05, 2.9680e-06],\n",
      "         ...,\n",
      "         [6.0458e-06, 8.8248e-06, 1.8993e-06,  ..., 4.6784e-06,\n",
      "          4.5175e-06, 3.4298e-06],\n",
      "         [5.5551e-06, 6.6855e-06, 1.9187e-06,  ..., 6.8412e-06,\n",
      "          1.4108e-05, 9.5415e-07],\n",
      "         [5.0711e-06, 6.9334e-06, 6.6763e-06,  ..., 7.1596e-06,\n",
      "          1.9006e-05, 6.2098e-06]],\n",
      "\n",
      "        [[1.5013e-05, 1.1784e-05, 4.2762e-06,  ..., 6.9153e-06,\n",
      "          1.7145e-05, 6.4776e-06],\n",
      "         [2.4325e-05, 7.8503e-06, 7.0745e-06,  ..., 6.1950e-06,\n",
      "          1.5598e-05, 6.0169e-06],\n",
      "         [3.1803e-05, 5.5437e-06, 1.0230e-05,  ..., 6.6121e-06,\n",
      "          1.0590e-05, 5.1878e-06],\n",
      "         ...,\n",
      "         [1.7922e-05, 1.3191e-05, 7.0310e-06,  ..., 5.2593e-06,\n",
      "          9.4760e-06, 2.4501e-06],\n",
      "         [2.5947e-05, 1.0317e-05, 7.7489e-06,  ..., 4.0314e-06,\n",
      "          1.5803e-05, 8.0698e-06],\n",
      "         [1.0015e-05, 4.3934e-06, 5.9023e-06,  ..., 5.1317e-06,\n",
      "          1.3611e-05, 3.8699e-06]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class MinimindLM_Dense(nn.Module):\n",
    "    def __init__(self,block_num,vocab_size,embed_dim,head_num,kv_head_num,ffn_dim,attn_dropout=0.1,ffn_dropout=0.1,Flash=False,max_seqlen=1024):\n",
    "        super(MinimindLM_Dense,self).__init__()\n",
    "        self.block_num=block_num\n",
    "        self.vocab_size=vocab_size\n",
    "        self.embed_dim=embed_dim\n",
    "        self.head_num=head_num\n",
    "        self.kv_head_num=kv_head_num\n",
    "        self.ffn_dim=ffn_dim\n",
    "        self.attn_dropout=attn_dropout\n",
    "        self.ffn_dropout=ffn_dropout\n",
    "        self.Flash=Flash\n",
    "        self.max_seqlen=max_seqlen\n",
    "\n",
    "        self.embedding=Embedding(self.vocab_size,embed_dim)\n",
    "        self.blocks=nn.ModuleList([MiniMind_Block_Dense(i,embed_dim,head_num,kv_head_num,ffn_dim,attn_dropout,ffn_dropout,Flash,max_seqlen) for i in range(block_num)])\n",
    "        self.norm=RMSNorm(embed_dim)\n",
    "        self.lm_output=nn.Linear(embed_dim,vocab_size)\n",
    "    def forward(self,input_ids):\n",
    "        bs,seqlen = input_ids.shape\n",
    "        X=self.embedding(input_ids)  # (bs,seqlen,embed_dim)\n",
    "        past_kv=None\n",
    "        for i,block in enumerate(self.blocks):\n",
    "            X,past_kv=block(X,past_key_value=past_kv,use_cache=False)\n",
    "        X=self.norm(X)\n",
    "        lm_logits=self.lm_output(X)  # (bs,seqlen,vocab_size)\n",
    "        lm_logits=F.softmax(lm_logits,dim=-1)  # 应用softmax得到概率分布\n",
    "        return lm_logits\n",
    "# Example usage\n",
    "block_num = 12  # 假设有12个Block\n",
    "vocab_size = 128000  # 假设词表大小为6400\n",
    "embed_dim = 1024  # 假设嵌入维度为512\n",
    "head_num = 8  # 假设头数为8\n",
    "kv_head_num = 4  # 假设键值头数为4\n",
    "ffn_dim = 2048  # 假设前馈网络维度为204\n",
    "attn_dropout = 0.1  # 假设注意力dropout率为\n",
    "ffn_dropout = 0.1  # 假设前馈网络dropout率为0.1\n",
    "max_seqlen = 1024  # 假设最大序列长度为102\n",
    "model = MinimindLM_Dense(block_num*3, vocab_size, embed_dim, head_num, kv_head_num, ffn_dim, attn_dropout, ffn_dropout, max_seqlen=max_seqlen)\n",
    "input_ids = torch.randint(0, vocab_size, (2, 10))  # 假设输入的input_ids为2个句子，每个句\n",
    "lm_logits = model(input_ids)\n",
    "print(lm_logits.shape)  # 输出形状应为(2, 10,\n",
    "print(lm_logits)  # 输出嵌入后的向量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c285645",
   "metadata": {},
   "source": [
    "# 至此我们torch原生dense模型搭建完毕\n",
    "统计下model的参数量吧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "61bf120b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总参数量: 0.602380288 B\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"总参数量: {total_params/1e9:,} B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f065a7d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
