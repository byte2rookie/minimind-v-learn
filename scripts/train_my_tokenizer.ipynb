{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d95c12a4",
   "metadata": {},
   "source": [
    "# 什么是tokenizer\n",
    "[tokenizer全解析](https://zhuanlan.zhihu.com/p/651430181)<br>\n",
    "NLP中的划分方法主要为:字划分、词划分、subword的划分<br>\n",
    "目前业界主要包括四种subword划分分词model:BPE,BBPE,wordpiece,unigram<br>\n",
    "subword的主要划分阶段为：归一化->预分词->模型分词->后处理四个阶段\n",
    "![tokenizer的过程](../images/tokenizer.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a524b35",
   "metadata": {},
   "source": [
    "[三种分词方法的本质](https://zhuanlan.zhihu.com/p/664717335)\n",
    "## BPE\n",
    "BPE的原理很简单，就是将原有的词语先拆分为字，然后根据字的组合频率一步步取出最高概率的组合词\n",
    "具体可以参考[第3节BPE](https://zhuanlan.zhihu.com/p/651430181)\n",
    "## wordpiece\n",
    "wordpiece主要原理和BPE近似，只是wordpiece选择的组合指标是互信息的大小\n",
    "## unigram\n",
    "unigram是一个自顶向下的过程，先组合好一个巨大的词表，然后在语料中寻找频率最高的前k条\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6ae96a",
   "metadata": {},
   "source": [
    "# 训练自己的tokenizer\n",
    "利用transformers库可以很方便的训练我们自己的BPE分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "327c1ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    Tokenizer,\n",
    "    models,\n",
    "    trainers,\n",
    "    decoders,\n",
    "    processors,\n",
    "    pre_tokenizers,\n",
    "    normalizers\n",
    ")\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebb086c",
   "metadata": {},
   "source": [
    "## 添加一些特殊标记用于单独分词\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0ec44c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens=[\"<endoftext>\",\"<|im_end|>\",\"<|im_start|>\",\"<pad>\",\"<mask>\",\"<s>\",\"</s>\",\"<unk>\",\"<UNK>\",\"<EOS>\",\"<zzy>\",\"<|s1|>\",\"<|s2|>\"]\n",
    "trainer=trainers.BpeTrainer(\n",
    "    vocab_size=6400,\n",
    "    special_tokens=special_tokens,\n",
    "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d2dc66",
   "metadata": {},
   "source": [
    "## 设计读取pretrain数据集的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac32d7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>鉴别一组中文文章的风格和特点，例如官方、口语、文言等。需要提供样例文章才能准确鉴别不同的风格和特点。<|im_end|> <|im_start|>好的，现在帮我查一下今天的天气怎么样?今天的天气依据地区而异。请问你需要我帮你查询哪个地区的天气呢？<|im_end|> <|im_start|>打开闹钟功能，定一个明天早上七点的闹钟。好的，我已经帮您打开闹钟功能，闹钟将在明天早上七点准时响起。<|im_end|> <|im_start|>为以下场景写一句话描述：一个孤独的老人坐在公园长椅上看着远处。一位孤独的老人坐在公园长椅上凝视远方。<|im_end|> <|im_start|>非常感谢你的回答。请告诉我，这些数据是关于什么主题的？这些数据是关于不同年龄段的男女人口比例分布的。<|im_end|> <|im_start|>帮我想一个有趣的标题。这个挺有趣的：\"如何成为一名成功的魔术师\" 调皮的标题往往会吸引读者的注意力。<|im_end|> <|im_start|>回答一个问题，地球的半径是多少？地球的平均半径约为6371公里，这是地球自赤道到两极的距离的平均值。<|im_end|> <|im_start|>识别文本中的语气，并将其分类为喜悦、悲伤、惊异等。\n",
      "文本：“今天是我的生日！”这个文本的语气是喜悦。<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "#采用的训练方法是train_from_iterator\n",
    "#定义一个迭代器函数读取数据集\n",
    "def read_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            yield data['text']  # 假设每行数据有一个'text'字段\n",
    "#测试一下读取的方法\n",
    "data_path=\"../data/pretrain_hq.jsonl\"\n",
    "#打印两条数据\n",
    "data_iter = read_data(data_path)\n",
    "for _ in range(1):\n",
    "    print(next(data_iter))\n",
    "del data_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03c08f7",
   "metadata": {},
   "source": [
    "## 利用iterator训练器开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e82284a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../model/vocab.json', '../model/merges.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path=\"../data/pretrain_hq.jsonl\"\n",
    "#开始训练\n",
    "data_iter = read_data(data_path)\n",
    "tokenizer.train_from_iterator(data_iter, trainer=trainer)\n",
    "#设置解码器\n",
    "tokenizer.decoder= decoders.ByteLevel()\n",
    "# 验证特殊词汇训练结果\n",
    "assert tokenizer.token_to_id(\"<endoftext>\") == 0\n",
    "assert tokenizer.token_to_id(\"<|im_end|>\") == 1\n",
    "assert tokenizer.token_to_id(\"<|im_start|>\") == 2\n",
    "assert tokenizer.token_to_id(\"<pad>\") == 3\n",
    "assert tokenizer.token_to_id(\"<mask>\") == 4\n",
    "assert tokenizer.token_to_id(\"<s>\") == 5\n",
    "assert tokenizer.token_to_id(\"</s>\") == 6\n",
    "assert tokenizer.token_to_id(\"<unk>\") == 7\n",
    "assert tokenizer.token_to_id(\"<UNK>\") == 8\n",
    "assert tokenizer.token_to_id(\"<EOS>\") == 9\n",
    "assert tokenizer.token_to_id(\"<zzy>\") == 10\n",
    "assert tokenizer.token_to_id(\"<|s1|>\") == 11\n",
    "assert tokenizer.token_to_id(\"<|s2|>\") == 12\n",
    "# 保存tokenizer\n",
    "import os\n",
    "save_path=\"../model\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "tokenizer.save(os.path.join(save_path, \"tokenizer.json\"))\n",
    "tokenizer.model.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5999f1",
   "metadata": {},
   "source": [
    "最终得到训练好的tokenizer.json文件以及对应的vocab.json和merges.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de406ac",
   "metadata": {},
   "source": [
    "# 手动创建一份配置文件tokenizer_config.json\n",
    "[关于配置文件](https://blog.csdn.net/xiezhipu/article/details/145585777)<br>\n",
    "[关于normalizer的配置说明](https://blog.csdn.net/weixin_49346755/article/details/126496833)<br>\n",
    "[关于post_process的配置说明](https://blog.csdn.net/weixin_49346755/article/details/126499720)<br>\n",
    "[chat_template的设计规则](https://www.guyuehome.com/detail?id=1888166611628642305)<br>\n",
    "配置文件的作用主要是记录tokenizer的normalize,pre_process,post_process,template,special_token等tokenizer的关键参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "000c4c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer training completed and saved.\n"
     ]
    }
   ],
   "source": [
    "# 手动创建配置文件\n",
    "config = {\n",
    "        \"add_bos_token\": False,\n",
    "        \"add_eos_token\": False,\n",
    "        \"add_prefix_space\": False,\n",
    "        \"added_tokens_decoder\": {\n",
    "            \"0\": {\n",
    "                \"content\": \"<|endoftext|>\",\n",
    "                \"lstrip\": False,\n",
    "                \"normalized\": False,\n",
    "                \"rstrip\": False,\n",
    "                \"single_word\": False,\n",
    "                \"special\": True\n",
    "            },\n",
    "            \"1\": {\n",
    "                \"content\": \"<|im_end|>\",\n",
    "                \"lstrip\": False,\n",
    "                \"normalized\": False,\n",
    "                \"rstrip\": False,\n",
    "                \"single_word\": False,\n",
    "                \"special\": True\n",
    "            },\n",
    "            \"2\": {\n",
    "                \"content\": \"<|im_start|>\",\n",
    "                \"lstrip\": False,\n",
    "                \"normalized\": False,\n",
    "                \"rstrip\": False,\n",
    "                \"single_word\": False,\n",
    "                \"special\": True\n",
    "            },\n",
    "            \"3\": {\n",
    "                \"content\": \"<pad>\",\n",
    "                \"lstrip\": False,\n",
    "                \"normalized\": False,\n",
    "                \"rstrip\": False,\n",
    "                \"single_word\": False,\n",
    "                \"special\": True\n",
    "            },\n",
    "            \"4\": {\n",
    "                \"content\": \"<mask>\",\n",
    "                \"lstrip\": False,\n",
    "                \"normalized\": False,\n",
    "                \"rstrip\": False,\n",
    "                \"single_word\": False,\n",
    "                \"special\": True\n",
    "            },\"5\": {\n",
    "                \"content\": \"<s>\",\n",
    "                \"lstrip\": False,\n",
    "                \"normalized\": False,\n",
    "                \"rstrip\": False,\n",
    "                \"single_word\": False,\n",
    "                \"special\": True\n",
    "            },\"6\": {\n",
    "                \"content\": \"</s>\",\n",
    "                \"lstrip\": False,\n",
    "                \"normalized\": False,\n",
    "                \"rstrip\": False,\n",
    "                \"single_word\": False,\n",
    "                \"special\": True\n",
    "            },\"7\": {\n",
    "                \"content\": \"<unk>\",\n",
    "                \"lstrip\": False,\n",
    "                \"normalized\": False,\n",
    "                \"rstrip\": False,\n",
    "                \"single_word\": False,\n",
    "                \"special\": True\n",
    "            },\"8\": {\n",
    "                \"content\": \"<UNK>\",\n",
    "                \"lstrip\": False,\n",
    "                \"normalized\": False,\n",
    "                \"rstrip\": False,\n",
    "                \"single_word\": False,\n",
    "                \"special\": True\n",
    "            },\"9\": {\n",
    "                \"content\": \"<EOS>\",\n",
    "                \"lstrip\": False,\n",
    "                \"normalized\": False,\n",
    "                \"rstrip\": False,\n",
    "                \"single_word\": False,\n",
    "                \"special\": True\n",
    "            },\"10\": {\n",
    "                \"content\": \"<zzy>\",\n",
    "                \"lstrip\": False,\n",
    "                \"normalized\": False,\n",
    "                \"rstrip\": False,\n",
    "                \"single_word\": False,\n",
    "                \"special\": True\n",
    "            },\"11\": {\n",
    "                \"content\": \"<|s1|>\",\n",
    "                \"lstrip\": False,\n",
    "                \"normalized\": False,\n",
    "                \"rstrip\": False,\n",
    "                \"single_word\": False,\n",
    "                \"special\": True\n",
    "            },\"12\": {\n",
    "                \"content\": \"<|s2|>\",\n",
    "                \"lstrip\": False,\n",
    "                \"normalized\": False,\n",
    "                \"rstrip\": False,\n",
    "                \"single_word\": False,\n",
    "                \"special\": True\n",
    "            },\n",
    "        },\n",
    "        \"additional_special_tokens\": [ \"<pad>\", \"<mask>\",\"<s>\",\"</s>\",\"<unk>\",\"<UNK>\",\"<EOS>\",\"<zzy>\",\"<|s1|>\",\"<|s2|>\"],\n",
    "        \"bos_token\": \"<|im_start|>\",\n",
    "        \"clean_up_tokenization_spaces\": False,\n",
    "        \"eos_token\": \"<|im_end|>\",\n",
    "        \"legacy\": True,\n",
    "        \"model_max_length\": 32768,\n",
    "        \"pad_token\": \"<pad>\",\n",
    "        \"sp_model_kwargs\": {},\n",
    "        \"spaces_between_special_tokens\": False,\n",
    "        \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
    "        \"unk_token\": \"<|endoftext|>\",\n",
    "        \"chat_template\": \"{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{{ '<|im_start|>system\\\\n' + system_message + '<|im_end|>\\\\n' }}{% else %}{{ '<|im_start|>system\\\\nYou are a helpful assistant<|im_end|>\\\\n' }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\\\\n' + content + '<|im_end|>\\\\n<|im_start|>assistant\\\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|im_end|>' + '\\\\n' }}{% endif %}{% endfor %}\"\n",
    "    }\n",
    "\n",
    "save_path = \"../model\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "# 保存配置文件\n",
    "with open(os.path.join(save_path, \"tokenizer_config.json\"), \"w\", encoding=\"utf-8\") as config_file:\n",
    "    json.dump(config, config_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Tokenizer training completed and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05278e2",
   "metadata": {},
   "source": [
    "### 典型问题\n",
    "config的added_tokens_decoder里<br>\n",
    "我的\"<|多余的flag|>\",\"<|zzy|>\"其实没有对应的encoder，所以会产生不对应的问题，这样的做法是错误的<br>\n",
    "具体添加特殊标记的方法请参考模型的官方链接\n",
    "[参考1：如何扩充词表](https://zhuanlan.zhihu.com/p/704346193#:~:text=%E7%AE%80%E5%8D%95%E6%9D%A5%E8%AF%B4%EF%BC%8C%E8%AF%BB%E5%85%A5%20tokenizer%20model%E4%B9%8B%E5%90%8E%EF%BC%8C%E8%B0%83%E7%94%A8%20tokenizer%20%E7%9A%84%20add_special_tokens%20%E6%96%B9%E6%B3%95%E7%BB%99%20tokenizer,model%20%E7%9A%84%20embedding%20size%EF%BC%8C%E9%80%9A%E8%BF%87%E8%B0%83%E7%94%A8%20model%20%E7%9A%84%20resize_token_embeddings%20%E6%96%B9%E6%B3%95%E6%9D%A5%E5%AE%9E%E7%8E%B0%E8%BF%99%E4%B8%80%E7%82%B9%E3%80%82)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7130139",
   "metadata": {},
   "source": [
    "#### template说明\n",
    "采用的为jinja的模板引擎，用来表示template的格式\n",
    "chat_template结构：\n",
    "```\n",
    "{% if messages[0]['role'] == 'system' %}\n",
    "  {% set system_message = messages[0]['content'] %}\n",
    "  {{ '<|im_start|>system\\n' + system_message + '<|im_end|>\\n' }}\n",
    "{% else %}\n",
    "  {{ '<|im_start|>system\\nYou are a helpful assistant<|im_end|>\\n' }}\n",
    "{% endif %}\n",
    "\n",
    "{% for message in messages %}\n",
    "  {% set content = message['content'] %}\n",
    "  {% if message['role'] == 'user' %}\n",
    "    {{ '<|im_start|>user\\n' + content + '<|im_end|>\\n<|im_start|>assistant\\n' }}\n",
    "  {% elif message['role'] == 'assistant' %}\n",
    "    {{ content + '<|im_end|>' + '\\n' }}\n",
    "  {% endif %}\n",
    "{% endfor %}\n",
    "```\n",
    "\n",
    "后续在读入我们的SFT的多轮对话数据集时候，就会用chat_template来转换数据集格式的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58755e47",
   "metadata": {},
   "source": [
    "#### 评估tokenizer的效果\n",
    "简而言之就是利用AutoTokenizer调用本地的tokenizer训练文件\n",
    "![调用过程](../images/AutoTokenizer.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5eda98cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/zyp/miniconda3/envs/minimind/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "你是一个优秀的聊天机器人，总是给我正确的回应！<|im_end|>\n",
      "<|im_start|>user\n",
      "你来自哪里？<|im_end|>\n",
      "<|im_start|>assistant\n",
      "我来自地球<|im_end|>\n",
      "\n",
      "Tokenizer vocabulary size: 6400\n",
      "Tokenizer actual size: 6401\n",
      "length of model inputs: 46\n",
      "Input IDs: [2, 95, 101, 315, 81, 89, 211, 405, 932, 5243, 3325, 2125, 273, 2611, 1140, 2607, 711, 480, 1005, 1, 211, 2, 97, 95, 3717, 211, 405, 2730, 3024, 433, 1, 211, 2, 77, 95, 95, 85, 315, 3932, 96, 211, 309, 2730, 1292, 1, 211]\n",
      "Response matches new prompt: True\n",
      "Tokenizer special tokens:\n",
      "{'bos_token': '<|im_start|>', 'eos_token': '<|im_end|>', 'unk_token': '<|endoftext|>', 'pad_token': '<pad>', 'additional_special_tokens': ['<pad>', '<mask>', '<s>', '</s>', '<unk>', '<UNK>', '<EOS>', '<zzy>', '<|s1|>', '<|s2|>']}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# 评估tokenizer的效果\n",
    "tokenizer_path=\"../model/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, use_fast=True)\n",
    "\n",
    "#后续SFT数据的格式\n",
    "messages = [\n",
    "        {\"role\": \"system\", \"content\": \"你是一个优秀的聊天机器人，总是给我正确的回应！\"},\n",
    "        {\"role\": \"user\", \"content\": '你来自哪里？'},\n",
    "        {\"role\": \"assistant\", \"content\": '我来自地球'}\n",
    "    ]\n",
    "new_prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False\n",
    "    ) #会把原始的dict转化为template格式的string\n",
    "print(new_prompt)\n",
    "\n",
    "#tokenizer理论词表长度\n",
    "print(\"Tokenizer vocabulary size:\", tokenizer.vocab_size)\n",
    "#tokenizer实际长度\n",
    "len_tokenizer = len(tokenizer)\n",
    "print(\"Tokenizer actual size:\", len_tokenizer)\n",
    "\n",
    "#encoder测试\n",
    "model_inputs=tokenizer(new_prompt)\n",
    "print(\"length of model inputs:\", len(model_inputs['input_ids']))\n",
    "input_ids= model_inputs['input_ids']\n",
    "print(\"Input IDs:\", input_ids)\n",
    "response= tokenizer.decode(input_ids, skip_special_tokens=False)\n",
    "#比对response和new_prompt是否一致\n",
    "print(\"Response matches new prompt:\", response == new_prompt)\n",
    "\n",
    "#打印tokenizer的特殊token\n",
    "print(\"Tokenizer special tokens:\")\n",
    "print(tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01fa8c9",
   "metadata": {},
   "source": [
    "### 结束语\n",
    "经过这几步处理，我们实现了：<br>\n",
    "- 利用BPE model和Pretrain数据集,训练出了自己的BPE-tokenizer\n",
    "- 利用transformers调用tokenizer.json和tokenizer_config.json实现了分词器的构建\n",
    "- 利用训练好的分词器，实现了word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bec212d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
